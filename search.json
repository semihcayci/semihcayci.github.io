[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Theoretical RL",
    "section": "",
    "text": "Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRL for POMDPs – Part I: TD Learning with Internal States\n\n\nHow does multi-step TD learning with internal states mitigate partial observability?\n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html",
    "href": "posts/td-learning-for-pomdps.html",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "",
    "text": "TL;DR. A classic result by (Singh, Jaakkola, and Jordan 1994) shows that memoryless TD learning fails in POMDPs. To address this, we studied a natural actor–critic (NAC) method for POMDPs in (Cayci, He, and Srikant 2024), with a critic that uses \\(m\\)-step TD learning with an internal state to mitigate perceptual aliasing problem. Below, I review this algorithm and provide some follow-up results."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#formal-pomdp-model",
    "href": "posts/td-learning-for-pomdps.html#formal-pomdp-model",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "1 Formal POMDP model",
    "text": "1 Formal POMDP model\nLet us consider a discounted POMDP \\[\n\\mathcal{M}=\\big(\\mathcal S,\\mathcal A,\\mathcal Y,\\; P,\\; O,\\; r,\\; \\gamma\\big),\n\\]\nwith:\n\nstate \\(S_k\\in\\mathcal S\\),\naction \\(A_k\\in\\mathcal A\\),\nobservation \\(Y_k\\in\\mathcal Y\\),\ntransition kernel \\(P(s' \\mid s,a)\\),\nobservation kernel \\(\\Phi(y \\mid s')\\),\nreward \\(r(s,a)\\in[0,1]\\) (w.l.o.g.),\ndiscount factor \\(\\gamma\\in(0,1)\\).\n\nTo avoid measure-theoretic concerns, let us consider finite \\(\\mathcal S, \\mathcal Y, \\mathcal A\\). These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is \\[\nS_{k+1}\\sim P(\\cdot\\mid S_k,A_k),\\qquad\nY_{k}\\sim \\Phi(\\cdot\\mid S_k),\\qquad\nR_k=r(S_k,A_k),\n\\] and the goal is to maximize \\[\n\\mathcal V^\\pi(\\lambda)=\\mathbb E_\\lambda^\\pi\\!\\left[\\sum_{k\\ge0}\\gamma^k\\,r(S_k,A_k)\\right],\n\\] where \\(\\lambda\\in\\mathcal{P}(\\mathcal{Y}}\\) is the distribution of \\(Y_0\\).\nWhy challenging?\nPartial observability implies that the optimal policy \\(\\pi^\\star=(\\pi_0^\\star,\\pi_1^\\star,\\ldots)\\) is non-stationary, therefore \\(\\pi_k^\\star\\) depends on the complete trajectory \\((Y_{0:k},A_{0:k-1})\\) for each \\(k\\in\\mathbb N\\) via the Bayes belief \\(b_k(s)=\\Pr(S_k=s\\mid Y_{0:k},A_{0:k-1})\\). As such, some very intuitive ideas for partially observable reinforcement learning (PORL) fails: \\begin{}\nTabular parameterization has a memory complexity of \\(\\mathcal{O}(|\\mathcal{Y}|^{k+1}|\\mathcal{A}|^k)\\) at time \\(k\\), growing exponentially. Even for small action-observation spaces, this would be an intractable solution.\nReactive (i.e., memoryless) methods for MDPs fail if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see examples below).\nA classical result attributed to (Åström 1965) (see (Yüksel 2025) for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state \\(b_k\\). However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over \\(\\mathcal S\\)), the conventional RL tools is highly impractical to solve this belief-MDP formulation. \\end{itemize}\nAn alternative and more elegant idea is to incorporate memory into the policies (Murphy 2000) and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state \\(Z_k^c\\), which summarizes/compresses \\((Y_{0:k-1},A_{0:k-1})\\) for each \\(k\\). In policy-based PORL, the idea is to use a parametric policy \\(\\pi_k(\\cdot\\mid Y_k, Z_k^c)\\) based on the internal state \\(Z_k^c\\)."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#setting-and-goal",
    "href": "posts/td-learning-for-pomdps.html#setting-and-goal",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "2 Setting and Goal",
    "text": "2 Setting and Goal\nAt first, we restrict policies to finite-state controllers (FSCs) that keep internal memory \\(Z_k^c\\) and act from \\((Y_k, Z_k^c)\\). The internal memory is updated in a Markovian way: \\(Z_{k+1}^c\\sim\\varphi(\\cdot\\mid z_k, y_k, a_k)\\) given \\(Z_k^c=z_k,Y_k=y_k,A_k=a_k\\).\n\nLearn the best policy within a fixed FSC class \\(\\pi_{Z,\\varphi}\\):\n\\[\n\\pi^\\star \\in \\arg\\max_{\\pi \\in \\pi_{Z,\\varphi}} \\; \\mathcal V^\\pi(\\xi).\n\\]\nA useful subclass is the sliding-window controller (SWC) with window \\(n\\):\n\\[\nZ_k^c = (Y_{k-n:k-1},\\; A_{k-n:k-1}) \\in \\mathcal Y^n \\times \\mathcal A^n,\n\\]\nwhich summarizes the last \\(n\\) observations/actions, where \\(n\\) is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.\n\nThe actor uses a linear-softmax FSC: \\[\n\\pi_\\theta(a \\mid y,z) =\n\\frac{\\exp\\{\\theta^\\top \\psi(a,y,z)\\}}\n{\\sum_{a'} \\exp\\{\\theta^\\top \\psi(a',y,z)\\}},\n\\] with features \\(\\psi(a,y,z)\\in\\mathbb R^d\\). The controller memory updates via a transition kernel \\(\\varphi\\): \\[\nZ_{k+1}^c\\sim\\varphi(\\cdot\\mid Z_k^c, Y_{k}, A_k).\n\\]\nIn order to compute a low-variance estimate of the (natural) policy gradient for this linear-softmax FSC within an actor-critic framework, we need to evaluate \\(\\mathcal{Q}_\\theta(A_k,Y_k,Z_k^c)\\) under this internal state representation. This blog post is about this part, the critic."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#hidden-markov-reward-processes-hmrp",
    "href": "posts/td-learning-for-pomdps.html#hidden-markov-reward-processes-hmrp",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "3 Hidden Markov Reward Processes (HMRP)",
    "text": "3 Hidden Markov Reward Processes (HMRP)\nA common simplification in TD learning analyses in an MDP setting is to consider a Markov reward process (MRP). Since the augmented state \\(X_k:=(S_k,A_k)\\) under a given stationary policy forms a Markov chain, the analyses automatically extend to policy evaluation (known as SARSA) since \\((X_k,r(X_k))\\) forms an MRP.\nIn POMDPs, we can make a similar simplification. First, let us consider a Hidden Markov Process with a Markov chain \\(S_k\\) and its partial observation process \\(Y_k\\sim\\Phi(\\cdot\\mid S_k)\\). Along with the reward \\(r(S_k)\\), this HMP induces a Hidden Markov Reward Process (HMRP) \\(\\{(S_k,r(S_k),Y_k):k\\in\\mathbb{N}\\}\\).\nConsider a finite-state controller with an internal state \\(Z_k^c\\). Then, we have \\(A_k \\sim \\pi_k(\\cdot\\mid Z_k^c)\\), thus \\(X_k:=(S_k,A_k,Z_k^c)\\) forms a Markov process with the transition kernel \\[P(S_{k+1}=s',A_{k+1}=a',Z_{k+1}^c=z'|S_k,A_k,Z_k^c) = \\sum_{y'\\in\\mathcal Y}P(s'|S_k,A_k)\\Phi(y'|s')\\varphi(z'|Z_k^c,y',A_k)\\pi_{k+1}(a'\\mid y',z').\\] The noisy observation is \\(Y_k~\\Phi(\\cdot|S_k)\\). Defining \\(\\tilde{r}(S_k,A_k,Z_k^c) = r(S_k,A_k)\\), we obtain the original process under \\(\\pi\\). Thus, the controlled process under \\(\\pi\\) induces the HMRP \\((X_k, \\tilde{r}(X_k), Y_k)\\)."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#critic-m-step-td-learning-with-an-internal-state",
    "href": "posts/td-learning-for-pomdps.html#critic-m-step-td-learning-with-an-internal-state",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "4 Critic: \\(m\\)-step TD Learning with an Internal State",
    "text": "4 Critic: \\(m\\)-step TD Learning with an Internal State\nIt is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology (Singh, Jaakkola, and Jordan 1994). For a concrete demonstration, let us focus on learning the value function in an HMRP: \\[V(z_0):=\\mathbb E[\\sum_{k=0}^\\infty \\gamma^k r(S_k)|Z_0=z_0]\\mbox{ and }V_s^\\star:=\\mathbb E[\\sum_{k=0}^\\infty r(S_k)|S_0=s]\\] for an internal state \\(Z_k\\) (e.g., \\(Z_t = (Y_{t-n+1},\\ldots,Y_t)\\)), where \\((S_k, r(S_k), Y_k)\\) is an HMRP.\n\n\n\n\n\n\nTip\n\n\n\nTwo objectives. In the above display, if we want to perform policy evaluation for an FSC with \\(\\{Z_k^c:k\\in\\mathbb N\\}\\), we would be interested in \\(V(z_0)\\). Additionally, we can use \\(m\\)-step TD Learning for estimating the value function \\(V_s^\\star\\). We discuss both problems in what follows.\n\n\nExample 1 (When TD(0) fails to learn \\(V_s^\\star\\)). Here we consider an HMRP with \\(\\mathcal S = \\{0,1\\}\\), \\(r(s) = s\\) and \\(\\Phi(1|s) = 1\\) for all \\(s\\in\\mathcal{S}\\). Let \\(P(0|1) = q\\) and \\(P(1|0) = p\\) for some \\(p, q \\in (0,1)\\), and \\(\\gamma \\in (0, 1)\\) be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to \\(v^\\star = \\frac{1}{1-\\gamma}\\cdot \\frac{p}{q+p} = V_0^\\star + \\frac{p}{p+q}(V_1^\\star-V_0^\\star)\\). In the proof of Theorem 4.1 in (Cayci, He, and Srikant 2024), we characterize the issues related to the fixed point of single-step TD(0) for general POMDPs.\nOn the other hand, the value function for a given \\(z_0\\) is \\[V(z_0) = V_0^\\star + (V_1^\\star-V_0^\\star)P(S_0=1|Z_0=z_0) = \\frac{\\gamma p + (1-\\gamma)P(S_0=1|Z_0=z_0)}{(1-\\gamma)\\Big[\\gamma(p+q)+1-\\gamma\\Big]},\\] since \\(V(z_0) = V_0^\\star+(V_1^\\star-V_0^\\star)P(S_0=1|Z_0=z_0)\\). Also, \\(V_{S_0}^\\star = V_0^\\star + (V_1^\\star-V_0^\\star)\\mathbf{1}_{\\{S_0=1\\}}\\). Therefore, \\[\\mathbb{E}[(V_{S_0}^\\star-v^\\star)^2] = (V_1^\\star-V_0^\\star)^2\\mathbb{E}[(\\frac{p}{p+q}-\\mathbf{1}_{\\{S_0=1\\}})^2]=Var(\\mathbf{1}_{\\{S_0=1\\}})(V_1^\\star-V_0^\\star)^2.\\]\nExample 2 (When TD(0) fails to learn \\(V(z_0)\\)). Consider the two-state HMRP with \\(\\mathcal S = \\{0,1\\}\\), \\(r(s) = s\\) and \\(\\Phi(1|s) = c_s\\in(0,1)\\). Let \\(P(0|1) = P(1|0) = 0.01\\), and \\(\\gamma = 0.99\\). Consider an internal state \\(Z_k = (Y_{k-1}, Y_k)\\). We want to compute \\(V(z_0) = \\mathbb{E}[\\sum_k \\gamma^k r(S_k)|Z_0=z_0]\\). For this purpose, one can use tabular-TD(0) with the surrogate state \\(Z_k=(Y_{k-1},Y_k)\\). In this case, the limit point of TD(0) would be the unique fixed point of \\[(T_1v)(z_0):=P(S_0=1|Z_0=z_0) + \\gamma \\sum_{z_1\\in\\{0,1\\}^2}v(z_1)P(Z_1=z_1|Z_0=z_0).\\] Now, let’s diagnose why the true value is not the fixed point of the Bellman equation. Note that we can write \\[\\begin{align*}\nV(z_0) &= \\mathbb E[r(S_0)|Z_0=z_0] + \\gamma \\mathbb{E}[\\sum_{k=0}^\\infty \\gamma^kr(S_{k+1})|Z_0=z_0]\\\\\n&= \\mathbb E[r(S_0)|Z_0=z_0] + \\gamma \\mathbb{E}\\left[\\mathbb{E}[\\sum_{k=0}^\\infty \\gamma^kr(S_{k+1})|Z_1, Z_0=z_0]\\big|Z_0=z_0\\right]\n\\end{align*}\\] Note that \\((Z_0,Z_1) = (Y_{-1},Y_0,Y_1)\\). Thus, \\(\\mathbb{E}[\\sum_{k=0}^\\infty \\gamma^kr(S_{k+1})|Z_1, Z_0=z_0] \\neq \\mathbb{E}[\\sum_{k=0}^\\infty \\gamma^kr(S_{k+1})|Z_1]\\), which implies that \\(V(z_0)\\) is not the fixed point of \\(T_1\\). To see how they differ, for \\(c_1=0.51\\) and \\(c_0 = 0.49\\), we have \\(\\sqrt{\\sum_{z_0\\in\\{0,1\\}^2}\\mu(z_0)|v_1(z_0)-V(z_0)|^2}\\approx 0.448863\\), where \\(\\mu\\) is the stationary distribution of \\(Z_k\\). This indicates that a naive application of TD(0) with the internal state \\(\\{Z_k:k\\in\\mathbb N\\}\\) does not return the true value function \\(V(z_0)\\). Thus, in case we want to evaluate the performance of a policy with an internal state \\(Z_k^c = (Y_{k-1}, A_{k-1})\\), TD(0) suffers from an estimation error.\n\n\n\n\n\n\nTip\n\n\n\nWhy \\(m\\)-step TD? Examples 1 and 2 show that, in POMDPs, one-step bootstrapping suffers from perceptual aliasing (same observation \\(Y_k\\), different latent state \\(S_k\\)). We will see in the following that looking \\(m\\) steps ahead stabilizes the target and reduces this bias.\n\n\nNow, let us focus on \\(m\\)-step TD learning with an internal state \\(Z_k\\) for the above problem, where \\(m\\in\\mathbb{Z}_+\\). Note that the internal state may match the controller state \\(Z_k^c\\) (in the case of control, see Section 3), or it can be used to obtain an estimate of \\(V_s^\\star\\).\n\n4.1 Learning \\(V(z_0)\\) by \\(m\\)-step TD(0) with an Internal State\nLet \\[(T_mv)(z) = \\bar{r}_m(z) + \\gamma^m(K_mv)(z),\\] where \\(\\bar{r}_m(z) := \\mathbb E[\\sum_{k=0}^{m-1}r(S_k)|Z_0=z]\\) and \\((K_mv)(z):=\\mathbb{E}[v(Z_m)|Z_0=z]\\). Then, it is straightforward to show that \\(T_m\\) is a contractive operator with modulus \\(\\gamma^m\\), and admits a unique fixed point \\[V(z) = \\mathbb E[\\sum_{k=0}^\\infty \\gamma^k r(S_k)|Z_0=z] = \\sum_{k=0}^\\infty (\\gamma^mK_m)^k\\bar{r}_m(z)\\] for any \\(m\\in\\mathbb N\\). Let \\(\\xi\\) be the stationary distribution of \\(\\{Z_k:k\\in\\mathbb N\\}\\), which exists given \\(S_k\\) is an ergodic unichain. Under a non-degenerate \\(\\mathbb{E}_{Z_t\\sim \\xi}[\\psi(Z_t)\\otimes\\psi(Z_t)]\\succ 0\\), \\(m\\)-step TD learning with the internal state \\(Z_t\\) converges to the unique fixed point \\(v_m\\) of the equation \\[v = \\Pi\\{\\bar{r}_m+\\gamma^m K_m v\\},\\] where \\(\\Pi\\) be the ortogonal projection onto the subspace \\(\\mathcal V := \\{z\\mapsto \\langle \\psi(z),w\\rangle: w \\in \\mathbb R^d\\}\\) with respect to \\(L^2(\\xi)\\). Thus, we have the following result.\n\n\n\n\n\n\nImportant\n\n\n\nFor any \\(m\\in\\mathbb{Z}_+\\), \\(m\\)-step TD(0) with an internal state \\(\\{Z_k:k\\in\\mathbb N\\}\\) converges to \\(v_m\\), which satisfies \\[\\|v_m - \\Pi V\\|_{L^2(\\xi)} \\leq \\frac{\\gamma^m}{1+\\gamma^m}\\|V-\\Pi V\\|_{L^2(\\xi)}.\\]\n\n\nAs such, in case we are interested in policy evaluation for an FSC, this formulation implies the near optimality of \\(m\\)-step TD learning, which uses the internal state of the controller as the critic internal state, with an additional error term \\(\\mathcal{O}(\\gamma^m)\\).\n\n\n4.2 Learning \\(V_s^\\star\\) by \\(m\\)-step TD(0) with a Sliding-Window Internal State\nThe analysis of \\(m\\)-step TD(0) with an internal state \\(Z_k=(Y_{k-n},\\ldots,Y_k)\\) can be taken one step further indeed. Let us consider Example 1 with a general \\(\\{\\Phi(y|s):s\\in\\mathcal S,y\\in\\mathcal{Y}\\}\\). Let \\(\\mathcal{F}_n :=\\sigma(Z_0)=\\sigma(Y_{-n+1},\\ldots,Y_0)\\) be the \\(\\sigma\\)-algebra generated by the internal state, and \\(b^{(n)} := P(S_0 = 1|\\mathcal{F}_n)\\). Then, \\(V_n(z_0) =  \\mathbb E[\\sum_{k=0}^\\infty\\gamma^kr(S_k)|\\mathcal{F}_n]\\). Since \\(V_{S_0}^\\star = V_0^\\star + (V_1^\\star-V_0^\\star)\\mathbf{1}_{\\{S_0=1\\}}\\) and \\(V_n(z_0) = V_0^\\star + (V_1^\\star-V_0^\\star)b^{(n)},\\) we obtain \\[\\mathbb{E}[(V_{S_0}^\\star-V_n(Z_0))^2] = (V_1^\\star-V_0^\\star)^2\\,\\mathbb E\\!\\left[(\\mathbf{1}_{\\{S_0=1\\}}-b^{(n)})^2\\right].\\] Furthermore, since \\(b^{(n)}=\\mathbb E[\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_n]\\), we have\n\\[\\begin{align}\n\\mathbb E\\!\\left[(\\mathbf{1}_{\\{S_0=1\\}}-b^{(n)})^2\\right]&=\\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_n)]\\\\\n&\\downarrow \\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_\\infty)]\n\\end{align}\\] as \\(n\\rightarrow\\infty\\) due to the law of total variance for nested \\(\\sigma\\)-algebras \\(\\mathcal{F}_n\\). Therefore, \\[\\mathbb{E}[(V_{S_0}^\\star-V_n(Z_0))^2] \\downarrow (V_1^\\star-V_0^\\star)^2\\,\\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_\\infty)].\\] If \\(S_0\\) is measurable with respect to \\(\\sigma(Y_{-\\infty},\\ldots,Y_0)\\), we have \\(V_{s_0}^\\star=V_\\infty(z_0))\\) almost surely. Used in conjunction with the previous result on \\(v_m-V_n(z_0)\\), a simple triangle inequality would demonstrate the effectiveness of TD learning with sliding-window controllers.\n\n\n\n\n\n\nImportant\n\n\n\nIn summary, for Example 1, we have \\[\\begin{align*}\n  \\mathbb{E}[(V_{S_0}^\\star - v^\\star)^2] &= (V_1^\\star-V_0^\\star)^2 Var(\\mathbf{1}_{\\{S_0=1\\}})\\\\\n&\\geq (V_1^\\star-V_0^\\star)^2\\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_n)] \\downarrow (V_1^\\star-V_0^\\star)^2\\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_\\infty)].\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#whats-next-nac-with-finite-state-controllers-for-pomdps",
    "href": "posts/td-learning-for-pomdps.html#whats-next-nac-with-finite-state-controllers-for-pomdps",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "5 What’s next: NAC with Finite-State Controllers for POMDPs",
    "text": "5 What’s next: NAC with Finite-State Controllers for POMDPs\nIn the next step, we will show how FSC’s yield improved performance (even global near-optimality) for NAC under certain ergodicity conditions. The discussion will be based on stochastic filtering, and establish a trade-off between memory complexity and optimality gap, governed by the size of the internal state space."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Semih Cayci, a math enthusiast working on theoretical ML problems."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nOct 12, 2025\n\n\nRL for POMDPs – Part I: TD Learning with Internal States\n\n\n \n\n\n\n\n\n\nNo matching items"
  }
]