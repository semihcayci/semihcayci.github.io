[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! Iâ€™m YOUR_NAME. This site is built with Quarto."
  },
  {
    "objectID": "posts/2025-10-12-hello-quarto.html",
    "href": "posts/2025-10-12-hello-quarto.html",
    "title": "Towards Provably Effective Policy Optimization for Large POMDPs",
    "section": "",
    "text": "TL;DR. The paper (Cayci, He, and Srikant 2024) proves finite-time guarantees for a natural actorâ€“critic (NAC) method in POMDPs. It learns finite-memory policies, evaluates them with an \\(m\\)-step TD critic to reduce perceptual aliasing, and gives a performance bound that separates optimization/approximation, evaluation, and inference (partial observability) errors. For sliding-window controllers, the inference error decays geometrically with the window size."
  },
  {
    "objectID": "posts/2025-10-12-hello-quarto.html#setting-and-goal",
    "href": "posts/2025-10-12-hello-quarto.html#setting-and-goal",
    "title": "Towards Provably Effective Policy Optimization for Large POMDPs",
    "section": "1 Setting and Goal",
    "text": "1 Setting and Goal\nWe work with a discounted POMDP but restrict policies to finite-state controllers (FSCs) that keep internal memory \\(Z_k\\) and act from \\((Y_k, Z_k)\\).\n\nGoal: learn the best policy within a fixed FSC class \\(\\Pi_{Z,\\varphi}\\):\n\\[\n\\pi^\\star \\in \\arg\\max_{\\pi \\in \\Pi_{Z,\\varphi}} \\; \\mathcal V^\\pi(\\xi),\n\\]\nwhere \\(\\mathcal V^\\pi(\\xi)\\) is the \\(\\gamma\\)-discounted value under prior \\(\\xi\\).\nA useful subclass is the sliding-window controller (SWC) with window \\(n\\):\n\\[\nZ_k = (Y_{k-n:k-1},\\; U_{k-n:k-1}) \\in \\mathcal Y^n \\times \\mathcal U^n,\n\\]\nwhich summarizes the last \\(n\\) observations/actions.\n\nThe actor uses a linear-softmax FSC: \\[\n\\pi_\\theta(u \\mid y,z) =\n\\frac{\\exp\\{\\theta^\\top \\psi(u,y,z)\\}}\n{\\sum_{u'} \\exp\\{\\theta^\\top \\psi(u',y,z)\\}},\n\\] with features \\(\\psi(u,y,z)\\in\\mathbb R^d\\)."
  },
  {
    "objectID": "posts/2025-10-12-hello-quarto.html#sampling-measure-for-analysis",
    "href": "posts/2025-10-12-hello-quarto.html#sampling-measure-for-analysis",
    "title": "Towards Provably Effective Policy Optimization for Large POMDPs",
    "section": "2 Sampling Measure (for Analysis)",
    "text": "2 Sampling Measure (for Analysis)\nLet the discounted visitation over information states be \\[\nd^\\pi_\\xi(y,z)=(1-\\gamma)\\sum_{k\\ge0}\\gamma^k\\,\n\\Pr^\\pi\\!\\big[(Y_k,Z_k)=(y,z)\\,\\big|\\,H_0\\sim\\xi\\big].\n\\] The analysis assumes access to samples from \\(d^\\pi_\\xi\\) (standard in NAC to match the Fisher metric)."
  },
  {
    "objectID": "posts/2025-10-12-hello-quarto.html#algorithm-sketch-fs-nac",
    "href": "posts/2025-10-12-hello-quarto.html#algorithm-sketch-fs-nac",
    "title": "Towards Provably Effective Policy Optimization for Large POMDPs",
    "section": "3 Algorithm Sketch (FS-NAC)",
    "text": "3 Algorithm Sketch (FS-NAC)\n\nCritic â€” run \\(m\\)-step TD(0) with linear function approximation to estimate \\(\\widehat{\\mathcal Q}^{\\pi_t}(y,z,u)\\).\nActor â€” take a natural-gradient step using the criticâ€™s advantage estimate.\n\n\n\n\n\n\n\nTip\n\n\n\nWhy \\(m\\)-step TD? In POMDPs, one-step bootstrapping suffers from perceptual aliasing (same observation, different hidden states). Looking \\(m\\) steps ahead stabilizes the target and reduces this bias."
  },
  {
    "objectID": "posts/2025-10-12-hello-quarto.html#critic-guarantee-policy-evaluation",
    "href": "posts/2025-10-12-hello-quarto.html#critic-guarantee-policy-evaluation",
    "title": "Towards Provably Effective Policy Optimization for Large POMDPs",
    "section": "4 Critic Guarantee (Policy Evaluation)",
    "text": "4 Critic Guarantee (Policy Evaluation)\nWith stepsize \\(\\alpha=K^{-1/2}\\) and parameter radius \\(R\\), after \\(K\\) critic updates: \\[\n\\mathbb E\\!\\left[\\big\\|\\mathcal Q^\\pi-\\widehat{\\mathcal Q}^\\pi_K\\big\\|_{2,\\,d^\\pi_\\xi\\otimes\\pi}\\right]\n\\;\\lesssim\\;\n\\underbrace{\\tfrac{K^{-1/4}}{1-\\gamma}}_{\\text{statistical}}\n+\\underbrace{\\tfrac{\\varepsilon_{\\text{app}}(R)}{1-\\gamma^m}}_{\\text{approximation}}\n+\\underbrace{\\varepsilon_{\\text{pa}}(\\gamma,m,R)}_{\\text{aliasing}},\n\\] where \\(\\varepsilon_{\\text{app}}(R)\\) is the best linear-approximation error within radius \\(R\\), and the aliasing term contracts geometrically: \\[\n\\varepsilon_{\\text{pa}}(\\gamma,m,R)=\\mathcal O\\!\\big(\\gamma^{m/2}\\,\\mathrm{poly}(R,(1-\\gamma)^{-1})\\big).\n\\]\nSampleâ€“accuracy trade-off. Each update uses \\(m\\) samples, so total evaluation samples are \\(\\Theta(mK)\\)."
  },
  {
    "objectID": "posts/2025-10-12-hello-quarto.html#full-actorcritic-performance-finite-time",
    "href": "posts/2025-10-12-hello-quarto.html#full-actorcritic-performance-finite-time",
    "title": "Towards Provably Effective Policy Optimization for Large POMDPs",
    "section": "5 Full Actorâ€“Critic Performance (Finite-Time)",
    "text": "5 Full Actorâ€“Critic Performance (Finite-Time)\nAfter \\(T\\) outer iterations (with tuned stepsizes), \\[\n(1-\\gamma)\\,\\min_{t&lt;T}\\mathbb E\\!\\big[\\mathcal V^{\\pi^\\star}(\\xi)-\\mathcal V^{\\pi_t}(\\xi)\\big]\n\\;\\lesssim\\;\n\\underbrace{T^{-1/2}}_{\\text{actor optimization}}\n+\\underbrace{\\varepsilon_{\\text{critic}}(K,m,R)}_{\\text{TD evaluation}}\n+\\underbrace{\\varepsilon_{\\text{actor}}(N,R)}_{\\text{compatible approx.}}\n+\\underbrace{\\varepsilon_{\\text{inf}}(\\xi)}_{\\text{inference penalty}}.\n\\]\nThe inference error is the price of partial observability: \\[\n\\varepsilon_{\\text{inf}}(\\xi)\n= \\mathbb E\\!\\left[\\sum_{k\\ge0}\\gamma^k\n\\left\\|\\,b_k(\\cdot)-b_0(\\cdot,I_k)\\,\\right\\|_{\\mathrm{TV}}\n\\right],\\qquad I_k=(Y_k,Z_k).\n\\]"
  },
  {
    "objectID": "posts/2025-10-12-hello-quarto.html#sliding-window-controllers-memory-vs.-accuracy",
    "href": "posts/2025-10-12-hello-quarto.html#sliding-window-controllers-memory-vs.-accuracy",
    "title": "Towards Provably Effective Policy Optimization for Large POMDPs",
    "section": "6 Sliding-Window Controllers (Memory vs.Â Accuracy)",
    "text": "6 Sliding-Window Controllers (Memory vs.Â Accuracy)\nUnder stochastic exploration and filter stability/minorization, the inference error decays geometrically with window length \\(n\\): \\[\n\\varepsilon_{\\text{inf}}(\\xi)\\;\\le\\;\\frac{1}{1-\\gamma}\\cdot\n\\mathcal O\\!\\Big(\\rho^{\\lfloor n/m_0\\rfloor}\\Big),\n\\] for constants \\(\\rho\\in(0,1)\\) and \\(m_0\\ge1\\). To reach tolerance \\(\\epsilon\\): \\[\nn=\\mathcal O\\!\\big(m_0\\log(1/\\epsilon)\\big).\n\\]"
  },
  {
    "objectID": "posts/2025-10-12-hello-quarto.html#practical-tuning",
    "href": "posts/2025-10-12-hello-quarto.html#practical-tuning",
    "title": "Towards Provably Effective Policy Optimization for Large POMDPs",
    "section": "7 Practical Tuning",
    "text": "7 Practical Tuning\n\nTD horizon \\(m\\): choose \\(m=\\Theta(\\log_{1/\\gamma}(1/\\epsilon))\\) to make aliasing \\(\\lesssim\\epsilon\\).\nCritic steps \\(K\\) and actor inner steps \\(N\\): about \\(\\Theta(\\epsilon^{-4})\\).\nFeatures: richer features shrink the compatible approximation term.\nWindow \\(n\\): grows like \\(\\mathcal O(\\log(1/\\epsilon))\\) under filter stability.\n\n\n\n7.1 Reference (already in your references.bib)\n```bibtex (article?){cayci2024nac, title = {Finite-Time Analysis of Natural Actor-Critic for POMDPs}, author = {Cayci, Semih and He, Niao and Srikant, R.}, journal = {SIAM Journal on Mathematics of Data Science}, year = {2024} }"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome! This is your Quarto site on GitHub Pages.\n\nWrite math with $e^{i\\pi}+1=0$ or display blocks:\n\n\\[\n\\int_0^1 x^2\\,dx = \\frac{1}{3}.\n\\]\n\nAdd citations by including a references.bib file and putting bibliography: references.bib in the post front matter.\n\nðŸ‘‰ New posts go in the posts/ folder as .qmd files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTowards Provably Effective Policy Optimization for Large POMDPs\n\n\nNatural actorâ€“critic for POMDPs.\n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers\n\n\nNatural actorâ€“critic with finite-state controllers for POMDPs.\n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "All Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nOct 12, 2025\n\n\nTowards Provably Effective Policy Optimization for Large POMDPs\n\n\nÂ \n\n\n\n\n\n\nOct 12, 2025\n\n\nNatural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers\n\n\nÂ \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nac-for-pomdps.html",
    "href": "posts/nac-for-pomdps.html",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "",
    "text": "TL;DR. In (Cayci, He, and Srikant 2024), we studied a natural actorâ€“critic (NAC) method in POMDPs. It learns finite-memory policies, evaluates them with an \\(m\\)-step TD critic to reduce perceptual aliasing, and gives a performance bound that separates optimization/approximation, evaluation, and inference (partial observability) errors. For sliding-window controllers, the inference error decays geometrically with the window size under certain ergodicity conditions."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#formal-pomdp-model",
    "href": "posts/nac-for-pomdps.html#formal-pomdp-model",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "1 Formal POMDP model",
    "text": "1 Formal POMDP model\nWe consider a discounted POMDP \\[\n\\mathcal{M}=\\big(\\mathcal S,\\mathcal A,\\mathcal Y,\\; P,\\; O,\\; r,\\; \\gamma,\\; \\xi\\big),\n\\] with: - latent state \\(S_k\\in\\mathcal S\\), - action \\(A_k\\in\\mathcal A\\), - observation \\(Y_k\\in\\mathcal Y\\), - transition kernel \\(P(s' \\mid s,a)\\), - observation kernel \\(\\Phi(y \\mid s')\\) (can depend on \\(a\\) in general), - reward \\(r(s,a)\\in[0,1]\\) (w.l.o.g.), - discount \\(\\gamma\\in(0,1)\\) and prior \\(\\xi\\) over \\(S_0\\). For the sake of simplicity, let us consider finite \\(\\mathcal S, \\mathcal Y, \\mathcal A\\). The generative process is \\[\nS_{k+1}\\sim P(\\cdot\\mid S_k,A_k),\\qquad\nY_{k}\\sim \\Phi(\\cdot\\mid S_k),\\qquad\nR_k=r(S_k,A_k),\n\\] and the goal is to maximize \\[\n\\mathcal V^\\pi(\\xi)=\\mathbb E_\\xi^\\pi\\!\\left[\\sum_{k\\ge0}\\gamma^k\\,r(S_k,A_k)\\right].\n\\]\nWhy challenging?\nPartial observability implies that the optimal policy \\(\\pi^\\star=(\\pi_0^\\star,\\pi_1^\\star,\\ldots)\\) is non-stationary, therefore \\(\\pi_k^\\star\\) depends on the complete trajectory \\((Y_{0:k},A_{0:k-1})\\) for each \\(k\\in\\mathbb N\\) via the Bayes belief \\(b_k(s)=\\Pr(S_k=s\\mid H_k)\\). A classical result that is attributed to [astrom1965] shows that a POMDP can be formulated as an MDP with a distribution-valued state \\(b_k\\). The paper sidesteps this by learning finite-memory controllers and using \\(m\\)-step TD to reduce aliasing."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#setting-and-goal",
    "href": "posts/nac-for-pomdps.html#setting-and-goal",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "2 Setting and Goal",
    "text": "2 Setting and Goal\nWe restrict policies to finite-state controllers (FSCs) that keep internal memory \\(Z_k\\) and act from \\((Y_k, Z_k)\\).\n\nLearn the best policy within a fixed FSC class \\(\\Pi_{Z,\\varphi}\\):\n\\[\n\\pi^\\star \\in \\arg\\max_{\\pi \\in \\Pi_{Z,\\varphi}} \\; \\mathcal V^\\pi(\\xi).\n\\]\nA useful subclass is the sliding-window controller (SWC) with window \\(n\\):\n\\[\nZ_k = (Y_{k-n:k-1},\\; A_{k-n:k-1}) \\in \\mathcal Y^n \\times \\mathcal A^n,\n\\]\nwhich summarizes the last \\(n\\) observations/actions.\n\nThe actor uses a linear-softmax FSC: \\[\n\\pi_\\theta(a \\mid y,z) =\n\\frac{\\exp\\{\\theta^\\top \\psi(a,y,z)\\}}\n{\\sum_{a'} \\exp\\{\\theta^\\top \\psi(a',y,z)\\}},\n\\] with features \\(\\psi(a,y,z)\\in\\mathbb R^d\\). The controller memory updates via some \\(\\varphi\\): \\[\nZ_{k+1}=\\varphi(Z_k, Y_{k+1}, A_k).\n\\]"
  },
  {
    "objectID": "posts/nac-for-pomdps.html#sampling-measure-for-analysis",
    "href": "posts/nac-for-pomdps.html#sampling-measure-for-analysis",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "3 Sampling Measure (for Analysis)",
    "text": "3 Sampling Measure (for Analysis)\nLet the discounted visitation over information states be \\[\nd^\\pi_\\xi(y,z)=(1-\\gamma)\\sum_{k\\ge0}\\gamma^k\\,\n\\Pr^\\pi\\!\\big[(Y_k,Z_k)=(y,z)\\,\\big|\\,H_0\\sim\\xi\\big].\n\\] The analysis assumes access to samples from \\(d^\\pi_\\xi\\) (standard in NAC to match the Fisher metric)."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#algorithm-sketch-fs-nac",
    "href": "posts/nac-for-pomdps.html#algorithm-sketch-fs-nac",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "4 Algorithm Sketch (FS-NAC)",
    "text": "4 Algorithm Sketch (FS-NAC)\n\nCritic â€” run \\(m\\)-step TD(0) with linear function approximation to estimate \\(\\widehat{\\mathcal Q}^{\\pi_t}(y,z,a)\\).\nActor â€” take a natural-gradient step using the criticâ€™s advantage estimate.\n\n\n\n\n\n\n\nTip\n\n\n\nWhy \\(m\\)-step TD? In POMDPs, one-step bootstrapping suffers from perceptual aliasing (same observation \\(Y_k\\), different latent state \\(S_k\\)). Looking \\(m\\) steps ahead stabilizes the target and reduces this bias."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#critic-guarantee-policy-evaluation",
    "href": "posts/nac-for-pomdps.html#critic-guarantee-policy-evaluation",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "5 Critic Guarantee (Policy Evaluation)",
    "text": "5 Critic Guarantee (Policy Evaluation)\nWith stepsize \\(\\alpha=K^{-1/2}\\) and parameter radius \\(R\\), after \\(K\\) critic updates: \\[\n\\mathbb E\\!\\left[\\big\\|\\mathcal Q^\\pi-\\widehat{\\mathcal Q}^\\pi_K\\big\\|_{2,\\,d^\\pi_\\xi\\otimes\\pi}\\right]\n\\;\\lesssim\\;\n\\underbrace{\\tfrac{K^{-1/4}}{1-\\gamma}}_{\\text{statistical}}\n+\\underbrace{\\tfrac{\\varepsilon_{\\text{app}}(R)}{1-\\gamma^m}}_{\\text{approximation}}\n+\\underbrace{\\varepsilon_{\\text{pa}}(\\gamma,m,R)}_{\\text{aliasing}},\n\\] where \\(\\varepsilon_{\\text{app}}(R)\\) is the best linear-approximation error within radius \\(R\\), and the aliasing term contracts geometrically: \\[\n\\varepsilon_{\\text{pa}}(\\gamma,m,R)=\\mathcal O\\!\\big(\\gamma^{m/2}\\,\\mathrm{poly}(R,(1-\\gamma)^{-1})\\big).\n\\]\nSampleâ€“accuracy trade-off. Each update uses \\(m\\) samples, so total evaluation samples are \\(\\Theta(mK)\\)."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#full-actorcritic-performance-finite-time",
    "href": "posts/nac-for-pomdps.html#full-actorcritic-performance-finite-time",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "6 Full Actorâ€“Critic Performance (Finite-Time)",
    "text": "6 Full Actorâ€“Critic Performance (Finite-Time)\nAfter \\(T\\) outer iterations (with tuned stepsizes), \\[\n(1-\\gamma)\\,\\min_{t&lt;T}\\mathbb E\\!\\big[\\mathcal V^{\\pi^\\star}(\\xi)-\\mathcal V^{\\pi_t}(\\xi)\\big]\n\\;\\lesssim\\;\n\\underbrace{T^{-1/2}}_{\\text{actor optimization}}\n+\\underbrace{\\varepsilon_{\\text{critic}}(K,m,R)}_{\\text{TD evaluation}}\n+\\underbrace{\\varepsilon_{\\text{actor}}(N,R)}_{\\text{compatible approx.}}\n+\\underbrace{\\varepsilon_{\\text{inf}}(\\xi)}_{\\text{inference penalty}}.\n\\]\nThe inference error is the price of partial observability: \\[\n\\varepsilon_{\\text{inf}}(\\xi)\n= \\mathbb E\\!\\left[\\sum_{k\\ge0}\\gamma^k\n\\left\\|\\,b_k(\\cdot)-b_0(\\cdot,I_k)\\,\\right\\|_{\\mathrm{TV}}\n\\right],\\qquad I_k=(Y_k,Z_k).\n\\]"
  },
  {
    "objectID": "posts/nac-for-pomdps.html#sliding-window-controllers-memory-vs.-accuracy",
    "href": "posts/nac-for-pomdps.html#sliding-window-controllers-memory-vs.-accuracy",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "7 Sliding-Window Controllers (Memory vs.Â Accuracy)",
    "text": "7 Sliding-Window Controllers (Memory vs.Â Accuracy)\nUnder stochastic exploration and filter stability/minorization, the inference error decays geometrically with window length \\(n\\): \\[\n\\varepsilon_{\\text{inf}}(\\xi)\\;\\le\\;\\frac{1}{1-\\gamma}\\cdot\n\\mathcal O\\!\\Big(\\rho^{\\lfloor n/m_0\\rfloor}\\Big),\n\\] for constants \\(\\rho\\in(0,1)\\) and \\(m_0\\ge1\\). To reach tolerance \\(\\epsilon\\): \\[\nn=\\mathcal O\\!\\big(m_0\\log(1/\\epsilon)\\big).\n\\]"
  },
  {
    "objectID": "posts/nac-for-pomdps.html#practical-tuning",
    "href": "posts/nac-for-pomdps.html#practical-tuning",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "8 Practical Tuning",
    "text": "8 Practical Tuning\n\nTD horizon \\(m\\): choose \\(m=\\Theta(\\log_{1/\\gamma}(1/\\epsilon))\\) to make aliasing \\(\\lesssim\\epsilon\\).\nCritic steps \\(K\\) and actor inner steps \\(N\\): about \\(\\Theta(\\epsilon^{-4})\\).\nFeatures: richer features shrink the compatible approximation term.\nWindow \\(n\\): grows like \\(\\mathcal O(\\log(1/\\epsilon))\\) under filter stability."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#whats-next-rnn-based-nac-for-pomdps",
    "href": "posts/nac-for-pomdps.html#whats-next-rnn-based-nac-for-pomdps",
    "title": "Natural Policy Gradient for POMDPs â€“ Part I: Finite-State Controllers",
    "section": "9 Whatâ€™s next: RNN-based NAC for POMDPs",
    "text": "9 Whatâ€™s next: RNN-based NAC for POMDPs\nA natural extension replaces the hand-engineered memory \\(Z_k\\) with a recurrent hidden state \\(H_k\\) learned end-to-end (RNN-based NAC). The policy becomes \\(\\pi_\\theta(a\\mid Y_k, H_k)\\) with \\(H_{k+1}=f_\\theta(H_k,Y_{k+1},A_k)\\), and the natural-gradient machinery is adapted to the recurrent parameterization. Iâ€™ll cover how this compares to FSCs (sample complexity vs.Â representation power, and how partial-observability error shows up) in a follow-up post about our RNN-based natural actorâ€“critic paper."
  }
]