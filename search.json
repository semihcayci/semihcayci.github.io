[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Semih Cayci, a math enthusiast working on theoretical ML problems."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nOct 12, 2025\n\n\nRL for POMDPs – Part I: TD Learning with Internal States\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Theoretical RL",
    "section": "",
    "text": "Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRL for POMDPs – Part I: TD Learning with Internal States\n\n\nHow does multi-step TD learning with internal states mitigate partial observability?\n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html",
    "href": "posts/td-learning-for-pomdps.html",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "",
    "text": "TL;DR. A classic result by (Singh, Jaakkola, and Jordan 1994) shows that memoryless TD learning fails in POMDPs. To address this, we studied a natural actor–critic (NAC) method for POMDPs in (Cayci, He, and Srikant 2024), with a critic that uses \\(m\\)-step TD learning with an internal state to mitigate perceptual aliasing problem. Below, I review this algorithm and provide some follow-up results."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#formal-pomdp-model",
    "href": "posts/td-learning-for-pomdps.html#formal-pomdp-model",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "1 Formal POMDP model",
    "text": "1 Formal POMDP model\nLet us consider a discounted POMDP \\[\n\\mathcal{M}=\\big(\\mathcal S,\\mathcal A,\\mathcal Y,\\; P,\\; O,\\; r,\\; \\gamma\\big),\n\\]\nwith:\n\nstate \\(S_k\\in\\mathcal S\\),\naction \\(A_k\\in\\mathcal A\\),\nobservation \\(Y_k\\in\\mathcal Y\\),\ntransition kernel \\(P(s' \\mid s,a)\\),\nobservation kernel \\(\\Phi(y \\mid s')\\),\nreward \\(r(s,a)\\in[0,1]\\) (w.l.o.g.),\ndiscount factor \\(\\gamma\\in(0,1)\\).\n\nTo avoid measure-theoretic concerns, let us consider finite \\(\\mathcal S, \\mathcal Y, \\mathcal A\\). These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is \\[\nS_{k+1}\\sim P(\\cdot\\mid S_k,A_k),\\qquad\nY_{k}\\sim \\Phi(\\cdot\\mid S_k),\\qquad\nR_k=r(S_k,A_k),\n\\] and the goal is to maximize \\[\n\\mathcal V^\\pi(\\xi)=\\mathbb E_\\xi^\\pi\\!\\left[\\sum_{k\\ge0}\\gamma^k\\,r(S_k,A_k)\\right].\n\\]\nWhy challenging?\nPartial observability implies that the optimal policy \\(\\pi^\\star=(\\pi_0^\\star,\\pi_1^\\star,\\ldots)\\) is non-stationary, therefore \\(\\pi_k^\\star\\) depends on the complete trajectory \\((Y_{0:k},A_{0:k-1})\\) for each \\(k\\in\\mathbb N\\) via the Bayes belief \\(b_k(s)=\\Pr(S_k=s\\mid Y_{0:k},A_{0:k-1})\\). Reactive (i.e., memoryless) methods for MDPs fail miserably if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see Example 1 below). A classical result attributed to (Åström 1965) (see (Yüksel 2025) for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state \\(b_k\\). However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over \\(\\mathcal S\\)), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies (Murphy 2000) and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state \\(Z_k^c\\), which summarizes/compresses \\((Y_{0:k-1},A_{0:k-1})\\) for each \\(k\\). The idea is to use a parametric policy \\(\\pi_k(\\cdot\\mid Y_k, Z_k^c)\\) based on the internal state \\(Z_k^c\\)."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#setting-and-goal",
    "href": "posts/td-learning-for-pomdps.html#setting-and-goal",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "2 Setting and Goal",
    "text": "2 Setting and Goal\nAt first, we restrict policies to finite-state controllers (FSCs) that keep internal memory \\(Z_k^c\\) and act from \\((Y_k, Z_k^c)\\). The internal memory is updated in a Markovian way: \\(Z_{k+1}^c~\\varphi(\\cdot\\mid z_k, y_k, a_k)\\) given \\(Z_k^c=z_k,Y_k=y_k,A_k=a_k\\).\n\nLearn the best policy within a fixed FSC class \\(\\pi_{Z,\\varphi}\\):\n\\[\n\\pi^\\star \\in \\arg\\max_{\\pi \\in \\pi_{Z,\\varphi}} \\; \\mathcal V^\\pi(\\xi).\n\\]\nA useful subclass is the sliding-window controller (SWC) with window \\(n\\):\n\\[\nZ_k^c = (Y_{k-n:k-1},\\; A_{k-n:k-1}) \\in \\mathcal Y^n \\times \\mathcal A^n,\n\\]\nwhich summarizes the last \\(n\\) observations/actions, where \\(n\\) is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.\n\nThe actor uses a linear-softmax FSC: \\[\n\\pi_\\theta(a \\mid y,z) =\n\\frac{\\exp\\{\\theta^\\top \\psi(a,y,z)\\}}\n{\\sum_{a'} \\exp\\{\\theta^\\top \\psi(a',y,z)\\}},\n\\] with features \\(\\psi(a,y,z)\\in\\mathbb R^d\\). The controller memory updates via some \\(\\varphi\\): \\[\nZ_{k+1}^c=\\varphi(Z_k^c, Y_{k+1}, A_k).\n\\]"
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#hidden-markov-reward-processes-hmrp",
    "href": "posts/td-learning-for-pomdps.html#hidden-markov-reward-processes-hmrp",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "3 Hidden Markov Reward Processes (HMRP)",
    "text": "3 Hidden Markov Reward Processes (HMRP)\nA common simplification in TD learning analyses in an MDP setting is to consider a Markov reward process (MRP). Since the augmented state \\(X_k:=(S_k,A_k)\\) under a given stationary policy forms a Markov chain, the analyses automatically extend to policy evaluation (known as SARSA) since \\((X_k,r(X_k))\\) forms an MRP.\nIn POMDPs, we can make a similar simplification. First, let us consider a Hidden Markov Process with a Markov chain \\(S_k\\) and its partial observation process \\(Y_k\\sim\\Phi(\\cdot\\mid S_k)\\). Along with the reward \\(r(S_k)\\), this HMP induces a Hidden Markov Reward Process (HMRP) \\(\\{(S_k,r(S_k),Y_k):k\\in\\mathbb{N}\\}\\).\nConsider a finite-state controller with an internal state \\(Z_k^c\\). Then, we have \\(A_k \\sim \\pi_k(\\cdot\\mid Z_k^c)\\), thus \\(X_k:=(S_k,A_k,Z_k^c)\\) forms a Markov process with the transition kernel \\[P(S_{k+1}=s',A_{k+1}=a',Z_{k+1}^c=z'|S_k,A_k,Z_k^c) = \\sum_{y'\\in\\mathcal Y}P(s'|S_k,A_k)\\Phi(y'|s')\\varphi(z'|Z_k^c,y',A_k)\\pi_{k+1}(a'\\mid y',z').\\] The noisy observation is \\(Y_k~\\Phi(\\cdot|S_k)\\). Defining \\(\\tilde{r}(S_k,A_k,Z_k^c) = r(S_k,A_k)\\), we obtain the original process under \\(\\pi\\). Thus, the controlled process under \\(\\pi\\) induces the HMRP \\((X_k, \\tilde{r}(X_k), Y_k)\\)."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#critic-m-step-td-learning-with-an-internal-state",
    "href": "posts/td-learning-for-pomdps.html#critic-m-step-td-learning-with-an-internal-state",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "4 Critic: \\(m\\)-step TD Learning with an Internal State",
    "text": "4 Critic: \\(m\\)-step TD Learning with an Internal State\nIt is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology (Singh, Jaakkola, and Jordan 1994). For a concrete demonstration, let us focus on learning the value function in an HMRP: \\[V(z_0):=\\mathbb E[\\sum_{k=0}^\\infty \\gamma^k r(S_k)|Z_0^c=z_0]\\mbox{ and }V_s^\\star:=\\mathbb E[\\sum_{k=0}^\\infty r(S_k)|S_0=s]\\] for an internal state \\(Z_t^c\\) (e.g., \\(Z_t^c = (Y_{t-n+1},\\ldots,Y_t)\\)), where \\((S_k, r(S_k), Y_k)\\) is an HMRP.\n\n\n\n\n\n\nTip\n\n\n\nTwo objectives. In the above display, if we want to perform policy evaluation for an FSC with \\(\\{Z_k^c:k\\in\\mathbb N\\}\\), we would be interested in \\(V(z_0)\\). Additionally, we can use \\(m\\)-step TD Learning for estimating the value function \\(V_s^\\star\\). We discuss both problems in what follows.\n\n\nExample 1 (When TD(0) fails). Here we consider an HMRP with \\(\\mathcal S = \\{0,1\\}\\), \\(r(s) = s\\) and \\(\\Phi(1|s) = 1\\) for all \\(s\\in\\mathcal{S}\\). Let \\(P(0|1) = p\\) and \\(P(1|0) = q\\) for some \\(p, q \\in (0,1)\\), and \\(\\gamma \\in (0, 1)\\) be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to \\(v^\\star = \\frac{1}{1-\\gamma}\\cdot \\frac{p}{q+p} = V_0^\\star + \\frac{p}{p+q}(V_1^\\star-V_0^\\star)\\). On the other hand, the value function for a given internal state \\(z_0\\) is \\[V(z_0) = V_0^\\star + (V_1^\\star-V_0^\\star)P(S_0=1|Z_0^c=z_0) = \\frac{\\gamma p + (1-\\gamma)P(S_0=1|Z_0=z_0)}{(1-\\gamma)\\Big[\\gamma(p+q)+1-\\gamma\\Big]}\\neq v^\\star,\\] since \\(V(z_0) = V_0^\\star+(V_1^\\star-V_0^\\star)P(S_0=1|Z_0^c=z_0)\\). Also, \\(V_{S_0}^\\star = V_0^\\star + (V_1^\\star-V_0^\\star)\\mathbf{1}_{\\{S_0=1\\}}\\). Therefore, \\[\\mathbb{E}[(V_{S_0}^\\star-v^\\star)^2] = (V_1^\\star-V_0^\\star)^2\\mathbb{E}[(\\frac{p}{p+q}-\\mathbf{1}_{\\{S_0=1\\}})^2]=Var(\\mathbf{1}_{\\{S_0=1\\}})(V_1^\\star-V_0^\\star)^2.\\]\n\n\n\n\n\n\nTip\n\n\n\nWhy \\(m\\)-step TD? In POMDPs, one-step bootstrapping suffers from perceptual aliasing (same observation \\(Y_k\\), different latent state \\(S_k\\)). Looking \\(m\\) steps ahead stabilizes the target and reduces this bias.\n\n\nNow, let us focus on \\(m\\)-step TD learning with an internal state \\(Z_k\\) for the above problem, where \\(m\\in\\mathbb{Z}_+\\). Note that the internal state may match the controller state \\(Z_k^c\\), or it can be used to obtain an estimate of \\(V_s^\\star\\).\nLet us focus on the limit point. To that end, let \\[(T_mv)(z) = \\bar{r}_m(z) + \\gamma^m(K_mv)(z),\\] where \\(\\bar{r}_m(z) := \\mathbb E[\\sum_{k=0}^{m-1}r(S_k)|Z_0=z]\\) and \\((K_mv)(z):=\\mathbb{E}[v(Z_m)|Z_0=z]\\). Then, it is straightforward to show that \\(T_m\\) is a contractive operator with modulus \\(\\gamma^m\\), and admits a unique fixed point \\[V(z) = \\mathbb E[\\sum_{k=0}^\\infty \\gamma^k r(S_k)|Z_0=z] = \\sum_{k=0}^\\infty (\\gamma^mK_m)^k\\bar{r}_m(z)\\] for any \\(m\\in\\mathbb N\\). Let \\(\\xi\\) be the invariant distribution of \\(\\{Z_k:k\\in\\mathbb N\\}\\): for an SWC of window-size \\(n\\), it is \\[\\xi(z_t)=\\sum_{s_{t-n+1:t}}\\prod_{k=0}^{n-1}\\Phi(y_{t-k}|s_{t-k})\\mu(s_{t-k}),\\] where \\(\\mu\\) is the stationary distribution of \\(\\{S_k:k\\in\\mathbb N\\}\\). Under a non-degenerate \\(\\mathbb{E}_{Z_t\\sim \\xi}[\\psi(Z_t)\\otimes\\psi(Z_t)]\\succ 0\\), \\(m\\)-step TD learning with the internal state \\(Z_t\\) converges to the unique fixed point \\(v_m\\) of the equation \\[v = \\Pi\\{\\bar{r}_m+\\gamma^m K_m v\\},\\] where \\(\\Pi\\) be the ortogonal projection onto the subspace \\(\\mathcal V := \\{z\\mapsto \\langle \\psi(z),w\\rangle: w \\in \\mathbb R^d\\) with respect to \\(L^2(\\xi)\\). Thus, one obtains \\[\\|v_m - \\Pi V\\|_{L^2(\\xi)} \\leq \\frac{\\gamma^m}{1+\\gamma^m}\\|V-\\Pi V\\|_{L^2(\\xi)}.\\] As such, in case we are interested in policy evaluation for an FSC, this formulation implies the near optimality of \\(m\\)-step TD learning, which uses the internal state of the controller as the critic internal state, with an additional error term \\(\\mathcal{O}(\\gamma^m)\\).\nThe analysis can be taken one step further indeed. The argument below can be shifted in time, so let us consider \\(t=0\\) for notational simplicity. Let \\(\\mathcal{F}_m :=\\sigma(Z_0)=\\sigma(Y_{-m+1},\\ldots,Y_0)\\) be the \\(\\sigma\\)-algebra generated by the internal state, and \\(b^{(m)} := P(S_0 = 1|\\mathcal{F}_m)\\). Then, \\(V(z_0) =  \\mathbb E[\\sum_{t=0}^\\infty\\gamma^tr(S_t)|\\mathcal{F}_m]\\). Since \\(V_s^\\star = V_0^\\star + (V_1^\\star-V_0^\\star)\\mathbf{1}_{\\{S_0=s\\}}\\) and \\(V(z_0) = V_0^\\star + (V_1^\\star-V_0^\\star)b^{(m)}\\), we obtain \\[\\mathbb{E}[(V_{S_0}^\\star-V(z_0))^2] = (V_1^\\star-V_0^\\star)^2\\,\\mathbb E\\!\\left[(\\mathbf{1}_{\\{S_0=1\\}}-b^{(m)})^2\\right].\\] Furthermore, since \\(b^{(m)}=\\mathbb E[\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_m]\\), we have\n\\[\\begin{align}\n\\mathbb E\\!\\left[(\\mathbf{1}_{\\{S_0=1\\}}-b^{(m)})^2\\right]&=\\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_m)]\\\\\n&\\downarrow \\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_\\infty)]\n\\end{align}\\] as \\(m\\rightarrow\\infty\\) due to the law of total variance for nested \\(\\sigma\\)-algebras \\(\\mathcal{F}_m\\). Therefore, \\[\\mathbb{E}[(V_{s_0}^\\star-V(z_0))^2] \\downarrow (V_1^\\star-V_0^\\star)^2\\,\\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_\\infty)].\\] If \\(S_0\\) is measurable with respect to \\(\\sigma(Y_{-\\infty},\\ldots,Y_0)\\), we have \\(V_{s_0}^\\star=V(z_0))\\) almost surely. Used in conjunction with the previous result on \\(v_m-V(z_0)\\), a simple triangle inequality would demonstrate the effectiveness of TD learning with sliding-window controllers.\nIn summary, we have \\[\\begin{align*}\n  \\mathbb{E}[(V_{S_0}^\\star - v^\\star)^2] &= (V_1^\\star-V_0^\\star)^2 Var(\\mathbf{1}_{\\{S_0=1\\}})\\\\\n&\\geq (V_1^\\star-V_0^\\star)^2\\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_m)] \\downarrow (V_1^\\star-V_0^\\star)^2\\mathbb E[Var(\\mathbf{1}_{\\{S_0=1\\}}|\\mathcal F_\\infty)].\n\\end{align*}\\]\nWith stepsize \\(\\alpha=K^{-1/2}\\) and parameter radius \\(R\\), after \\(K\\) critic updates: \\[\n\\sqrt{\\mathbb E\\!\\left[\\big\\|\\mathcal Q^\\pi-\\widehat{\\mathcal Q}^\\pi_K\\big\\|^2_{\\,d^\\pi_\\xi\\otimes\\pi}\\right]}\n\\;\\lesssim\\;\n\\underbrace{\\tfrac{K^{-1/4}}{1-\\gamma}}_{\\text{statistical}}\n+\\underbrace{\\tfrac{\\varepsilon_{\\text{app}}(R)}{1-\\gamma^m}}_{\\text{approximation}}\n+\\underbrace{\\varepsilon_{\\text{pa}}(\\gamma,m,R)}_{\\text{aliasing}},\n\\] where \\(\\varepsilon_{\\text{app}}(R)\\) is the best linear-approximation error within radius \\(R\\), and the aliasing term contracts geometrically: \\[\n\\varepsilon_{\\text{pa}}(\\gamma,m,R)=\\mathcal O\\!\\big(\\gamma^{m/2}\\,\\mathrm{poly}(R,(1-\\gamma)^{-1})\\big).\n\\]\nSample–accuracy trade-off. Each update uses \\(m\\) samples, so total evaluation samples are \\(\\Theta(mK)\\)."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#full-actorcritic-performance-finite-time",
    "href": "posts/td-learning-for-pomdps.html#full-actorcritic-performance-finite-time",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "5 Full Actor–Critic Performance (Finite-Time)",
    "text": "5 Full Actor–Critic Performance (Finite-Time)\nAfter \\(T\\) outer iterations (with tuned stepsizes), \\[\n(1-\\gamma)\\,\\min_{t&lt;T}\\mathbb E\\!\\big[\\mathcal V^{\\pi^\\star}(\\xi)-\\mathcal V^{\\pi_t}(\\xi)\\big]\n\\;\\lesssim\\;\n\\underbrace{T^{-1/2}}_{\\text{actor optimization}}\n+\\underbrace{\\varepsilon_{\\text{critic}}(K,m,R)}_{\\text{TD evaluation}}\n+\\underbrace{\\varepsilon_{\\text{actor}}(N,R)}_{\\text{compatible approx.}}\n+\\underbrace{\\varepsilon_{\\text{inf}}(\\xi)}_{\\text{inference penalty}}.\n\\]\nThe inference error is the price of partial observability: \\[\n\\varepsilon_{\\text{inf}}(\\xi)\n= \\mathbb E\\!\\left[\\sum_{k\\ge0}\\gamma^k\n\\left\\|\\,b_k(\\cdot)-b_0(\\cdot,I_k)\\,\\right\\|_{\\mathrm{TV}}\n\\right],\\qquad I_k=(Y_k,Z_k).\n\\]"
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#sliding-window-controllers-memory-vs.-accuracy",
    "href": "posts/td-learning-for-pomdps.html#sliding-window-controllers-memory-vs.-accuracy",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "6 Sliding-Window Controllers (Memory vs. Accuracy)",
    "text": "6 Sliding-Window Controllers (Memory vs. Accuracy)\nUnder stochastic exploration and filter stability/minorization, the inference error decays geometrically with window length \\(n\\): \\[\n\\varepsilon_{\\text{inf}}(\\xi)\\;\\le\\;\\frac{1}{1-\\gamma}\\cdot\n\\mathcal O\\!\\Big(\\rho^{\\lfloor n/m_0\\rfloor}\\Big),\n\\] for constants \\(\\rho\\in(0,1)\\) and \\(m_0\\ge1\\). To reach tolerance \\(\\epsilon\\): \\[\nn=\\mathcal O\\!\\big(m_0\\log(1/\\epsilon)\\big).\n\\]"
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#practical-tuning",
    "href": "posts/td-learning-for-pomdps.html#practical-tuning",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "7 Practical Tuning",
    "text": "7 Practical Tuning\n\nTD horizon \\(m\\): choose \\(m=\\Theta(\\log_{1/\\gamma}(1/\\epsilon))\\) to make aliasing \\(\\lesssim\\epsilon\\).\nCritic steps \\(K\\) and actor inner steps \\(N\\): about \\(\\Theta(\\epsilon^{-4})\\).\nFeatures: richer features shrink the compatible approximation term.\nWindow \\(n\\): grows like \\(\\mathcal O(\\log(1/\\epsilon))\\) under filter stability."
  },
  {
    "objectID": "posts/td-learning-for-pomdps.html#whats-next-rnn-based-nac-for-pomdps",
    "href": "posts/td-learning-for-pomdps.html#whats-next-rnn-based-nac-for-pomdps",
    "title": "RL for POMDPs – Part I: TD Learning with Internal States",
    "section": "8 What’s next: RNN-based NAC for POMDPs",
    "text": "8 What’s next: RNN-based NAC for POMDPs\nA natural extension replaces the hand-engineered memory \\(Z_k\\) with a recurrent hidden state \\(H_k\\) learned end-to-end (RNN-based NAC). The policy becomes \\(\\pi_\\theta(a\\mid Y_k, H_k)\\) with \\(H_{k+1}=f_\\theta(H_k,Y_{k+1},A_k)\\), and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs. representation power, and how partial-observability error shows up) in a follow-up post about our RNN-based natural actor–critic paper."
  }
]