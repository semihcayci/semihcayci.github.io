[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m YOUR_NAME. This site is built with Quarto."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "All Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nOct 12, 2025\n\n\nNatural Policy Gradient for POMDPs – Part I: Finite-State Controllers\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "On this page, I will share some of my recent research interests in an informal way. For any questions, you can contact me by email.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Policy Gradient for POMDPs – Part I: Finite-State Controllers\n\n\nNatural actor–critic with finite-state controllers for POMDPs.\n\n\n\n\n\nOct 12, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nac-for-pomdps.html",
    "href": "posts/nac-for-pomdps.html",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "",
    "text": "TL;DR. In (Cayci, He, and Srikant 2024), we studied a natural actor–critic (NAC) method in POMDPs. Critic uses \\(m\\)-step TD learning to mitigate perceptual aliasing and the actor uses an internal state to incorporate memory. We establish non-asymptotic bounds for optimization/approximation, evaluation, and inference (partial observability) errors. For sliding-window controllers, the inference error decays geometrically with the window size under certain ergodicity conditions."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#formal-pomdp-model",
    "href": "posts/nac-for-pomdps.html#formal-pomdp-model",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "1 Formal POMDP model",
    "text": "1 Formal POMDP model\nLet us consider a discounted POMDP \\[\n\\mathcal{M}=\\big(\\mathcal S,\\mathcal A,\\mathcal Y,\\; P,\\; O,\\; r,\\; \\gamma\\big),\n\\] with: - latent state \\(S_k\\in\\mathcal S\\), - action \\(A_k\\in\\mathcal A\\), - observation \\(Y_k\\in\\mathcal Y\\), - transition kernel \\(P(s' \\mid s,a)\\), - observation kernel \\(\\Phi(y \\mid s')\\) (can depend on \\(a\\) in general), - reward \\(r(s,a)\\in[0,1]\\) (w.l.o.g.), - discount factor \\(\\gamma\\in(0,1)\\). To avoid measure-theoretic concerns, let us consider finite \\(\\mathcal S, \\mathcal Y, \\mathcal A\\). These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is \\[\nS_{k+1}\\sim P(\\cdot\\mid S_k,A_k),\\qquad\nY_{k}\\sim \\Phi(\\cdot\\mid S_k),\\qquad\nR_k=r(S_k,A_k),\n\\] and the goal is to maximize \\[\n\\mathcal V^\\pi(\\xi)=\\mathbb E_\\xi^\\pi\\!\\left[\\sum_{k\\ge0}\\gamma^k\\,r(S_k,A_k)\\right].\n\\]\nWhy challenging?\nPartial observability implies that the optimal policy \\(\\pi^\\star=(\\pi_0^\\star,\\pi_1^\\star,\\ldots)\\) is non-stationary, therefore \\(\\pi_k^\\star\\) depends on the complete trajectory \\((Y_{0:k},A_{0:k-1})\\) for each \\(k\\in\\mathbb N\\) via the Bayes belief \\(b_k(s)=\\Pr(S_k=s\\mid H_k)\\). A classical result attributed to (Åström 1965) (see (Yüksel 2025) for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state \\(b_k\\). However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over \\(\\mathcal S\\)), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies (Murphy 2000) and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state \\(Z_k\\), which summarizes/compresses \\((Y_{0:k-1},A_{0:k-1})\\) for each \\(k\\). The idea is to use a parametric policy \\(\\pi_k(\\cdot\\mid Y_k, Z_k)\\) based on the internal state \\(Z_k\\)."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#setting-and-goal",
    "href": "posts/nac-for-pomdps.html#setting-and-goal",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "2 Setting and Goal",
    "text": "2 Setting and Goal\nAt first, we restrict policies to finite-state controllers (FSCs) that keep internal memory \\(Z_k\\) and act from \\((Y_k, Z_k)\\). The internal memory is updated in a Markovian way: \\(Z_{k+1}~\\varphi(\\cdot\\mid z_k, y_k, a_k)\\) given \\(Z_k=z_k,Y_k=y_k,A_k=a_k\\).\n\nLearn the best policy within a fixed FSC class \\(\\Pi_{Z,\\varphi}\\):\n\\[\n\\pi^\\star \\in \\arg\\max_{\\pi \\in \\Pi_{Z,\\varphi}} \\; \\mathcal V^\\pi(\\xi).\n\\]\nA useful subclass is the sliding-window controller (SWC) with window \\(n\\):\n\\[\nZ_k = (Y_{k-n:k-1},\\; A_{k-n:k-1}) \\in \\mathcal Y^n \\times \\mathcal A^n,\n\\]\nwhich summarizes the last \\(n\\) observations/actions, where \\(n\\) is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.\n\nThe actor uses a linear-softmax FSC: \\[\n\\pi_\\theta(a \\mid y,z) =\n\\frac{\\exp\\{\\theta^\\top \\psi(a,y,z)\\}}\n{\\sum_{a'} \\exp\\{\\theta^\\top \\psi(a',y,z)\\}},\n\\] with features \\(\\psi(a,y,z)\\in\\mathbb R^d\\). The controller memory updates via some \\(\\varphi\\): \\[\nZ_{k+1}=\\varphi(Z_k, Y_{k+1}, A_k).\n\\]"
  },
  {
    "objectID": "posts/nac-for-pomdps.html#sampling-measure",
    "href": "posts/nac-for-pomdps.html#sampling-measure",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "3 Sampling Measure",
    "text": "3 Sampling Measure\nLet the discounted visitation distribution over information states be \\[\nd^\\pi_\\xi(y,z)=(1-\\gamma)\\sum_{k\\ge0}\\gamma^k\\,\nP^\\pi\\!\\big[(Y_k,Z_k)=(y,z)\\,\\big|\\,H_0\\sim\\xi\\big].\n\\]"
  },
  {
    "objectID": "posts/nac-for-pomdps.html#algorithm-sketch-fs-nac",
    "href": "posts/nac-for-pomdps.html#algorithm-sketch-fs-nac",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "4 Algorithm Sketch (FS-NAC)",
    "text": "4 Algorithm Sketch (FS-NAC)\n\nCritic — run \\(m\\)-step TD(0) with linear function approximation to estimate \\(\\widehat{\\mathcal Q}^{\\pi_t}(y,z,a)\\).\nActor — take a natural-gradient step using the critic’s advantage estimate."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#critic-m-step-td-learning-with-memory",
    "href": "posts/nac-for-pomdps.html#critic-m-step-td-learning-with-memory",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "5 Critic: \\(m\\)-step TD Learning with Memory",
    "text": "5 Critic: \\(m\\)-step TD Learning with Memory\n\n\n\n\n\n\nTip\n\n\n\nWhy \\(m\\)-step TD? In POMDPs, one-step bootstrapping suffers from perceptual aliasing (same observation \\(Y_k\\), different latent state \\(S_k\\)). Looking \\(m\\) steps ahead stabilizes the target and reduces this bias.\n\n\nIt is very easy to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a process with memory (Singh, Jaakkola, and Jordan 1994). Here, we want to compute \\(V(z_0):=\\mathbb E[\\sum_{k=0}^\\infty \\gamma^k r(S_k)|Z_0=z_0]\\) for an internal state \\(Z_t\\) (e.g., \\(Z_t = (Y_{t-m+1},\\ldots,Y_t)\\)), where \\((S_k, r(S_k), Y_k)\\) is a hidden Markov reward process.\nExample Consider a (hidden) Markov reward process with \\(\\mathcal S = \\{0,1\\}\\), \\(r(s) = s\\) and \\(\\Phi(1|s) = 1\\) for all \\(s\\in\\mathcal{S}\\). Let \\(P(0|1) = p\\) and \\(P(1|0) = q\\) for some \\(p, q \\in (0,1)\\), and \\(\\gamma \\in (0, 1)\\) be the discount factor. Then, TD(0) converges to \\(v^\\star = \\frac{1}{1-\\gamma}\\cdot \\frac{p}{q+p}\\). On the other hand, the value function for a given initial internal state \\(Z_0=z_0\\) is \\(V(z_0) = \\frac{\\gamma p + (1-\\gamma)P(S_0=1|Z_0=z_0)}{(1-\\gamma)\\Big[\\gamma(p+q)+1-\\gamma\\Big]}\\). As such, memoryless TD learning cannot learn \\(V(z_0)\\).\nLet \\(\\bar{r}_m(z) := \\mathbb E[\\sum_{k=0}^{m-1}r(S_k)|Z_0=z], (K_mv)(z):=\\mathbb{E}[v(Z_m)|Z_0=z]\\), and \\((T_mv)(z) = \\bar{r}_m(z) + \\gamma^m(K_mv)(z)\\).\nWith stepsize \\(\\alpha=K^{-1/2}\\) and parameter radius \\(R\\), after \\(K\\) critic updates: \\[\n\\sqrt{\\mathbb E\\!\\left[\\big\\|\\mathcal Q^\\pi-\\widehat{\\mathcal Q}^\\pi_K\\big\\|^2_{\\,d^\\pi_\\xi\\otimes\\pi}\\right]}\n\\;\\lesssim\\;\n\\underbrace{\\tfrac{K^{-1/4}}{1-\\gamma}}_{\\text{statistical}}\n+\\underbrace{\\tfrac{\\varepsilon_{\\text{app}}(R)}{1-\\gamma^m}}_{\\text{approximation}}\n+\\underbrace{\\varepsilon_{\\text{pa}}(\\gamma,m,R)}_{\\text{aliasing}},\n\\] where \\(\\varepsilon_{\\text{app}}(R)\\) is the best linear-approximation error within radius \\(R\\), and the aliasing term contracts geometrically: \\[\n\\varepsilon_{\\text{pa}}(\\gamma,m,R)=\\mathcal O\\!\\big(\\gamma^{m/2}\\,\\mathrm{poly}(R,(1-\\gamma)^{-1})\\big).\n\\]\nSample–accuracy trade-off. Each update uses \\(m\\) samples, so total evaluation samples are \\(\\Theta(mK)\\)."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#full-actorcritic-performance-finite-time",
    "href": "posts/nac-for-pomdps.html#full-actorcritic-performance-finite-time",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "6 Full Actor–Critic Performance (Finite-Time)",
    "text": "6 Full Actor–Critic Performance (Finite-Time)\nAfter \\(T\\) outer iterations (with tuned stepsizes), \\[\n(1-\\gamma)\\,\\min_{t&lt;T}\\mathbb E\\!\\big[\\mathcal V^{\\pi^\\star}(\\xi)-\\mathcal V^{\\pi_t}(\\xi)\\big]\n\\;\\lesssim\\;\n\\underbrace{T^{-1/2}}_{\\text{actor optimization}}\n+\\underbrace{\\varepsilon_{\\text{critic}}(K,m,R)}_{\\text{TD evaluation}}\n+\\underbrace{\\varepsilon_{\\text{actor}}(N,R)}_{\\text{compatible approx.}}\n+\\underbrace{\\varepsilon_{\\text{inf}}(\\xi)}_{\\text{inference penalty}}.\n\\]\nThe inference error is the price of partial observability: \\[\n\\varepsilon_{\\text{inf}}(\\xi)\n= \\mathbb E\\!\\left[\\sum_{k\\ge0}\\gamma^k\n\\left\\|\\,b_k(\\cdot)-b_0(\\cdot,I_k)\\,\\right\\|_{\\mathrm{TV}}\n\\right],\\qquad I_k=(Y_k,Z_k).\n\\]"
  },
  {
    "objectID": "posts/nac-for-pomdps.html#sliding-window-controllers-memory-vs.-accuracy",
    "href": "posts/nac-for-pomdps.html#sliding-window-controllers-memory-vs.-accuracy",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "7 Sliding-Window Controllers (Memory vs. Accuracy)",
    "text": "7 Sliding-Window Controllers (Memory vs. Accuracy)\nUnder stochastic exploration and filter stability/minorization, the inference error decays geometrically with window length \\(n\\): \\[\n\\varepsilon_{\\text{inf}}(\\xi)\\;\\le\\;\\frac{1}{1-\\gamma}\\cdot\n\\mathcal O\\!\\Big(\\rho^{\\lfloor n/m_0\\rfloor}\\Big),\n\\] for constants \\(\\rho\\in(0,1)\\) and \\(m_0\\ge1\\). To reach tolerance \\(\\epsilon\\): \\[\nn=\\mathcal O\\!\\big(m_0\\log(1/\\epsilon)\\big).\n\\]"
  },
  {
    "objectID": "posts/nac-for-pomdps.html#practical-tuning",
    "href": "posts/nac-for-pomdps.html#practical-tuning",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "8 Practical Tuning",
    "text": "8 Practical Tuning\n\nTD horizon \\(m\\): choose \\(m=\\Theta(\\log_{1/\\gamma}(1/\\epsilon))\\) to make aliasing \\(\\lesssim\\epsilon\\).\nCritic steps \\(K\\) and actor inner steps \\(N\\): about \\(\\Theta(\\epsilon^{-4})\\).\nFeatures: richer features shrink the compatible approximation term.\nWindow \\(n\\): grows like \\(\\mathcal O(\\log(1/\\epsilon))\\) under filter stability."
  },
  {
    "objectID": "posts/nac-for-pomdps.html#whats-next-rnn-based-nac-for-pomdps",
    "href": "posts/nac-for-pomdps.html#whats-next-rnn-based-nac-for-pomdps",
    "title": "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers",
    "section": "9 What’s next: RNN-based NAC for POMDPs",
    "text": "9 What’s next: RNN-based NAC for POMDPs\nA natural extension replaces the hand-engineered memory \\(Z_k\\) with a recurrent hidden state \\(H_k\\) learned end-to-end (RNN-based NAC). The policy becomes \\(\\pi_\\theta(a\\mid Y_k, H_k)\\) with \\(H_{k+1}=f_\\theta(H_k,Y_{k+1},A_k)\\), and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs. representation power, and how partial-observability error shows up) in a follow-up post about our RNN-based natural actor–critic paper."
  }
]