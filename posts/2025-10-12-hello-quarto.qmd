---
title: "Towards Provably Effective Policy Optimization for Large POMDPs"
date: 2025-10-12
description: "Natural actor–critic for POMDPs."
categories: [reinforcement-learning, POMDP, actor-critic, theory]
toc: true
draft: false
---

> **TL;DR.** The paper [@cayci2024nac] proves finite-time guarantees for a natural actor–critic (NAC) method in POMDPs. It learns finite-memory policies, evaluates them with an $m$-step TD critic to reduce perceptual aliasing, and gives a performance bound that separates optimization/approximation, evaluation, and inference (partial observability) errors. For sliding-window controllers, the inference error decays geometrically with the window size.

## Setting and Goal

We work with a discounted POMDP but restrict policies to **finite-state controllers (FSCs)** that keep internal memory $Z_k$ and act from $(Y_k, Z_k)$.

- Goal: learn the best policy within a fixed FSC class $\Pi_{Z,\varphi}$:
  
  $$
  \pi^\star \in \arg\max_{\pi \in \Pi_{Z,\varphi}} \; \mathcal V^\pi(\xi),
  $$
  
  where $\mathcal V^\pi(\xi)$ is the $\gamma$-discounted value under prior $\xi$.

- A useful subclass is the **sliding-window controller (SWC)** with window $n$:
  
  $$
  Z_k = (Y_{k-n:k-1},\; U_{k-n:k-1}) \in \mathcal Y^n \times \mathcal U^n,
  $$
  
  which summarizes the last $n$ observations/actions.

The actor uses a **linear-softmax FSC**:
$$
\pi_\theta(u \mid y,z) = 
\frac{\exp\{\theta^\top \psi(u,y,z)\}}
{\sum_{u'} \exp\{\theta^\top \psi(u',y,z)\}},
$$
with features $\psi(u,y,z)\in\mathbb R^d$.

## Sampling Measure (for Analysis)

Let the **discounted visitation** over information states be
$$
d^\pi_\xi(y,z)=(1-\gamma)\sum_{k\ge0}\gamma^k\,
\Pr^\pi\!\big[(Y_k,Z_k)=(y,z)\,\big|\,H_0\sim\xi\big].
$$
The analysis assumes access to samples from $d^\pi_\xi$ (standard in NAC to match the Fisher metric).

## Algorithm Sketch (FS-NAC)

1. **Critic** — run **$m$-step TD(0)** with linear function approximation to estimate $\widehat{\mathcal Q}^{\pi_t}(y,z,u)$.
2. **Actor** — take a **natural-gradient** step using the critic’s advantage estimate.

::: callout-tip
**Why $m$-step TD?** In POMDPs, one-step bootstrapping suffers from **perceptual aliasing** (same observation, different hidden states). Looking $m$ steps ahead stabilizes the target and reduces this bias.
:::

## Critic Guarantee (Policy Evaluation)

With stepsize $\alpha=K^{-1/2}$ and parameter radius $R$, after $K$ critic updates:
$$
\mathbb E\!\left[\big\|\mathcal Q^\pi-\widehat{\mathcal Q}^\pi_K\big\|_{2,\,d^\pi_\xi\otimes\pi}\right]
\;\lesssim\;
\underbrace{\tfrac{K^{-1/4}}{1-\gamma}}_{\text{statistical}}
+\underbrace{\tfrac{\varepsilon_{\text{app}}(R)}{1-\gamma^m}}_{\text{approximation}}
+\underbrace{\varepsilon_{\text{pa}}(\gamma,m,R)}_{\text{aliasing}},
$$
where $\varepsilon_{\text{app}}(R)$ is the best linear-approximation error within radius $R$, and the aliasing term contracts geometrically:
$$
\varepsilon_{\text{pa}}(\gamma,m,R)=\mathcal O\!\big(\gamma^{m/2}\,\mathrm{poly}(R,(1-\gamma)^{-1})\big).
$$

**Sample–accuracy trade-off.** Each update uses $m$ samples, so total evaluation samples are $\Theta(mK)$.

## Full Actor–Critic Performance (Finite-Time)

After $T$ outer iterations (with tuned stepsizes),
$$
(1-\gamma)\,\min_{t<T}\mathbb E\!\big[\mathcal V^{\pi^\star}(\xi)-\mathcal V^{\pi_t}(\xi)\big]
\;\lesssim\;
\underbrace{T^{-1/2}}_{\text{actor optimization}}
+\underbrace{\varepsilon_{\text{critic}}(K,m,R)}_{\text{TD evaluation}}
+\underbrace{\varepsilon_{\text{actor}}(N,R)}_{\text{compatible approx.}}
+\underbrace{\varepsilon_{\text{inf}}(\xi)}_{\text{inference penalty}}.
$$

The **inference error** is the price of partial observability:
$$
\varepsilon_{\text{inf}}(\xi)
= \mathbb E\!\left[\sum_{k\ge0}\gamma^k
\left\|\,b_k(\cdot)-b_0(\cdot,I_k)\,\right\|_{\mathrm{TV}}
\right],\qquad I_k=(Y_k,Z_k).
$$

## Sliding-Window Controllers (Memory vs. Accuracy)

Under stochastic exploration and filter stability/minorization, the inference error **decays geometrically** with window length $n$:
$$
\varepsilon_{\text{inf}}(\xi)\;\le\;\frac{1}{1-\gamma}\cdot
\mathcal O\!\Big(\rho^{\lfloor n/m_0\rfloor}\Big),
$$
for constants $\rho\in(0,1)$ and $m_0\ge1$. To reach tolerance $\epsilon$:
$$
n=\mathcal O\!\big(m_0\log(1/\epsilon)\big).
$$

## Practical Tuning

- **TD horizon $m$:** choose $m=\Theta(\log_{1/\gamma}(1/\epsilon))$ to make aliasing $\lesssim\epsilon$.
- **Critic steps $K$ and actor inner steps $N$:** about $\Theta(\epsilon^{-4})$.
- **Features:** richer features shrink the compatible approximation term.
- **Window $n$:** grows like $\mathcal O(\log(1/\epsilon))$ under filter stability.

---

### Reference (already in your `references.bib`)
```bibtex
@article{cayci2024nac,
  title   = {Finite-Time Analysis of Natural Actor-Critic for POMDPs},
  author  = {Cayci, Semih and He, Niao and Srikant, R.},
  journal = {SIAM Journal on Mathematics of Data Science},
  year    = {2024}
}
