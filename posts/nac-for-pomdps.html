<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-12">
<meta name="description" content="Natural actor–critic with finite-state controllers for POMDPs.">

<title>Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers – Semih Cayci — Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-355c415dd6b9922ddaf04a2341011fe1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Semih Cayci — Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../posts/index.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#formal-pomdp-model" id="toc-formal-pomdp-model" class="nav-link active" data-scroll-target="#formal-pomdp-model"><span class="header-section-number">1</span> Formal POMDP model</a></li>
  <li><a href="#setting-and-goal" id="toc-setting-and-goal" class="nav-link" data-scroll-target="#setting-and-goal"><span class="header-section-number">2</span> Setting and Goal</a></li>
  <li><a href="#sampling-measure" id="toc-sampling-measure" class="nav-link" data-scroll-target="#sampling-measure"><span class="header-section-number">3</span> Sampling Measure</a></li>
  <li><a href="#algorithm-fs-nac" id="toc-algorithm-fs-nac" class="nav-link" data-scroll-target="#algorithm-fs-nac"><span class="header-section-number">4</span> Algorithm (FS-NAC)</a></li>
  <li><a href="#critic-m-step-td-learning-with-an-internal-state" id="toc-critic-m-step-td-learning-with-an-internal-state" class="nav-link" data-scroll-target="#critic-m-step-td-learning-with-an-internal-state"><span class="header-section-number">5</span> Critic: <span class="math inline">\(m\)</span>-step TD Learning with an Internal State</a></li>
  <li><a href="#full-actorcritic-performance-finite-time" id="toc-full-actorcritic-performance-finite-time" class="nav-link" data-scroll-target="#full-actorcritic-performance-finite-time"><span class="header-section-number">6</span> Full Actor–Critic Performance (Finite-Time)</a></li>
  <li><a href="#sliding-window-controllers-memory-vs.-accuracy" id="toc-sliding-window-controllers-memory-vs.-accuracy" class="nav-link" data-scroll-target="#sliding-window-controllers-memory-vs.-accuracy"><span class="header-section-number">7</span> Sliding-Window Controllers (Memory vs.&nbsp;Accuracy)</a></li>
  <li><a href="#practical-tuning" id="toc-practical-tuning" class="nav-link" data-scroll-target="#practical-tuning"><span class="header-section-number">8</span> Practical Tuning</a></li>
  <li><a href="#whats-next-rnn-based-nac-for-pomdps" id="toc-whats-next-rnn-based-nac-for-pomdps" class="nav-link" data-scroll-target="#whats-next-rnn-based-nac-for-pomdps"><span class="header-section-number">9</span> What’s next: RNN-based NAC for POMDPs</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers</h1>
  <div class="quarto-categories">
    <div class="quarto-category">reinforcement-learning</div>
    <div class="quarto-category">POMDP</div>
    <div class="quarto-category">actor-critic</div>
    <div class="quarto-category">theory</div>
  </div>
  </div>

<div>
  <div class="description">
    Natural actor–critic with finite-state controllers for POMDPs.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 12, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>TL;DR.</strong> In <span class="citation" data-cites="cayci2024nac">(<a href="#ref-cayci2024nac" role="doc-biblioref">Cayci, He, and Srikant 2024</a>)</span>, we studied a natural actor–critic (NAC) method in POMDPs. Critic uses <span class="math inline">\(m\)</span>-step TD learning to mitigate perceptual aliasing and the actor uses an internal state to incorporate memory. We establish non-asymptotic bounds for optimization/approximation, evaluation, and inference (partial observability) errors. For sliding-window controllers, the inference error decays geometrically with the window size under certain ergodicity conditions.</p>
</blockquote>
<section id="formal-pomdp-model" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="formal-pomdp-model"><span class="header-section-number">1</span> Formal POMDP model</h2>
<p>Let us consider a discounted POMDP <span class="math display">\[
\mathcal{M}=\big(\mathcal S,\mathcal A,\mathcal Y,\; P,\; O,\; r,\; \gamma\big),
\]</span></p>
<p>with:</p>
<ul>
<li>state <span class="math inline">\(S_k\in\mathcal S\)</span>,</li>
<li>action <span class="math inline">\(A_k\in\mathcal A\)</span>,</li>
<li>observation <span class="math inline">\(Y_k\in\mathcal Y\)</span>,</li>
<li>transition kernel <span class="math inline">\(P(s' \mid s,a)\)</span>,</li>
<li>observation kernel <span class="math inline">\(\Phi(y \mid s')\)</span>,</li>
<li>reward <span class="math inline">\(r(s,a)\in[0,1]\)</span> (w.l.o.g.),</li>
<li>discount factor <span class="math inline">\(\gamma\in(0,1)\)</span>.</li>
</ul>
<p>To avoid measure-theoretic concerns, let us consider finite <span class="math inline">\(\mathcal S, \mathcal Y, \mathcal A\)</span>. These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is <span class="math display">\[
S_{k+1}\sim P(\cdot\mid S_k,A_k),\qquad
Y_{k}\sim \Phi(\cdot\mid S_k),\qquad
R_k=r(S_k,A_k),
\]</span> and the goal is to maximize <span class="math display">\[
\mathcal V^\pi(\xi)=\mathbb E_\xi^\pi\!\left[\sum_{k\ge0}\gamma^k\,r(S_k,A_k)\right].
\]</span></p>
<p><strong>Why challenging?</strong><br>
Partial observability implies that the optimal policy <span class="math inline">\(\pi^\star=(\pi_0^\star,\pi_1^\star,\ldots)\)</span> is non-stationary, therefore <span class="math inline">\(\pi_k^\star\)</span> depends on the complete trajectory <span class="math inline">\((Y_{0:k},A_{0:k-1})\)</span> for each <span class="math inline">\(k\in\mathbb N\)</span> via the Bayes belief <span class="math inline">\(b_k(s)=\Pr(S_k=s\mid Y_{0:k},A_{0:k-1})\)</span>. Reactive (i.e., memoryless) methods for MDPs fail miserably if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see <strong>Example 1</strong> below). A classical result attributed to <span class="citation" data-cites="astrom1965">(<a href="#ref-astrom1965" role="doc-biblioref">Åström 1965</a>)</span> (see <span class="citation" data-cites="yuksel2025another">(<a href="#ref-yuksel2025another" role="doc-biblioref">Yüksel 2025</a>)</span> for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state <span class="math inline">\(b_k\)</span>. However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over <span class="math inline">\(\mathcal S\)</span>), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies <span class="citation" data-cites="murphy2000survey">(<a href="#ref-murphy2000survey" role="doc-biblioref">Murphy 2000</a>)</span> and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state <span class="math inline">\(Z_k\)</span>, which summarizes/compresses <span class="math inline">\((Y_{0:k-1},A_{0:k-1})\)</span> for each <span class="math inline">\(k\)</span>. The idea is to use a parametric policy <span class="math inline">\(\pi_k(\cdot\mid Y_k, Z_k)\)</span> based on the internal state <span class="math inline">\(Z_k\)</span>.</p>
</section>
<section id="setting-and-goal" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="setting-and-goal"><span class="header-section-number">2</span> Setting and Goal</h2>
<p>At first, we restrict policies to <strong>finite-state controllers (FSCs)</strong> that keep internal memory <span class="math inline">\(Z_k\)</span> and act from <span class="math inline">\((Y_k, Z_k)\)</span>. The internal memory is updated in a Markovian way: <span class="math inline">\(Z_{k+1}~\varphi(\cdot\mid z_k, y_k, a_k)\)</span> given <span class="math inline">\(Z_k=z_k,Y_k=y_k,A_k=a_k\)</span>.</p>
<ul>
<li><p>Learn the best policy within a fixed FSC class <span class="math inline">\(\pi_{Z,\varphi}\)</span>:</p>
<p><span class="math display">\[
\pi^\star \in \arg\max_{\pi \in \pi_{Z,\varphi}} \; \mathcal V^\pi(\xi).
\]</span></p></li>
<li><p>A useful subclass is the <strong>sliding-window controller (SWC)</strong> with window <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
Z_k = (Y_{k-n:k-1},\; A_{k-n:k-1}) \in \mathcal Y^n \times \mathcal A^n,
\]</span></p>
<p>which summarizes the last <span class="math inline">\(n\)</span> observations/actions, where <span class="math inline">\(n\)</span> is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.</p></li>
</ul>
<p>The actor uses a <strong>linear-softmax FSC</strong>: <span class="math display">\[
\pi_\theta(a \mid y,z) =
\frac{\exp\{\theta^\top \psi(a,y,z)\}}
{\sum_{a'} \exp\{\theta^\top \psi(a',y,z)\}},
\]</span> with features <span class="math inline">\(\psi(a,y,z)\in\mathbb R^d\)</span>. The controller memory updates via some <span class="math inline">\(\varphi\)</span>: <span class="math display">\[
Z_{k+1}=\varphi(Z_k, Y_{k+1}, A_k).
\]</span></p>
</section>
<section id="sampling-measure" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sampling-measure"><span class="header-section-number">3</span> Sampling Measure</h2>
<p>Let the <strong>discounted visitation distribution</strong> over information states be <span class="math display">\[
d^\pi_\xi(y,z)=(1-\gamma)\sum_{k\ge0}\gamma^k\,
P^\pi\!\big[(Y_k,Z_k)=(y,z)\,\big|\,H_0\sim\xi\big].
\]</span></p>
</section>
<section id="algorithm-fs-nac" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="algorithm-fs-nac"><span class="header-section-number">4</span> Algorithm (FS-NAC)</h2>
<ol type="1">
<li><strong>Critic</strong> — run <strong><span class="math inline">\(m\)</span>-step TD(0)</strong> with linear function approximation to estimate <span class="math inline">\(\widehat{\mathcal Q}^{\pi_t}(y,z,a)\)</span>.</li>
<li><strong>Actor</strong> — take a <strong>natural-gradient</strong> step using the critic’s advantage estimate.</li>
</ol>
</section>
<section id="critic-m-step-td-learning-with-an-internal-state" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="critic-m-step-td-learning-with-an-internal-state"><span class="header-section-number">5</span> Critic: <span class="math inline">\(m\)</span>-step TD Learning with an Internal State</h2>
<p>It is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology <span class="citation" data-cites="singh1994learning">(<a href="#ref-singh1994learning" role="doc-biblioref">Singh, Jaakkola, and Jordan 1994</a>)</span>. For a concrete demonstration, let us focus on learning the value function in a Hidden Markov Reward Process (HMRP): <span class="math display">\[V(z_0):=\mathbb E[\sum_{k=0}^\infty \gamma^k r(S_k)|Z_0=z_0]\]</span> for an internal state <span class="math inline">\(Z_t\)</span> (e.g., <span class="math inline">\(Z_t = (Y_{t-m+1},\ldots,Y_t)\)</span>), where <span class="math inline">\((S_k, r(S_k), Y_k)\)</span> is a HMRP.</p>
<p><strong>Example 1</strong> Consider an HMRP with <span class="math inline">\(\mathcal S = \{0,1\}\)</span>, <span class="math inline">\(r(s) = s\)</span> and <span class="math inline">\(\Phi(1|s) = 1\)</span> for all <span class="math inline">\(s\in\mathcal{S}\)</span>. Let <span class="math inline">\(P(0|1) = p\)</span> and <span class="math inline">\(P(1|0) = q\)</span> for some <span class="math inline">\(p, q \in (0,1)\)</span>, and <span class="math inline">\(\gamma \in (0, 1)\)</span> be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to <span class="math inline">\(v^\star = \frac{1}{1-\gamma}\cdot \frac{p}{q+p}\)</span>. On the other hand, the value function for a given internal state <span class="math inline">\(Z_0=z_0\)</span> is <span class="math display">\[V(z_0) = \frac{\gamma p + (1-\gamma)P(S_0=1|Z_0=z_0)}{(1-\gamma)\Big[\gamma(p+q)+1-\gamma\Big]}\neq v^\star.\]</span> As such, memoryless TD learning cannot learn <span class="math inline">\(V(z_0)\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Why <span class="math inline">\(m\)</span>-step TD?</strong> In POMDPs, one-step bootstrapping suffers from <strong>perceptual aliasing</strong> (same observation <span class="math inline">\(Y_k\)</span>, different latent state <span class="math inline">\(S_k\)</span>). Looking <span class="math inline">\(m\)</span> steps ahead stabilizes the target and reduces this bias.</p>
</div>
</div>
<p>To see how <span class="math inline">\(m\)</span>-step TD learning with internal state <span class="math inline">\(Z_t = (Y_{t-m+1},\ldots,Y_t)\)</span> mitigates perceptual aliasing, let us focus on the limit point. To that end, let <span class="math display">\[(T_mv)(z) = \bar{r}_m(z) + \gamma^m(K_mv)(z),\]</span> where <span class="math inline">\(\bar{r}_m(z) := \mathbb E[\sum_{k=0}^{m-1}r(S_k)|Z_0=z]\)</span> and <span class="math inline">\((K_mv)(z):=\mathbb{E}[v(Z_m)|Z_0=z]\)</span>. Then, it is straightforward to show that <span class="math inline">\(T_m\)</span> is a contractive operator with modulus <span class="math inline">\(\gamma^m\)</span>, and admits a unique fixed point <span class="math display">\[V(z) = \mathbb E[\sum_{k=0}^\infty r(S_k)|Z_0=z] = \sum_{k=0}^\infty (\gamma^mK_m)^k\bar{r}_m(z)\]</span> for any <span class="math inline">\(m\in\mathbb N\)</span>. Let <span class="math inline">\(\xi\)</span> be the invariant distribution of <span class="math inline">\(\{Z_k:k\in\mathbb N\}\)</span>: <span class="math display">\[\xi(z_t)=\sum_{s_{t-m+1:t}}\prod_{k=0}^{m-1}\Phi(y_{t-k}|s_{t-k})\mu(s_{t-k}),\]</span> where <span class="math inline">\(\mu\)</span> is the stationary distribution of <span class="math inline">\(\{S_k:k\in\mathbb N\}\)</span>. Under a non-degenerate <span class="math inline">\(\mathbb{E}_{Z_t\sim \xi}[\psi(Z_t)\otimes\psi(Z_t)]\succ 0\)</span>, <span class="math inline">\(m\)</span>-step TD learning with the internal state <span class="math inline">\(Z_t\)</span> converges to the unique fixed point <span class="math inline">\(v_m\)</span> of the equation <span class="math display">\[v = \pi\{\bar{r}_m+\gamma^m K_m v\},\]</span> where <span class="math inline">\(\pi\)</span> be the ortogonal projection onto the subspace <span class="math inline">\(\mathcal V := \{z\mapsto \langle \psi(z),w\rangle: w \in \mathbb R^d\)</span> with respect to <span class="math inline">\(L^2(\xi)\)</span>. Then, Using this, we can show that <span class="math display">\[\|v_m - \pi V\|_{L^2(\xi)} \leq \frac{\gamma^m}{1+\gamma^m}\|V-\pi V\|_{L^2(\xi)}.\]</span> As such, it is possible to learn <span class="math inline">\(V(z_0)\)</span> via <span class="math inline">\(m\)</span>-step TD learning with an additional error term <span class="math inline">\(\mathcal{O}(\gamma^m)\)</span>.</p>
<p>With stepsize <span class="math inline">\(\alpha=K^{-1/2}\)</span> and parameter radius <span class="math inline">\(R\)</span>, after <span class="math inline">\(K\)</span> critic updates: <span class="math display">\[
\sqrt{\mathbb E\!\left[\big\|\mathcal Q^\pi-\widehat{\mathcal Q}^\pi_K\big\|^2_{\,d^\pi_\xi\otimes\pi}\right]}
\;\lesssim\;
\underbrace{\tfrac{K^{-1/4}}{1-\gamma}}_{\text{statistical}}
+\underbrace{\tfrac{\varepsilon_{\text{app}}(R)}{1-\gamma^m}}_{\text{approximation}}
+\underbrace{\varepsilon_{\text{pa}}(\gamma,m,R)}_{\text{aliasing}},
\]</span> where <span class="math inline">\(\varepsilon_{\text{app}}(R)\)</span> is the best linear-approximation error within radius <span class="math inline">\(R\)</span>, and the aliasing term contracts geometrically: <span class="math display">\[
\varepsilon_{\text{pa}}(\gamma,m,R)=\mathcal O\!\big(\gamma^{m/2}\,\mathrm{poly}(R,(1-\gamma)^{-1})\big).
\]</span></p>
<p><strong>Sample–accuracy trade-off.</strong> Each update uses <span class="math inline">\(m\)</span> samples, so total evaluation samples are <span class="math inline">\(\Theta(mK)\)</span>.</p>
</section>
<section id="full-actorcritic-performance-finite-time" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="full-actorcritic-performance-finite-time"><span class="header-section-number">6</span> Full Actor–Critic Performance (Finite-Time)</h2>
<p>After <span class="math inline">\(T\)</span> outer iterations (with tuned stepsizes), <span class="math display">\[
(1-\gamma)\,\min_{t&lt;T}\mathbb E\!\big[\mathcal V^{\pi^\star}(\xi)-\mathcal V^{\pi_t}(\xi)\big]
\;\lesssim\;
\underbrace{T^{-1/2}}_{\text{actor optimization}}
+\underbrace{\varepsilon_{\text{critic}}(K,m,R)}_{\text{TD evaluation}}
+\underbrace{\varepsilon_{\text{actor}}(N,R)}_{\text{compatible approx.}}
+\underbrace{\varepsilon_{\text{inf}}(\xi)}_{\text{inference penalty}}.
\]</span></p>
<p>The <strong>inference error</strong> is the price of partial observability: <span class="math display">\[
\varepsilon_{\text{inf}}(\xi)
= \mathbb E\!\left[\sum_{k\ge0}\gamma^k
\left\|\,b_k(\cdot)-b_0(\cdot,I_k)\,\right\|_{\mathrm{TV}}
\right],\qquad I_k=(Y_k,Z_k).
\]</span></p>
</section>
<section id="sliding-window-controllers-memory-vs.-accuracy" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="sliding-window-controllers-memory-vs.-accuracy"><span class="header-section-number">7</span> Sliding-Window Controllers (Memory vs.&nbsp;Accuracy)</h2>
<p>Under stochastic exploration and filter stability/minorization, the inference error <strong>decays geometrically</strong> with window length <span class="math inline">\(n\)</span>: <span class="math display">\[
\varepsilon_{\text{inf}}(\xi)\;\le\;\frac{1}{1-\gamma}\cdot
\mathcal O\!\Big(\rho^{\lfloor n/m_0\rfloor}\Big),
\]</span> for constants <span class="math inline">\(\rho\in(0,1)\)</span> and <span class="math inline">\(m_0\ge1\)</span>. To reach tolerance <span class="math inline">\(\epsilon\)</span>: <span class="math display">\[
n=\mathcal O\!\big(m_0\log(1/\epsilon)\big).
\]</span></p>
</section>
<section id="practical-tuning" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="practical-tuning"><span class="header-section-number">8</span> Practical Tuning</h2>
<ul>
<li><strong>TD horizon <span class="math inline">\(m\)</span>:</strong> choose <span class="math inline">\(m=\Theta(\log_{1/\gamma}(1/\epsilon))\)</span> to make aliasing <span class="math inline">\(\lesssim\epsilon\)</span>.</li>
<li><strong>Critic steps <span class="math inline">\(K\)</span> and actor inner steps <span class="math inline">\(N\)</span>:</strong> about <span class="math inline">\(\Theta(\epsilon^{-4})\)</span>.</li>
<li><strong>Features:</strong> richer features shrink the compatible approximation term.</li>
<li><strong>Window <span class="math inline">\(n\)</span>:</strong> grows like <span class="math inline">\(\mathcal O(\log(1/\epsilon))\)</span> under filter stability.</li>
</ul>
</section>
<section id="whats-next-rnn-based-nac-for-pomdps" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="whats-next-rnn-based-nac-for-pomdps"><span class="header-section-number">9</span> What’s next: RNN-based NAC for POMDPs</h2>
<p>A natural extension replaces the hand-engineered memory <span class="math inline">\(Z_k\)</span> with a <strong>recurrent hidden state</strong> <span class="math inline">\(H_k\)</span> learned end-to-end (RNN-based NAC). The policy becomes <span class="math inline">\(\pi_\theta(a\mid Y_k, H_k)\)</span> with <span class="math inline">\(H_{k+1}=f_\theta(H_k,Y_{k+1},A_k)\)</span>, and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs.&nbsp;representation power, and how partial-observability error shows up) in a follow-up post about our <strong>RNN-based natural actor–critic</strong> paper.</p>
<hr>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-astrom1965" class="csl-entry" role="listitem">
Åström, Karl J. 1965. <span>“Optimal Control of Markov Processes with Incomplete State Information i.”</span> <em>Journal of Mathematical Analysis and Applications</em> 10 (1): 174–205. <a href="https://doi.org/10.1016/0022-247X(65)90154-X">https://doi.org/10.1016/0022-247X(65)90154-X</a>.
</div>
<div id="ref-cayci2024nac" class="csl-entry" role="listitem">
Cayci, Semih, Niao He, and R. Srikant. 2024. <span>“Finite-Time Analysis of Natural Actor-Critic for POMDPs.”</span> <em>SIAM Journal on Mathematics of Data Science</em>.
</div>
<div id="ref-murphy2000survey" class="csl-entry" role="listitem">
Murphy, Kevin P. 2000. <span>“A Survey of POMDP Solution Techniques.”</span> <em>Environment</em> 2 (10).
</div>
<div id="ref-singh1994learning" class="csl-entry" role="listitem">
Singh, Satinder P, Tommi Jaakkola, and Michael I Jordan. 1994. <span>“Learning Without State-Estimation in Partially Observable Markovian Decision Processes.”</span> In <em>Machine Learning Proceedings 1994</em>, 284–92. Elsevier.
</div>
<div id="ref-yuksel2025another" class="csl-entry" role="listitem">
Yüksel, Serdar. 2025. <span>“Another Look at Partially Observed Optimal Stochastic Control: Existence, Ergodicity, and Approximations Without Belief-Reduction.”</span> <em>Applied Mathematics &amp; Optimization</em> 91 (1): 16.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/semihcayci\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>