---
title: "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers"
date: 2025-10-12
description: "Natural actor–critic with finite-state controllers for POMDPs."
categories: [reinforcement-learning, POMDP, actor-critic, theory]
toc: true
draft: false
---

> **TL;DR.** In [@cayci2024nac], we studied a natural actor–critic (NAC) method in POMDPs. Critic uses $m$-step TD learning to mitigate perceptual aliasing and the actor uses an internal state to incorporate memory. We establish non-asymptotic bounds for optimization/approximation, evaluation, and inference (partial observability) errors. For sliding-window controllers, the inference error decays geometrically with the window size under certain ergodicity conditions.

## Formal POMDP model

Let us consider a discounted POMDP
$$
\mathcal{M}=\big(\mathcal S,\mathcal A,\mathcal Y,\; P,\; O,\; r,\; \gamma\big),
$$
with:
- latent state $S_k\in\mathcal S$,
- action $A_k\in\mathcal A$,
- observation $Y_k\in\mathcal Y$,
- transition kernel $P(s' \mid s,a)$,
- observation kernel $\Phi(y \mid s')$ (can depend on $a$ in general),
- reward $r(s,a)\in[0,1]$ (w.l.o.g.),
- discount factor $\gamma\in(0,1)$.
To avoid measure-theoretic concerns, let us consider finite $\mathcal S, \mathcal Y, \mathcal A$. These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is
$$
S_{k+1}\sim P(\cdot\mid S_k,A_k),\qquad
Y_{k}\sim \Phi(\cdot\mid S_k),\qquad
R_k=r(S_k,A_k),
$$
and the goal is to maximize
$$
\mathcal V^\pi(\xi)=\mathbb E_\xi^\pi\!\left[\sum_{k\ge0}\gamma^k\,r(S_k,A_k)\right].
$$

**Why challenging?**  
Partial observability implies that the optimal policy $\pi^\star=(\pi_0^\star,\pi_1^\star,\ldots)$ is non-stationary, therefore $\pi_k^\star$ depends on the complete trajectory $(Y_{0:k},A_{0:k-1})$ for each $k\in\mathbb N$ via the Bayes belief $b_k(s)=\Pr(S_k=s\mid H_k)$. A classical result attributed to [@astrom1965] (see [@yuksel2025another] for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state $b_k$. However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over $\mathcal S$), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies [@murphy2000survey] and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state $Z_k$, which summarizes/compresses $(Y_{0:k-1},A_{0:k-1})$ for each $k$. The idea is to use a parametric policy $\pi_k(\cdot\mid Y_k, Z_k)$ based on the internal state $Z_k$.

## Setting and Goal

At first, we restrict policies to **finite-state controllers (FSCs)** that keep internal memory $Z_k$ and act from $(Y_k, Z_k)$. The internal memory is updated in a Markovian way: $Z_{k+1}~\varphi(\cdot\mid z_k, y_k, a_k)$ given $Z_k=z_k,Y_k=y_k,A_k=a_k$.

- Learn the best policy within a fixed FSC class $\Pi_{Z,\varphi}$:
  
  $$
  \pi^\star \in \arg\max_{\pi \in \Pi_{Z,\varphi}} \; \mathcal V^\pi(\xi).
  $$

- A useful subclass is the **sliding-window controller (SWC)** with window $n$:
  
  $$
  Z_k = (Y_{k-n:k-1},\; A_{k-n:k-1}) \in \mathcal Y^n \times \mathcal A^n,
  $$
  
  which summarizes the last $n$ observations/actions, where $n$ is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.

The actor uses a **linear-softmax FSC**:
$$
\pi_\theta(a \mid y,z) = 
\frac{\exp\{\theta^\top \psi(a,y,z)\}}
{\sum_{a'} \exp\{\theta^\top \psi(a',y,z)\}},
$$
with features $\psi(a,y,z)\in\mathbb R^d$. The controller memory updates via some $\varphi$:
$$
Z_{k+1}=\varphi(Z_k, Y_{k+1}, A_k).
$$

## Sampling Measure

Let the **discounted visitation distribution** over information states be
$$
d^\pi_\xi(y,z)=(1-\gamma)\sum_{k\ge0}\gamma^k\,
P^\pi\!\big[(Y_k,Z_k)=(y,z)\,\big|\,H_0\sim\xi\big].
$$

## Algorithm Sketch (FS-NAC)

1. **Critic** — run **$m$-step TD(0)** with linear function approximation to estimate $\widehat{\mathcal Q}^{\pi_t}(y,z,a)$.
2. **Actor** — take a **natural-gradient** step using the critic’s advantage estimate.



## Critic: $m$-step TD Learning with Memory
::: callout-tip
**Why $m$-step TD?** In POMDPs, one-step bootstrapping suffers from **perceptual aliasing** (same observation $Y_k$, different latent state $S_k$). Looking $m$ steps ahead stabilizes the target and reduces this bias.
:::
It is very easy to construct an example which shows that (memoryless) TD learning cannot learn the value functions [@singh1994learning].
**Example** Consider a (hidden) Markov reward process with $\mathcal S = \{0,1\}$, $r(s) = s$ and $\Phi(1|s) = 1$ for all $s\in\mathcal{S}$. Let $P(0|1) = p$ and $P(1|0) = q$ for some $p, q \in (0,1)$, and $\gamma \in (0, 1)$ be the discount factor. Then, TD(0) converges to $v^\star = \frac{1}{1-\gamma}\cdot \frac{q}{q+p}$. 

With stepsize $\alpha=K^{-1/2}$ and parameter radius $R$, after $K$ critic updates:
$$
\sqrt{\mathbb E\!\left[\big\|\mathcal Q^\pi-\widehat{\mathcal Q}^\pi_K\big\|^2_{\,d^\pi_\xi\otimes\pi}\right]}
\;\lesssim\;
\underbrace{\tfrac{K^{-1/4}}{1-\gamma}}_{\text{statistical}}
+\underbrace{\tfrac{\varepsilon_{\text{app}}(R)}{1-\gamma^m}}_{\text{approximation}}
+\underbrace{\varepsilon_{\text{pa}}(\gamma,m,R)}_{\text{aliasing}},
$$
where $\varepsilon_{\text{app}}(R)$ is the best linear-approximation error within radius $R$, and the aliasing term contracts geometrically:
$$
\varepsilon_{\text{pa}}(\gamma,m,R)=\mathcal O\!\big(\gamma^{m/2}\,\mathrm{poly}(R,(1-\gamma)^{-1})\big).
$$

**Sample–accuracy trade-off.** Each update uses $m$ samples, so total evaluation samples are $\Theta(mK)$.

## Full Actor–Critic Performance (Finite-Time)

After $T$ outer iterations (with tuned stepsizes),
$$
(1-\gamma)\,\min_{t<T}\mathbb E\!\big[\mathcal V^{\pi^\star}(\xi)-\mathcal V^{\pi_t}(\xi)\big]
\;\lesssim\;
\underbrace{T^{-1/2}}_{\text{actor optimization}}
+\underbrace{\varepsilon_{\text{critic}}(K,m,R)}_{\text{TD evaluation}}
+\underbrace{\varepsilon_{\text{actor}}(N,R)}_{\text{compatible approx.}}
+\underbrace{\varepsilon_{\text{inf}}(\xi)}_{\text{inference penalty}}.
$$

The **inference error** is the price of partial observability:
$$
\varepsilon_{\text{inf}}(\xi)
= \mathbb E\!\left[\sum_{k\ge0}\gamma^k
\left\|\,b_k(\cdot)-b_0(\cdot,I_k)\,\right\|_{\mathrm{TV}}
\right],\qquad I_k=(Y_k,Z_k).
$$

## Sliding-Window Controllers (Memory vs. Accuracy)

Under stochastic exploration and filter stability/minorization, the inference error **decays geometrically** with window length $n$:
$$
\varepsilon_{\text{inf}}(\xi)\;\le\;\frac{1}{1-\gamma}\cdot
\mathcal O\!\Big(\rho^{\lfloor n/m_0\rfloor}\Big),
$$
for constants $\rho\in(0,1)$ and $m_0\ge1$. To reach tolerance $\epsilon$:
$$
n=\mathcal O\!\big(m_0\log(1/\epsilon)\big).
$$

## Practical Tuning

- **TD horizon $m$:** choose $m=\Theta(\log_{1/\gamma}(1/\epsilon))$ to make aliasing $\lesssim\epsilon$.
- **Critic steps $K$ and actor inner steps $N$:** about $\Theta(\epsilon^{-4})$.
- **Features:** richer features shrink the compatible approximation term.
- **Window $n$:** grows like $\mathcal O(\log(1/\epsilon))$ under filter stability.

## What’s next: RNN-based NAC for POMDPs

A natural extension replaces the hand-engineered memory $Z_k$ with a **recurrent hidden state** $H_k$ learned end-to-end (RNN-based NAC). The policy becomes $\pi_\theta(a\mid Y_k, H_k)$ with $H_{k+1}=f_\theta(H_k,Y_{k+1},A_k)$, and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs. representation power, and how partial-observability error shows up) in a follow-up post about our **RNN-based natural actor–critic** paper.

---
