---
title: "Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers"
date: 2025-10-12
description: "Natural actor–critic with finite-state controllers for POMDPs."
categories: [reinforcement-learning, POMDP, actor-critic, theory]
toc: true
draft: false
---

> **TL;DR.** In [@cayci2024nac], we studied a natural actor–critic (NAC) method in POMDPs. It learns finite-memory policies, evaluates them with an $m$-step TD critic to reduce perceptual aliasing, and gives a performance bound that separates optimization/approximation, evaluation, and inference (partial observability) errors. For sliding-window controllers, the inference error decays geometrically with the window size under certain ergodicity conditions.

## Formal POMDP model

We consider a discounted POMDP
$$
\mathcal{M}=\big(\mathcal S,\mathcal A,\mathcal Y,\; P,\; O,\; r,\; \gamma,\; \xi\big),
$$
with:
- latent state $S_k\in\mathcal S$,
- action $A_k\in\mathcal A$,
- observation $Y_k\in\mathcal Y$,
- transition kernel $P(s' \mid s,a)$,
- observation kernel $\Phi(y \mid s')$ (can depend on $a$ in general),
- reward $r(s,a)\in[0,1]$ (w.l.o.g.),
- discount $\gamma\in(0,1)$ and prior $\xi$ over $S_0$.
For the sake of simplicity, let us consider finite $\mathcal S, \mathcal Y, \mathcal A$. The generative process is
$$
S_{k+1}\sim P(\cdot\mid S_k,A_k),\qquad
Y_{k}\sim \Phi(\cdot\mid S_k),\qquad
R_k=r(S_k,A_k),
$$
and the goal is to maximize
$$
\mathcal V^\pi(\xi)=\mathbb E_\xi^\pi\!\left[\sum_{k\ge0}\gamma^k\,r(S_k,A_k)\right].
$$

**Why challenging?**  
Partial observability implies that the optimal policy $\pi^\star=(\pi_0^\star,\pi_1^\star,\ldots)$ is non-stationary, therefore $\pi_k^\star$ depends on the complete trajectory $(Y_{0:k},A_{0:k-1})$ for each $k\in\mathbb N$ via the Bayes belief $b_k(s)=\Pr(S_k=s\mid H_k)$. A classical result that is attributed to [astrom1965] shows that a POMDP can be formulated as an MDP with a distribution-valued state $b_k$. The paper sidesteps this by learning **finite-memory controllers** and using **$m$-step TD** to reduce aliasing.

## Setting and Goal

We restrict policies to **finite-state controllers (FSCs)** that keep internal memory $Z_k$ and act from $(Y_k, Z_k)$.

- Learn the best policy within a fixed FSC class $\Pi_{Z,\varphi}$:
  
  $$
  \pi^\star \in \arg\max_{\pi \in \Pi_{Z,\varphi}} \; \mathcal V^\pi(\xi).
  $$

- A useful subclass is the **sliding-window controller (SWC)** with window $n$:
  
  $$
  Z_k = (Y_{k-n:k-1},\; A_{k-n:k-1}) \in \mathcal Y^n \times \mathcal A^n,
  $$
  
  which summarizes the last $n$ observations/actions.

The actor uses a **linear-softmax FSC**:
$$
\pi_\theta(a \mid y,z) = 
\frac{\exp\{\theta^\top \psi(a,y,z)\}}
{\sum_{a'} \exp\{\theta^\top \psi(a',y,z)\}},
$$
with features $\psi(a,y,z)\in\mathbb R^d$. The controller memory updates via some $\varphi$:
$$
Z_{k+1}=\varphi(Z_k, Y_{k+1}, A_k).
$$

## Sampling Measure (for Analysis)

Let the **discounted visitation** over information states be
$$
d^\pi_\xi(y,z)=(1-\gamma)\sum_{k\ge0}\gamma^k\,
\Pr^\pi\!\big[(Y_k,Z_k)=(y,z)\,\big|\,H_0\sim\xi\big].
$$
The analysis assumes access to samples from $d^\pi_\xi$ (standard in NAC to match the Fisher metric).

## Algorithm Sketch (FS-NAC)

1. **Critic** — run **$m$-step TD(0)** with linear function approximation to estimate $\widehat{\mathcal Q}^{\pi_t}(y,z,a)$.
2. **Actor** — take a **natural-gradient** step using the critic’s advantage estimate.

::: callout-tip
**Why $m$-step TD?** In POMDPs, one-step bootstrapping suffers from **perceptual aliasing** (same observation $Y_k$, different latent state $S_k$). Looking $m$ steps ahead stabilizes the target and reduces this bias.
:::

## Critic Guarantee (Policy Evaluation)

With stepsize $\alpha=K^{-1/2}$ and parameter radius $R$, after $K$ critic updates:
$$
\mathbb E\!\left[\big\|\mathcal Q^\pi-\widehat{\mathcal Q}^\pi_K\big\|_{2,\,d^\pi_\xi\otimes\pi}\right]
\;\lesssim\;
\underbrace{\tfrac{K^{-1/4}}{1-\gamma}}_{\text{statistical}}
+\underbrace{\tfrac{\varepsilon_{\text{app}}(R)}{1-\gamma^m}}_{\text{approximation}}
+\underbrace{\varepsilon_{\text{pa}}(\gamma,m,R)}_{\text{aliasing}},
$$
where $\varepsilon_{\text{app}}(R)$ is the best linear-approximation error within radius $R$, and the aliasing term contracts geometrically:
$$
\varepsilon_{\text{pa}}(\gamma,m,R)=\mathcal O\!\big(\gamma^{m/2}\,\mathrm{poly}(R,(1-\gamma)^{-1})\big).
$$

**Sample–accuracy trade-off.** Each update uses $m$ samples, so total evaluation samples are $\Theta(mK)$.

## Full Actor–Critic Performance (Finite-Time)

After $T$ outer iterations (with tuned stepsizes),
$$
(1-\gamma)\,\min_{t<T}\mathbb E\!\big[\mathcal V^{\pi^\star}(\xi)-\mathcal V^{\pi_t}(\xi)\big]
\;\lesssim\;
\underbrace{T^{-1/2}}_{\text{actor optimization}}
+\underbrace{\varepsilon_{\text{critic}}(K,m,R)}_{\text{TD evaluation}}
+\underbrace{\varepsilon_{\text{actor}}(N,R)}_{\text{compatible approx.}}
+\underbrace{\varepsilon_{\text{inf}}(\xi)}_{\text{inference penalty}}.
$$

The **inference error** is the price of partial observability:
$$
\varepsilon_{\text{inf}}(\xi)
= \mathbb E\!\left[\sum_{k\ge0}\gamma^k
\left\|\,b_k(\cdot)-b_0(\cdot,I_k)\,\right\|_{\mathrm{TV}}
\right],\qquad I_k=(Y_k,Z_k).
$$

## Sliding-Window Controllers (Memory vs. Accuracy)

Under stochastic exploration and filter stability/minorization, the inference error **decays geometrically** with window length $n$:
$$
\varepsilon_{\text{inf}}(\xi)\;\le\;\frac{1}{1-\gamma}\cdot
\mathcal O\!\Big(\rho^{\lfloor n/m_0\rfloor}\Big),
$$
for constants $\rho\in(0,1)$ and $m_0\ge1$. To reach tolerance $\epsilon$:
$$
n=\mathcal O\!\big(m_0\log(1/\epsilon)\big).
$$

## Practical Tuning

- **TD horizon $m$:** choose $m=\Theta(\log_{1/\gamma}(1/\epsilon))$ to make aliasing $\lesssim\epsilon$.
- **Critic steps $K$ and actor inner steps $N$:** about $\Theta(\epsilon^{-4})$.
- **Features:** richer features shrink the compatible approximation term.
- **Window $n$:** grows like $\mathcal O(\log(1/\epsilon))$ under filter stability.

## What’s next: RNN-based NAC for POMDPs

A natural extension replaces the hand-engineered memory $Z_k$ with a **recurrent hidden state** $H_k$ learned end-to-end (RNN-based NAC). The policy becomes $\pi_\theta(a\mid Y_k, H_k)$ with $H_{k+1}=f_\theta(H_k,Y_{k+1},A_k)$, and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs. representation power, and how partial-observability error shows up) in a follow-up post about our **RNN-based natural actor–critic** paper.

---
