<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-12">
<meta name="description" content="How does multi-step TD learning with internal states mitigate partial observability?">

<title>RL for POMDPs – Part I: TD Learning with Internal States – Notes on Theoretical RL</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-355c415dd6b9922ddaf04a2341011fe1.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Notes on Theoretical RL</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../posts/index.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#formal-pomdp-model" id="toc-formal-pomdp-model" class="nav-link active" data-scroll-target="#formal-pomdp-model"><span class="header-section-number">1</span> Formal POMDP model</a></li>
  <li><a href="#setting-and-goal" id="toc-setting-and-goal" class="nav-link" data-scroll-target="#setting-and-goal"><span class="header-section-number">2</span> Setting and Goal</a></li>
  <li><a href="#hidden-markov-reward-processes-hmrp" id="toc-hidden-markov-reward-processes-hmrp" class="nav-link" data-scroll-target="#hidden-markov-reward-processes-hmrp"><span class="header-section-number">3</span> Hidden Markov Reward Processes (HMRP)</a></li>
  <li><a href="#critic-m-step-td-learning-with-an-internal-state" id="toc-critic-m-step-td-learning-with-an-internal-state" class="nav-link" data-scroll-target="#critic-m-step-td-learning-with-an-internal-state"><span class="header-section-number">4</span> Critic: <span class="math inline">\(m\)</span>-step TD Learning with an Internal State</a>
  <ul class="collapse">
  <li><a href="#learning-vz_0-by-m-step-td0-with-an-internal-state" id="toc-learning-vz_0-by-m-step-td0-with-an-internal-state" class="nav-link" data-scroll-target="#learning-vz_0-by-m-step-td0-with-an-internal-state"><span class="header-section-number">4.1</span> Learning <span class="math inline">\(V(z_0)\)</span> by <span class="math inline">\(m\)</span>-step TD(0) with an Internal State</a></li>
  </ul></li>
  <li><a href="#full-actorcritic-performance-finite-time" id="toc-full-actorcritic-performance-finite-time" class="nav-link" data-scroll-target="#full-actorcritic-performance-finite-time"><span class="header-section-number">5</span> Full Actor–Critic Performance (Finite-Time)</a></li>
  <li><a href="#sliding-window-controllers-memory-vs.-accuracy" id="toc-sliding-window-controllers-memory-vs.-accuracy" class="nav-link" data-scroll-target="#sliding-window-controllers-memory-vs.-accuracy"><span class="header-section-number">6</span> Sliding-Window Controllers (Memory vs.&nbsp;Accuracy)</a></li>
  <li><a href="#practical-tuning" id="toc-practical-tuning" class="nav-link" data-scroll-target="#practical-tuning"><span class="header-section-number">7</span> Practical Tuning</a></li>
  <li><a href="#whats-next-rnn-based-nac-for-pomdps" id="toc-whats-next-rnn-based-nac-for-pomdps" class="nav-link" data-scroll-target="#whats-next-rnn-based-nac-for-pomdps"><span class="header-section-number">8</span> What’s next: RNN-based NAC for POMDPs</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">RL for POMDPs – Part I: TD Learning with Internal States</h1>
  <div class="quarto-categories">
    <div class="quarto-category">reinforcement-learning</div>
    <div class="quarto-category">POMDP</div>
    <div class="quarto-category">temporal difference learning</div>
  </div>
  </div>

<div>
  <div class="description">
    How does multi-step TD learning with internal states mitigate partial observability?
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 12, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><strong>TL;DR.</strong> A classic result by <span class="citation" data-cites="singh1994learning">(<a href="#ref-singh1994learning" role="doc-biblioref">Singh, Jaakkola, and Jordan 1994</a>)</span> shows that memoryless TD learning fails in POMDPs. To address this, we studied a natural actor–critic (NAC) method for POMDPs in <span class="citation" data-cites="cayci2024nac">(<a href="#ref-cayci2024nac" role="doc-biblioref">Cayci, He, and Srikant 2024</a>)</span>, with a critic that uses <span class="math inline">\(m\)</span>-step TD learning with an internal state to mitigate perceptual aliasing problem. Below, I review this algorithm and provide some follow-up results.</p>
</blockquote>
<section id="formal-pomdp-model" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="formal-pomdp-model"><span class="header-section-number">1</span> Formal POMDP model</h2>
<p>Let us consider a discounted POMDP <span class="math display">\[
\mathcal{M}=\big(\mathcal S,\mathcal A,\mathcal Y,\; P,\; O,\; r,\; \gamma\big),
\]</span></p>
<p>with:</p>
<ul>
<li>state <span class="math inline">\(S_k\in\mathcal S\)</span>,</li>
<li>action <span class="math inline">\(A_k\in\mathcal A\)</span>,</li>
<li>observation <span class="math inline">\(Y_k\in\mathcal Y\)</span>,</li>
<li>transition kernel <span class="math inline">\(P(s' \mid s,a)\)</span>,</li>
<li>observation kernel <span class="math inline">\(\Phi(y \mid s')\)</span>,</li>
<li>reward <span class="math inline">\(r(s,a)\in[0,1]\)</span> (w.l.o.g.),</li>
<li>discount factor <span class="math inline">\(\gamma\in(0,1)\)</span>.</li>
</ul>
<p>To avoid measure-theoretic concerns, let us consider finite <span class="math inline">\(\mathcal S, \mathcal Y, \mathcal A\)</span>. These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is <span class="math display">\[
S_{k+1}\sim P(\cdot\mid S_k,A_k),\qquad
Y_{k}\sim \Phi(\cdot\mid S_k),\qquad
R_k=r(S_k,A_k),
\]</span> and the goal is to maximize <span class="math display">\[
\mathcal V^\pi(\xi)=\mathbb E_\xi^\pi\!\left[\sum_{k\ge0}\gamma^k\,r(S_k,A_k)\right].
\]</span></p>
<p><strong>Why challenging?</strong><br>
Partial observability implies that the optimal policy <span class="math inline">\(\pi^\star=(\pi_0^\star,\pi_1^\star,\ldots)\)</span> is non-stationary, therefore <span class="math inline">\(\pi_k^\star\)</span> depends on the complete trajectory <span class="math inline">\((Y_{0:k},A_{0:k-1})\)</span> for each <span class="math inline">\(k\in\mathbb N\)</span> via the Bayes belief <span class="math inline">\(b_k(s)=\Pr(S_k=s\mid Y_{0:k},A_{0:k-1})\)</span>. Reactive (i.e., memoryless) methods for MDPs fail miserably if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see <strong>Example 1</strong> below). A classical result attributed to <span class="citation" data-cites="astrom1965">(<a href="#ref-astrom1965" role="doc-biblioref">Åström 1965</a>)</span> (see <span class="citation" data-cites="yuksel2025another">(<a href="#ref-yuksel2025another" role="doc-biblioref">Yüksel 2025</a>)</span> for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state <span class="math inline">\(b_k\)</span>. However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over <span class="math inline">\(\mathcal S\)</span>), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies <span class="citation" data-cites="murphy2000survey">(<a href="#ref-murphy2000survey" role="doc-biblioref">Murphy 2000</a>)</span> and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state <span class="math inline">\(Z_k^c\)</span>, which summarizes/compresses <span class="math inline">\((Y_{0:k-1},A_{0:k-1})\)</span> for each <span class="math inline">\(k\)</span>. The idea is to use a parametric policy <span class="math inline">\(\pi_k(\cdot\mid Y_k, Z_k^c)\)</span> based on the internal state <span class="math inline">\(Z_k^c\)</span>.</p>
</section>
<section id="setting-and-goal" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="setting-and-goal"><span class="header-section-number">2</span> Setting and Goal</h2>
<p>At first, we restrict policies to <strong>finite-state controllers (FSCs)</strong> that keep internal memory <span class="math inline">\(Z_k^c\)</span> and act from <span class="math inline">\((Y_k, Z_k^c)\)</span>. The internal memory is updated in a Markovian way: <span class="math inline">\(Z_{k+1}^c\sim\varphi(\cdot\mid z_k, y_k, a_k)\)</span> given <span class="math inline">\(Z_k^c=z_k,Y_k=y_k,A_k=a_k\)</span>.</p>
<ul>
<li><p>Learn the best policy within a fixed FSC class <span class="math inline">\(\pi_{Z,\varphi}\)</span>:</p>
<p><span class="math display">\[
\pi^\star \in \arg\max_{\pi \in \pi_{Z,\varphi}} \; \mathcal V^\pi(\xi).
\]</span></p></li>
<li><p>A useful subclass is the <strong>sliding-window controller (SWC)</strong> with window <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
Z_k^c = (Y_{k-n:k-1},\; A_{k-n:k-1}) \in \mathcal Y^n \times \mathcal A^n,
\]</span></p>
<p>which summarizes the last <span class="math inline">\(n\)</span> observations/actions, where <span class="math inline">\(n\)</span> is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.</p></li>
</ul>
<p>The actor uses a <strong>linear-softmax FSC</strong>: <span class="math display">\[
\pi_\theta(a \mid y,z) =
\frac{\exp\{\theta^\top \psi(a,y,z)\}}
{\sum_{a'} \exp\{\theta^\top \psi(a',y,z)\}},
\]</span> with features <span class="math inline">\(\psi(a,y,z)\in\mathbb R^d\)</span>. The controller memory updates via some <span class="math inline">\(\varphi\)</span>: <span class="math display">\[
Z_{k+1}^c=\varphi(Z_k^c, Y_{k+1}, A_k).
\]</span></p>
</section>
<section id="hidden-markov-reward-processes-hmrp" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="hidden-markov-reward-processes-hmrp"><span class="header-section-number">3</span> Hidden Markov Reward Processes (HMRP)</h2>
<p>A common simplification in TD learning analyses in an MDP setting is to consider a Markov reward process (MRP). Since the augmented state <span class="math inline">\(X_k:=(S_k,A_k)\)</span> under a given stationary policy forms a Markov chain, the analyses automatically extend to policy evaluation (known as SARSA) since <span class="math inline">\((X_k,r(X_k))\)</span> forms an MRP.</p>
<p>In POMDPs, we can make a similar simplification. First, let us consider a Hidden Markov Process with a Markov chain <span class="math inline">\(S_k\)</span> and its partial observation process <span class="math inline">\(Y_k\sim\Phi(\cdot\mid S_k)\)</span>. Along with the reward <span class="math inline">\(r(S_k)\)</span>, this HMP induces a Hidden Markov Reward Process (HMRP) <span class="math inline">\(\{(S_k,r(S_k),Y_k):k\in\mathbb{N}\}\)</span>.</p>
<p>Consider a finite-state controller with an internal state <span class="math inline">\(Z_k^c\)</span>. Then, we have <span class="math inline">\(A_k \sim \pi_k(\cdot\mid Z_k^c)\)</span>, thus <span class="math inline">\(X_k:=(S_k,A_k,Z_k^c)\)</span> forms a Markov process with the transition kernel <span class="math display">\[P(S_{k+1}=s',A_{k+1}=a',Z_{k+1}^c=z'|S_k,A_k,Z_k^c) = \sum_{y'\in\mathcal Y}P(s'|S_k,A_k)\Phi(y'|s')\varphi(z'|Z_k^c,y',A_k)\pi_{k+1}(a'\mid y',z').\]</span> The noisy observation is <span class="math inline">\(Y_k~\Phi(\cdot|S_k)\)</span>. Defining <span class="math inline">\(\tilde{r}(S_k,A_k,Z_k^c) = r(S_k,A_k)\)</span>, we obtain the original process under <span class="math inline">\(\pi\)</span>. Thus, the controlled process under <span class="math inline">\(\pi\)</span> induces the HMRP <span class="math inline">\((X_k, \tilde{r}(X_k), Y_k)\)</span>.</p>
</section>
<section id="critic-m-step-td-learning-with-an-internal-state" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="critic-m-step-td-learning-with-an-internal-state"><span class="header-section-number">4</span> Critic: <span class="math inline">\(m\)</span>-step TD Learning with an Internal State</h2>
<p>It is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology <span class="citation" data-cites="singh1994learning">(<a href="#ref-singh1994learning" role="doc-biblioref">Singh, Jaakkola, and Jordan 1994</a>)</span>. For a concrete demonstration, let us focus on learning the value function in an HMRP: <span class="math display">\[V(z_0):=\mathbb E[\sum_{k=0}^\infty \gamma^k r(S_k)|Z_0=z_0]\mbox{ and }V_s^\star:=\mathbb E[\sum_{k=0}^\infty r(S_k)|S_0=s]\]</span> for an internal state <span class="math inline">\(Z_k\)</span> (e.g., <span class="math inline">\(Z_t = (Y_{t-n+1},\ldots,Y_t)\)</span>), where <span class="math inline">\((S_k, r(S_k), Y_k)\)</span> is an HMRP.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Two objectives.</strong> In the above display, if we want to perform policy evaluation for an FSC with <span class="math inline">\(\{Z_k^c:k\in\mathbb N\}\)</span>, we would be interested in <span class="math inline">\(V(z_0)\)</span>. Additionally, we can use <span class="math inline">\(m\)</span>-step TD Learning for estimating the value function <span class="math inline">\(V_s^\star\)</span>. We discuss both problems in what follows.</p>
</div>
</div>
<p><strong>Example 1 (When TD(0) fails to learn <span class="math inline">\(V_s^\star\)</span>).</strong> Here we consider an HMRP with <span class="math inline">\(\mathcal S = \{0,1\}\)</span>, <span class="math inline">\(r(s) = s\)</span> and <span class="math inline">\(\Phi(1|s) = 1\)</span> for all <span class="math inline">\(s\in\mathcal{S}\)</span>. Let <span class="math inline">\(P(0|1) = q\)</span> and <span class="math inline">\(P(1|0) = p\)</span> for some <span class="math inline">\(p, q \in (0,1)\)</span>, and <span class="math inline">\(\gamma \in (0, 1)\)</span> be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to <span class="math inline">\(v^\star = \frac{1}{1-\gamma}\cdot \frac{p}{q+p} = V_0^\star + \frac{p}{p+q}(V_1^\star-V_0^\star)\)</span>. In the proof of Theorem 4.1 in <span class="citation" data-cites="cayci2024nac">(<a href="#ref-cayci2024nac" role="doc-biblioref">Cayci, He, and Srikant 2024</a>)</span>, we characterize the issues related to the fixed point of single-step TD(0) for general POMDPs.</p>
<p>On the other hand, the value function for a given <span class="math inline">\(z_0\)</span> is <span class="math display">\[V(z_0) = V_0^\star + (V_1^\star-V_0^\star)P(S_0=1|Z_0=z_0) = \frac{\gamma p + (1-\gamma)P(S_0=1|Z_0=z_0)}{(1-\gamma)\Big[\gamma(p+q)+1-\gamma\Big]},\]</span> since <span class="math inline">\(V(z_0) = V_0^\star+(V_1^\star-V_0^\star)P(S_0=1|Z_0=z_0)\)</span>. Also, <span class="math inline">\(V_{S_0}^\star = V_0^\star + (V_1^\star-V_0^\star)\mathbf{1}_{\{S_0=1\}}\)</span>. Therefore, <span class="math display">\[\mathbb{E}[(V_{S_0}^\star-v^\star)^2] = (V_1^\star-V_0^\star)^2\mathbb{E}[(\frac{p}{p+q}-\mathbf{1}_{\{S_0=1\}})^2]=Var(\mathbf{1}_{\{S_0=1\}})(V_1^\star-V_0^\star)^2.\]</span></p>
<p><strong>Example 2 (When TD(0) fails to learn <span class="math inline">\(V(z_0)\)</span>).</strong> Consider the two-state HMRP with <span class="math inline">\(\mathcal S = \{0,1\}\)</span>, <span class="math inline">\(r(s) = s\)</span> and <span class="math inline">\(\Phi(1|s) = c_s\in(0,1)\)</span> now. Let <span class="math inline">\(P(0|1) = P(1|0) = 1/2\)</span>, and <span class="math inline">\(\gamma = 0.9\)</span>. Consider an internal state <span class="math inline">\(Z_k = (Y_{k-1}, Y_k)\)</span>. We want to compute <span class="math inline">\(V(z_0) = \mathbb{E}[\sum_k \gamma^k r(S_k)|Z_0=z_0]\)</span>. For this purpose, one can use tabular-TD(0) with the surrogate state <span class="math inline">\(Z_k=(Y_{k-1},Y_k)\)</span>. In this case, the limit point of TD(0) would be the unique fixed point of <span class="math display">\[v_1(z_0) = P(S_0=1|Z_0=z_0) + \gamma \sum_{z_1\in\{0,1\}^2}v(z_1)P(Z_1=z_1|Z_0=z_0).\]</span> Substituting <span class="math inline">\(c_0 = 1/4, c_1 = 3/4\)</span>, we obtain the fixed point <span class="math display">\[
v_1(z_0) =
\begin{cases}
4.1396, &amp; z_0 = (0,0), \\
5.2739, &amp; z_0 = (0,1), \\
4.7261, &amp; z_0 = (1,0), \\
5.8604, &amp; z_0 = (1,1).
\end{cases}
\]</span> On the other hand, the true value function <span class="math inline">\(V(z_0)\)</span> is as follows: <span class="math display">\[
V(z_0) =
\begin{cases}
1.6306, &amp; z_0 = (0,0), \\
5.0561, &amp; z_0 = (0,1), \\
4.9439, &amp; z_0 = (1,0), \\
8.3694, &amp; z_0 = (1,1).
\end{cases}
\]</span> This indicates that a naive application of TD(0) with the internal state <span class="math inline">\(\{Z_k:k\in\mathbb N\}\)</span> does not return the true value function <span class="math inline">\(V(z_0)\)</span>. Thus, in case we want to evaluate the performance of a policy with an internal state <span class="math inline">\(Z_k^c = (Y_{k-1}, A_{k-1})\)</span>, TD(0) is not useful.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Why <span class="math inline">\(m\)</span>-step TD?</strong> Examples 1 and 2 show that, in POMDPs, one-step bootstrapping suffers from <strong>perceptual aliasing</strong> (same observation <span class="math inline">\(Y_k\)</span>, different latent state <span class="math inline">\(S_k\)</span>). We will see in the following that looking <span class="math inline">\(m\)</span> steps ahead stabilizes the target and reduces this bias.</p>
</div>
</div>
<p>Now, let us focus on <span class="math inline">\(m\)</span>-step TD learning with an internal state <span class="math inline">\(Z_k\)</span> for the above problem, where <span class="math inline">\(m\in\mathbb{Z}_+\)</span>. Note that the internal state may match the controller state <span class="math inline">\(Z_k^c\)</span> (in the case of control, see Section 3), or it can be used to obtain an estimate of <span class="math inline">\(V_s^\star\)</span>.</p>
<section id="learning-vz_0-by-m-step-td0-with-an-internal-state" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="learning-vz_0-by-m-step-td0-with-an-internal-state"><span class="header-section-number">4.1</span> Learning <span class="math inline">\(V(z_0)\)</span> by <span class="math inline">\(m\)</span>-step TD(0) with an Internal State</h3>
<p>Let <span class="math display">\[(T_mv)(z) = \bar{r}_m(z) + \gamma^m(K_mv)(z),\]</span> where <span class="math inline">\(\bar{r}_m(z) := \mathbb E[\sum_{k=0}^{m-1}r(S_k)|Z_0=z]\)</span> and <span class="math inline">\((K_mv)(z):=\mathbb{E}[v(Z_m)|Z_0=z]\)</span>. Then, it is straightforward to show that <span class="math inline">\(T_m\)</span> is a contractive operator with modulus <span class="math inline">\(\gamma^m\)</span>, and admits a unique fixed point <span class="math display">\[V(z) = \mathbb E[\sum_{k=0}^\infty \gamma^k r(S_k)|Z_0=z] = \sum_{k=0}^\infty (\gamma^mK_m)^k\bar{r}_m(z)\]</span> for any <span class="math inline">\(m\in\mathbb N\)</span>. Let <span class="math inline">\(\xi\)</span> be the stationary distribution of <span class="math inline">\(\{Z_k:k\in\mathbb N\}\)</span>, which exists given <span class="math inline">\(S_k\)</span> is an ergodic unichain. Under a non-degenerate <span class="math inline">\(\mathbb{E}_{Z_t\sim \xi}[\psi(Z_t)\otimes\psi(Z_t)]\succ 0\)</span>, <span class="math inline">\(m\)</span>-step TD learning with the internal state <span class="math inline">\(Z_t\)</span> converges to the unique fixed point <span class="math inline">\(v_m\)</span> of the equation <span class="math display">\[v = \Pi\{\bar{r}_m+\gamma^m K_m v\},\]</span> where <span class="math inline">\(\Pi\)</span> be the ortogonal projection onto the subspace <span class="math inline">\(\mathcal V := \{z\mapsto \langle \psi(z),w\rangle: w \in \mathbb R^d\)</span> with respect to <span class="math inline">\(L^2(\xi)\)</span>. Thus, we have the following result.</p>
<div class="theorem">
<p>For any <span class="math inline">\(m\in\mathbb{Z}_+\)</span>, <span class="math inline">\(m\)</span>-step TD(0) with an internal state <span class="math inline">\(\{Z_k:k\in\mathbb N\}\)</span> converges to <span class="math inline">\(v_m\)</span>, which satisfies <span class="math display">\[\|v_m - \Pi V\|_{L^2(\xi)} \leq \frac{\gamma^m}{1+\gamma^m}\|V-\Pi V\|_{L^2(\xi)}.\]</span></p>
</div>
<p>As such, in case we are interested in policy evaluation for an FSC, this formulation implies the near optimality of <span class="math inline">\(m\)</span>-step TD learning, which uses the internal state of the controller as the critic internal state, with an additional error term <span class="math inline">\(\mathcal{O}(\gamma^m)\)</span>.</p>
<p>The analysis can be taken one step further indeed. The argument below can be shifted in time, so let us consider <span class="math inline">\(t=0\)</span> for notational simplicity. Let <span class="math inline">\(\mathcal{F}_m :=\sigma(Z_0)=\sigma(Y_{-m+1},\ldots,Y_0)\)</span> be the <span class="math inline">\(\sigma\)</span>-algebra generated by the internal state, and <span class="math inline">\(b^{(m)} := P(S_0 = 1|\mathcal{F}_m)\)</span>. Then, <span class="math inline">\(V(z_0) =  \mathbb E[\sum_{t=0}^\infty\gamma^tr(S_t)|\mathcal{F}_m]\)</span>. Since <span class="math inline">\(V_s^\star = V_0^\star + (V_1^\star-V_0^\star)\mathbf{1}_{\{S_0=s\}}\)</span> and <span class="math inline">\(V(z_0) = V_0^\star + (V_1^\star-V_0^\star)b^{(m)}\)</span>, we obtain <span class="math display">\[\mathbb{E}[(V_{S_0}^\star-V(z_0))^2] = (V_1^\star-V_0^\star)^2\,\mathbb E\!\left[(\mathbf{1}_{\{S_0=1\}}-b^{(m)})^2\right].\]</span> Furthermore, since <span class="math inline">\(b^{(m)}=\mathbb E[\mathbf{1}_{\{S_0=1\}}|\mathcal F_m]\)</span>, we have</p>
<p><span class="math display">\[\begin{align}
\mathbb E\!\left[(\mathbf{1}_{\{S_0=1\}}-b^{(m)})^2\right]&amp;=\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_m)]\\
&amp;\downarrow \mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_\infty)]
\end{align}\]</span> as <span class="math inline">\(m\rightarrow\infty\)</span> due to the law of total variance for nested <span class="math inline">\(\sigma\)</span>-algebras <span class="math inline">\(\mathcal{F}_m\)</span>. Therefore, <span class="math display">\[\mathbb{E}[(V_{s_0}^\star-V(z_0))^2] \downarrow (V_1^\star-V_0^\star)^2\,\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_\infty)].\]</span> If <span class="math inline">\(S_0\)</span> is measurable with respect to <span class="math inline">\(\sigma(Y_{-\infty},\ldots,Y_0)\)</span>, we have <span class="math inline">\(V_{s_0}^\star=V(z_0))\)</span> almost surely. Used in conjunction with the previous result on <span class="math inline">\(v_m-V(z_0)\)</span>, a simple triangle inequality would demonstrate the effectiveness of TD learning with sliding-window controllers.</p>
<p>In summary, we have <span class="math display">\[\begin{align*}
  \mathbb{E}[(V_{S_0}^\star - v^\star)^2] &amp;= (V_1^\star-V_0^\star)^2 Var(\mathbf{1}_{\{S_0=1\}})\\
&amp;\geq (V_1^\star-V_0^\star)^2\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_m)] \downarrow (V_1^\star-V_0^\star)^2\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_\infty)].
\end{align*}\]</span></p>
<p>With stepsize <span class="math inline">\(\alpha=K^{-1/2}\)</span> and parameter radius <span class="math inline">\(R\)</span>, after <span class="math inline">\(K\)</span> critic updates: <span class="math display">\[
\sqrt{\mathbb E\!\left[\big\|\mathcal Q^\pi-\widehat{\mathcal Q}^\pi_K\big\|^2_{\,d^\pi_\xi\otimes\pi}\right]}
\;\lesssim\;
\underbrace{\tfrac{K^{-1/4}}{1-\gamma}}_{\text{statistical}}
+\underbrace{\tfrac{\varepsilon_{\text{app}}(R)}{1-\gamma^m}}_{\text{approximation}}
+\underbrace{\varepsilon_{\text{pa}}(\gamma,m,R)}_{\text{aliasing}},
\]</span> where <span class="math inline">\(\varepsilon_{\text{app}}(R)\)</span> is the best linear-approximation error within radius <span class="math inline">\(R\)</span>, and the aliasing term contracts geometrically: <span class="math display">\[
\varepsilon_{\text{pa}}(\gamma,m,R)=\mathcal O\!\big(\gamma^{m/2}\,\mathrm{poly}(R,(1-\gamma)^{-1})\big).
\]</span></p>
<p><strong>Sample–accuracy trade-off.</strong> Each update uses <span class="math inline">\(m\)</span> samples, so total evaluation samples are <span class="math inline">\(\Theta(mK)\)</span>.</p>
</section>
</section>
<section id="full-actorcritic-performance-finite-time" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="full-actorcritic-performance-finite-time"><span class="header-section-number">5</span> Full Actor–Critic Performance (Finite-Time)</h2>
<p>After <span class="math inline">\(T\)</span> outer iterations (with tuned stepsizes), <span class="math display">\[
(1-\gamma)\,\min_{t&lt;T}\mathbb E\!\big[\mathcal V^{\pi^\star}(\xi)-\mathcal V^{\pi_t}(\xi)\big]
\;\lesssim\;
\underbrace{T^{-1/2}}_{\text{actor optimization}}
+\underbrace{\varepsilon_{\text{critic}}(K,m,R)}_{\text{TD evaluation}}
+\underbrace{\varepsilon_{\text{actor}}(N,R)}_{\text{compatible approx.}}
+\underbrace{\varepsilon_{\text{inf}}(\xi)}_{\text{inference penalty}}.
\]</span></p>
<p>The <strong>inference error</strong> is the price of partial observability: <span class="math display">\[
\varepsilon_{\text{inf}}(\xi)
= \mathbb E\!\left[\sum_{k\ge0}\gamma^k
\left\|\,b_k(\cdot)-b_0(\cdot,I_k)\,\right\|_{\mathrm{TV}}
\right],\qquad I_k=(Y_k,Z_k).
\]</span></p>
</section>
<section id="sliding-window-controllers-memory-vs.-accuracy" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="sliding-window-controllers-memory-vs.-accuracy"><span class="header-section-number">6</span> Sliding-Window Controllers (Memory vs.&nbsp;Accuracy)</h2>
<p>Under stochastic exploration and filter stability/minorization, the inference error <strong>decays geometrically</strong> with window length <span class="math inline">\(n\)</span>: <span class="math display">\[
\varepsilon_{\text{inf}}(\xi)\;\le\;\frac{1}{1-\gamma}\cdot
\mathcal O\!\Big(\rho^{\lfloor n/m_0\rfloor}\Big),
\]</span> for constants <span class="math inline">\(\rho\in(0,1)\)</span> and <span class="math inline">\(m_0\ge1\)</span>. To reach tolerance <span class="math inline">\(\epsilon\)</span>: <span class="math display">\[
n=\mathcal O\!\big(m_0\log(1/\epsilon)\big).
\]</span></p>
</section>
<section id="practical-tuning" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="practical-tuning"><span class="header-section-number">7</span> Practical Tuning</h2>
<ul>
<li><strong>TD horizon <span class="math inline">\(m\)</span>:</strong> choose <span class="math inline">\(m=\Theta(\log_{1/\gamma}(1/\epsilon))\)</span> to make aliasing <span class="math inline">\(\lesssim\epsilon\)</span>.</li>
<li><strong>Critic steps <span class="math inline">\(K\)</span> and actor inner steps <span class="math inline">\(N\)</span>:</strong> about <span class="math inline">\(\Theta(\epsilon^{-4})\)</span>.</li>
<li><strong>Features:</strong> richer features shrink the compatible approximation term.</li>
<li><strong>Window <span class="math inline">\(n\)</span>:</strong> grows like <span class="math inline">\(\mathcal O(\log(1/\epsilon))\)</span> under filter stability.</li>
</ul>
</section>
<section id="whats-next-rnn-based-nac-for-pomdps" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="whats-next-rnn-based-nac-for-pomdps"><span class="header-section-number">8</span> What’s next: RNN-based NAC for POMDPs</h2>
<p>A natural extension replaces the hand-engineered memory <span class="math inline">\(Z_k\)</span> with a <strong>recurrent hidden state</strong> <span class="math inline">\(H_k\)</span> learned end-to-end (RNN-based NAC). The policy becomes <span class="math inline">\(\pi_\theta(a\mid Y_k, H_k)\)</span> with <span class="math inline">\(H_{k+1}=f_\theta(H_k,Y_{k+1},A_k)\)</span>, and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs.&nbsp;representation power, and how partial-observability error shows up) in a follow-up post about our <strong>RNN-based natural actor–critic</strong> paper.</p>
<hr>




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-astrom1965" class="csl-entry" role="listitem">
Åström, Karl J. 1965. <span>“Optimal Control of Markov Processes with Incomplete State Information i.”</span> <em>Journal of Mathematical Analysis and Applications</em> 10 (1): 174–205. <a href="https://doi.org/10.1016/0022-247X(65)90154-X">https://doi.org/10.1016/0022-247X(65)90154-X</a>.
</div>
<div id="ref-cayci2024nac" class="csl-entry" role="listitem">
Cayci, Semih, Niao He, and R. Srikant. 2024. <span>“Finite-Time Analysis of Natural Actor-Critic for POMDPs.”</span> <em>SIAM Journal on Mathematics of Data Science</em>.
</div>
<div id="ref-murphy2000survey" class="csl-entry" role="listitem">
Murphy, Kevin P. 2000. <span>“A Survey of POMDP Solution Techniques.”</span> <em>Environment</em> 2 (10).
</div>
<div id="ref-singh1994learning" class="csl-entry" role="listitem">
Singh, Satinder P, Tommi Jaakkola, and Michael I Jordan. 1994. <span>“Learning Without State-Estimation in Partially Observable Markovian Decision Processes.”</span> In <em>Machine Learning Proceedings 1994</em>, 284–92. Elsevier.
</div>
<div id="ref-yuksel2025another" class="csl-entry" role="listitem">
Yüksel, Serdar. 2025. <span>“Another Look at Partially Observed Optimal Stochastic Control: Existence, Ergodicity, and Approximations Without Belief-Reduction.”</span> <em>Applied Mathematics &amp; Optimization</em> 91 (1): 16.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/semihcayci\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>