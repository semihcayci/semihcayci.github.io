---
title: "RL for POMDPs – Part I: TD Learning with Internal States"
date: 2025-10-12
description: "How does multi-step TD learning with internal states mitigate partial observability?"
categories: [reinforcement-learning, POMDP, temporal difference learning]
toc: true
draft: false
---

> **TL;DR.** A classic result by [@singh1994learning] shows that memoryless TD learning fails in POMDPs. To address this, we studied a natural actor–critic (NAC) method for POMDPs in [@cayci2024nac], with a critic that uses $m$-step TD learning with an internal state to mitigate perceptual aliasing problem. Below, I review this algorithm and provide some follow-up results.

## Formal POMDP model

Let us consider a discounted POMDP
$$
\mathcal{M}=\big(\mathcal S,\mathcal A,\mathcal Y,\; P,\; O,\; r,\; \gamma\big),
$$

with:

- state $S_k\in\mathcal S$,
- action $A_k\in\mathcal A$,
- observation $Y_k\in\mathcal Y$,
- transition kernel $P(s' \mid s,a)$,
- observation kernel $\Phi(y \mid s')$,
- reward $r(s,a)\in[0,1]$ (w.l.o.g.),
- discount factor $\gamma\in(0,1)$.

To avoid measure-theoretic concerns, let us consider finite $\mathcal S, \mathcal Y, \mathcal A$. These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is
$$
S_{k+1}\sim P(\cdot\mid S_k,A_k),\qquad
Y_{k}\sim \Phi(\cdot\mid S_k),\qquad
R_k=r(S_k,A_k),
$$
and the goal is to maximize
$$
\mathcal V^\pi(\xi)=\mathbb E_\xi^\pi\!\left[\sum_{k\ge0}\gamma^k\,r(S_k,A_k)\right].
$$

**Why challenging?**  
Partial observability implies that the optimal policy $\pi^\star=(\pi_0^\star,\pi_1^\star,\ldots)$ is non-stationary, therefore $\pi_k^\star$ depends on the complete trajectory $(Y_{0:k},A_{0:k-1})$ for each $k\in\mathbb N$ via the Bayes belief $b_k(s)=\Pr(S_k=s\mid Y_{0:k},A_{0:k-1})$. Reactive (i.e., memoryless) methods for MDPs fail miserably if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see **Example 1** below). A classical result attributed to [@astrom1965] (see [@yuksel2025another] for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state $b_k$. However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over $\mathcal S$), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies [@murphy2000survey] and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state $Z_k^c$, which summarizes/compresses $(Y_{0:k-1},A_{0:k-1})$ for each $k$. The idea is to use a parametric policy $\pi_k(\cdot\mid Y_k, Z_k^c)$ based on the internal state $Z_k^c$.

## Setting and Goal

At first, we restrict policies to **finite-state controllers (FSCs)** that keep internal memory $Z_k^c$ and act from $(Y_k, Z_k^c)$. The internal memory is updated in a Markovian way: $Z_{k+1}^c\sim\varphi(\cdot\mid z_k, y_k, a_k)$ given $Z_k^c=z_k,Y_k=y_k,A_k=a_k$.

- Learn the best policy within a fixed FSC class $\pi_{Z,\varphi}$:
  
  $$
  \pi^\star \in \arg\max_{\pi \in \pi_{Z,\varphi}} \; \mathcal V^\pi(\xi).
  $$

- A useful subclass is the **sliding-window controller (SWC)** with window $n$:
  
  $$
  Z_k^c = (Y_{k-n:k-1},\; A_{k-n:k-1}) \in \mathcal Y^n \times \mathcal A^n,
  $$
  
  which summarizes the last $n$ observations/actions, where $n$ is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.

The actor uses a **linear-softmax FSC**:
$$
\pi_\theta(a \mid y,z) = 
\frac{\exp\{\theta^\top \psi(a,y,z)\}}
{\sum_{a'} \exp\{\theta^\top \psi(a',y,z)\}},
$$
with features $\psi(a,y,z)\in\mathbb R^d$. The controller memory updates via some $\varphi$:
$$
Z_{k+1}^c=\varphi(Z_k^c, Y_{k+1}, A_k).
$$

## Hidden Markov Reward Processes (HMRP)
A common simplification in TD learning analyses in an MDP setting is to consider a Markov reward process (MRP). Since the augmented state $X_k:=(S_k,A_k)$ under a given stationary policy forms a Markov chain, the analyses automatically extend to policy evaluation (known as SARSA) since $(X_k,r(X_k))$ forms an MRP. 

In POMDPs, we can make a similar simplification. First, let us consider a Hidden Markov Process with a Markov chain $S_k$ and its partial observation process $Y_k\sim\Phi(\cdot\mid S_k)$. Along with the reward $r(S_k)$, this HMP induces a Hidden Markov Reward Process (HMRP) $\{(S_k,r(S_k),Y_k):k\in\mathbb{N}\}$.

Consider a finite-state controller with an internal state $Z_k^c$. Then, we have $A_k \sim \pi_k(\cdot\mid Z_k^c)$, thus $X_k:=(S_k,A_k,Z_k^c)$ forms a Markov process with the transition kernel $$P(S_{k+1}=s',A_{k+1}=a',Z_{k+1}^c=z'|S_k,A_k,Z_k^c) = \sum_{y'\in\mathcal Y}P(s'|S_k,A_k)\Phi(y'|s')\varphi(z'|Z_k^c,y',A_k)\pi_{k+1}(a'\mid y',z').$$ The noisy observation is $Y_k~\Phi(\cdot|S_k)$. Defining $\tilde{r}(S_k,A_k,Z_k^c) = r(S_k,A_k)$, we obtain the original process under $\pi$. Thus, the controlled process under $\pi$ induces the HMRP $(X_k, \tilde{r}(X_k), Y_k)$.

## Critic: $m$-step TD Learning with an Internal State

It is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology [@singh1994learning]. For a concrete demonstration, let us focus on learning the value function in an HMRP: $$V(z_0):=\mathbb E[\sum_{k=0}^\infty \gamma^k r(S_k)|Z_0=z_0]\mbox{ and }V_s^\star:=\mathbb E[\sum_{k=0}^\infty r(S_k)|S_0=s]$$ for an internal state $Z_k$ (e.g., $Z_t = (Y_{t-n+1},\ldots,Y_t)$), where $(S_k, r(S_k), Y_k)$ is an HMRP.

::: callout-tip
**Two objectives.** In the above display, if we want to perform policy evaluation for an FSC with $\{Z_k^c:k\in\mathbb N\}$, we would be interested in $V(z_0)$. Additionally, we can use $m$-step TD Learning for estimating the value function $V_s^\star$. We discuss both problems in what follows.
:::

**Example 1 (When TD(0) fails to learn $V_s^\star$).** Here we consider an HMRP with $\mathcal S = \{0,1\}$, $r(s) = s$ and $\Phi(1|s) = 1$ for all $s\in\mathcal{S}$. Let $P(0|1) = q$ and $P(1|0) = p$ for some $p, q \in (0,1)$, and $\gamma \in (0, 1)$ be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to $v^\star = \frac{1}{1-\gamma}\cdot \frac{p}{q+p} = V_0^\star + \frac{p}{p+q}(V_1^\star-V_0^\star)$. In the proof of Theorem 4.1 in [@cayci2024nac], we characterize the issues related to the fixed point of single-step TD(0) for general POMDPs.

On the other hand, the value function for a given $z_0$ is $$V(z_0) = V_0^\star + (V_1^\star-V_0^\star)P(S_0=1|Z_0=z_0) = \frac{\gamma p + (1-\gamma)P(S_0=1|Z_0=z_0)}{(1-\gamma)\Big[\gamma(p+q)+1-\gamma\Big]},$$ since $V(z_0) = V_0^\star+(V_1^\star-V_0^\star)P(S_0=1|Z_0=z_0)$. Also, $V_{S_0}^\star = V_0^\star + (V_1^\star-V_0^\star)\mathbf{1}_{\{S_0=1\}}$. Therefore, $$\mathbb{E}[(V_{S_0}^\star-v^\star)^2] = (V_1^\star-V_0^\star)^2\mathbb{E}[(\frac{p}{p+q}-\mathbf{1}_{\{S_0=1\}})^2]=Var(\mathbf{1}_{\{S_0=1\}})(V_1^\star-V_0^\star)^2.$$

**Example 2 (When TD(0) fails to learn $V(z_0)$).** Consider the two-state HMRP with $\mathcal S = \{0,1\}$, $r(s) = s$ and $\Phi(1|s) = c_s\in(0,1)$ now. Let $P(0|1) = P(1|0) = 1/2$, and $\gamma = 0.9$. Consider an internal state $Z_k = (Y_{k-1}, Y_k)$. We want to compute $V(z_0) = \mathbb{E}[\sum_k \gamma^k r(S_k)|Z_0=z_0]$. For this purpose, one can use tabular-TD(0) with the surrogate state $Z_k=(Y_{k-1},Y_k)$. In this case, the limit point of TD(0) would be the unique fixed point of $$v_1(z_0) = P(S_0=1|Z_0=z_0) + \gamma \sum_{z_1\in\{0,1\}^2}v(z_1)P(Z_1=z_1|Z_0=z_0).$$ Substituting $c_0 = 1/4, c_1 = 3/4$, we obtain the fixed point 
$$
v_1(z_0) =
\begin{cases}
4.1396, & z_0 = (0,0), \\
5.2739, & z_0 = (0,1), \\
4.7261, & z_0 = (1,0), \\
5.8604, & z_0 = (1,1).
\end{cases}
$$
On the other hand, the true value function $V(z_0)$ is as follows:
$$
V(z_0) =
\begin{cases}
1.6306, & z_0 = (0,0), \\
5.0561, & z_0 = (0,1), \\
4.9439, & z_0 = (1,0), \\
8.3694, & z_0 = (1,1).
\end{cases}
$$
This indicates that a naive application of TD(0) with the internal state $\{Z_k:k\in\mathbb N\}$ does not return the true value function $V(z_0)$. Thus, in case we want to evaluate the performance of a policy with an internal state $Z_k^c = (Y_{k-1}, A_{k-1})$, TD(0) is not useful. 

::: callout-tip
**Why $m$-step TD?** Examples 1 and 2 show that, in POMDPs, one-step bootstrapping suffers from **perceptual aliasing** (same observation $Y_k$, different latent state $S_k$). We will see in the following that looking $m$ steps ahead stabilizes the target and reduces this bias.
:::

Now, let us focus on $m$-step TD learning with an internal state $Z_k$ for the above problem, where $m\in\mathbb{Z}_+$. Note that the internal state may match the controller state $Z_k^c$ (in the case of control, see Section 3), or it can be used to obtain an estimate of $V_s^\star$. 

### Learning $V(z_0)$ by $m$-step TD(0) with an Internal State

Let $$(T_mv)(z) = \bar{r}_m(z) + \gamma^m(K_mv)(z),$$ where $\bar{r}_m(z) := \mathbb E[\sum_{k=0}^{m-1}r(S_k)|Z_0=z]$ and $(K_mv)(z):=\mathbb{E}[v(Z_m)|Z_0=z]$. Then, it is straightforward to show that $T_m$ is a contractive operator with modulus $\gamma^m$, and admits a unique fixed point $$V(z) = \mathbb E[\sum_{k=0}^\infty \gamma^k r(S_k)|Z_0=z] = \sum_{k=0}^\infty (\gamma^mK_m)^k\bar{r}_m(z)$$ for any $m\in\mathbb N$. Let $\xi$ be the stationary distribution of $\{Z_k:k\in\mathbb N\}$, which exists given $S_k$ is an ergodic unichain. Under a non-degenerate $\mathbb{E}_{Z_t\sim \xi}[\psi(Z_t)\otimes\psi(Z_t)]\succ 0$, $m$-step TD learning with the internal state $Z_t$ converges to the unique fixed point $v_m$ of the equation $$v = \Pi\{\bar{r}_m+\gamma^m K_m v\},$$ where $\Pi$ be the ortogonal projection onto the subspace $\mathcal V := \{z\mapsto \langle \psi(z),w\rangle: w \in \mathbb R^d$ with respect to $L^2(\xi)$. 
Thus, we have the following result.

::: callout-important
For any $m\in\mathbb{Z}_+$, $m$-step TD(0) with an internal state $\{Z_k:k\in\mathbb N\}$ converges to $v_m$, which satisfies
$$\|v_m - \Pi V\|_{L^2(\xi)} \leq \frac{\gamma^m}{1+\gamma^m}\|V-\Pi V\|_{L^2(\xi)}.$$ 
:::

As such, in case we are interested in policy evaluation for an FSC, this formulation implies the near optimality of $m$-step TD learning, which uses the internal state of the controller as the critic internal state, with an additional error term $\mathcal{O}(\gamma^m)$.

### Learning $V_s^\star$ by $m$-step TD(0) with a Sliding-Window Internal State

The analysis of $m$-step TD(0) with an internal state $Z_k=(Y_{k-n},\ldots,Y_k)$ in the setting of Example 1 can be taken one step further indeed. The argument below can be shifted in time, so let us consider $k=0$ for notational simplicity. Let $\mathcal{F}_n :=\sigma(Z_0)=\sigma(Y_{-n+1},\ldots,Y_0)$ be the $\sigma$-algebra generated by the internal state, and $b^{(n)} := P(S_0 = 1|\mathcal{F}_n)$. Then, $V_n(z_0) =  \mathbb E[\sum_{k=0}^\infty\gamma^kr(S_k)|\mathcal{F}_n]$. Since $V_s^\star = V_0^\star + (V_1^\star-V_0^\star)\mathbf{1}_{\{S_0=s\}}$ and $V_n(z_0) = V_0^\star + (V_1^\star-V_0^\star)b^{(n)}$, we obtain $$\mathbb{E}[(V_{S_0}^\star-V_n(Z_0))^2] = (V_1^\star-V_0^\star)^2\,\mathbb E\!\left[(\mathbf{1}_{\{S_0=1\}}-b^{(n)})^2\right].$$ Furthermore, since $b^{(n)}=\mathbb E[\mathbf{1}_{\{S_0=1\}}|\mathcal F_n]$, we have 

\begin{align}
\mathbb E\!\left[(\mathbf{1}_{\{S_0=1\}}-b^{(n)})^2\right]&=\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_n)]\\
&\downarrow \mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_\infty)]
\end{align}
as $n\rightarrow\infty$ due to the law of total variance for nested $\sigma$-algebras $\mathcal{F}_n$. Therefore, $$\mathbb{E}[(V_{S_0}^\star-V_n(Z_0))^2] \downarrow (V_1^\star-V_0^\star)^2\,\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_\infty)].$$ If $S_0$ is measurable with respect to $\sigma(Y_{-\infty},\ldots,Y_0)$, we have $V_{s_0}^\star=V_\infty(z_0))$ almost surely. Used in conjunction with the previous result on $v_m-V_n(z_0)$, a simple triangle inequality would demonstrate the effectiveness of TD learning with sliding-window controllers.

::: callout-important
In summary, for Example 1, we have
\begin{align*}
  \mathbb{E}[(V_{S_0}^\star - v^\star)^2] &= (V_1^\star-V_0^\star)^2 Var(\mathbf{1}_{\{S_0=1\}})\\
&\geq (V_1^\star-V_0^\star)^2\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_n)] \downarrow (V_1^\star-V_0^\star)^2\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_\infty)].
\end{align*}
:::

## What’s next: NAC with Finite-State Controllers for POMDPs

In the next step, we will show how FSC's yield improved performance (even global near-optimality) for NAC under certain ergodicity conditions. The discussion will be based on stochastic filtering, and establish a trade-off between memory complexity and optimality gap, governed by the size of the internal state space.

---
