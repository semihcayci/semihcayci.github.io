---
title: "Reinforcement Learning for POMDPs – Part I: TD Learning with Internal States"
date: 2025-10-12
description: "How does multi-step TD learning with internal states mitigate partial observability?"
categories: [reinforcement-learning, POMDP, temporal difference learning]
toc: true
draft: false
---

> **TL;DR.** A classic result by [@singh1994learning] shows that memoryless TD learning fails in POMDPs. To address this, we studied a natural actor–critic (NAC) method for POMDPs in [@cayci2024nac], with a critic that uses $m$-step TD learning with an internal state to mitigate perceptual aliasing problem. Below, I review this algorithm and provide some follow-up results.

## Formal POMDP model

Let us consider a discounted POMDP
$$
\mathcal{M}=\big(\mathcal S,\mathcal A,\mathcal Y,\; P,\; O,\; r,\; \gamma\big),
$$

with:

- state $S_k\in\mathcal S$,
- action $A_k\in\mathcal A$,
- observation $Y_k\in\mathcal Y$,
- transition kernel $P(s' \mid s,a)$,
- observation kernel $\Phi(y \mid s')$,
- reward $r(s,a)\in[0,1]$ (w.l.o.g.),
- discount factor $\gamma\in(0,1)$.

To avoid measure-theoretic concerns, let us consider finite $\mathcal S, \mathcal Y, \mathcal A$. These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is
$$
S_{k+1}\sim P(\cdot\mid S_k,A_k),\qquad
Y_{k}\sim \Phi(\cdot\mid S_k),\qquad
R_k=r(S_k,A_k),
$$
and the goal is to maximize
$$
\mathcal V^\pi(\xi)=\mathbb E_\xi^\pi\!\left[\sum_{k\ge0}\gamma^k\,r(S_k,A_k)\right].
$$

**Why challenging?**  
Partial observability implies that the optimal policy $\pi^\star=(\pi_0^\star,\pi_1^\star,\ldots)$ is non-stationary, therefore $\pi_k^\star$ depends on the complete trajectory $(Y_{0:k},A_{0:k-1})$ for each $k\in\mathbb N$ via the Bayes belief $b_k(s)=\Pr(S_k=s\mid Y_{0:k},A_{0:k-1})$. Reactive (i.e., memoryless) methods for MDPs fail miserably if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see **Example 1** below). A classical result attributed to [@astrom1965] (see [@yuksel2025another] for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state $b_k$. However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over $\mathcal S$), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies [@murphy2000survey] and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state $Z_k$, which summarizes/compresses $(Y_{0:k-1},A_{0:k-1})$ for each $k$. The idea is to use a parametric policy $\pi_k(\cdot\mid Y_k, Z_k)$ based on the internal state $Z_k$.

## Setting and Goal

At first, we restrict policies to **finite-state controllers (FSCs)** that keep internal memory $Z_k$ and act from $(Y_k, Z_k)$. The internal memory is updated in a Markovian way: $Z_{k+1}~\varphi(\cdot\mid z_k, y_k, a_k)$ given $Z_k=z_k,Y_k=y_k,A_k=a_k$.

- Learn the best policy within a fixed FSC class $\pi_{Z,\varphi}$:
  
  $$
  \pi^\star \in \arg\max_{\pi \in \pi_{Z,\varphi}} \; \mathcal V^\pi(\xi).
  $$

- A useful subclass is the **sliding-window controller (SWC)** with window $n$:
  
  $$
  Z_k = (Y_{k-n:k-1},\; A_{k-n:k-1}) \in \mathcal Y^n \times \mathcal A^n,
  $$
  
  which summarizes the last $n$ observations/actions, where $n$ is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.

The actor uses a **linear-softmax FSC**:
$$
\pi_\theta(a \mid y,z) = 
\frac{\exp\{\theta^\top \psi(a,y,z)\}}
{\sum_{a'} \exp\{\theta^\top \psi(a',y,z)\}},
$$
with features $\psi(a,y,z)\in\mathbb R^d$. The controller memory updates via some $\varphi$:
$$
Z_{k+1}=\varphi(Z_k, Y_{k+1}, A_k).
$$

## Critic: $m$-step TD Learning with an Internal State

It is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology [@singh1994learning]. For a concrete demonstration, let us focus on learning the value function in a Hidden Markov Reward Process (HMRP): $$V(z_0):=\mathbb E[\sum_{k=0}^\infty \gamma^k r(S_k)|Z_0=z_0]\mbox{ and }V_s^\star:=\mathbb E[\sum_{k=0}^\infty r(S_k)|S_0=s]$$ for an internal state $Z_t$ (e.g., $Z_t = (Y_{t-m+1},\ldots,Y_t)$), where $(S_k, r(S_k), Y_k)$ is a HMRP.

**Example 1** Consider an HMRP with $\mathcal S = \{0,1\}$, $r(s) = s$ and $\Phi(1|s) = 1$ for all $s\in\mathcal{S}$. Let $P(0|1) = p$ and $P(1|0) = q$ for some $p, q \in (0,1)$, and $\gamma \in (0, 1)$ be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to $v^\star = \frac{1}{1-\gamma}\cdot \frac{p}{q+p}$. On the other hand, the value function for a given internal state $Z_0=z_0$ is $$V(z_0) = \frac{\gamma p + (1-\gamma)P(S_0=1|Z_0=z_0)}{(1-\gamma)\Big[\gamma(p+q)+1-\gamma\Big]}\neq v^\star,$$ since $V(z_0) = V_0^\star+(V_1^\star-V_0^\star)P(S_0=1|Z_0=z_0)$.

::: callout-tip
**Why $m$-step TD?** In POMDPs, one-step bootstrapping suffers from **perceptual aliasing** (same observation $Y_k$, different latent state $S_k$). Looking $m$ steps ahead stabilizes the target and reduces this bias.
:::

To see how $m$-step TD learning with internal state $Z_t = (Y_{t-m+1},\ldots,Y_t)$ mitigates perceptual aliasing, let us focus on the limit point. To that end, let $$(T_mv)(z) = \bar{r}_m(z) + \gamma^m(K_mv)(z),$$ where $\bar{r}_m(z) := \mathbb E[\sum_{k=0}^{m-1}r(S_k)|Z_0=z]$ and $(K_mv)(z):=\mathbb{E}[v(Z_m)|Z_0=z]$. Then, it is straightforward to show that $T_m$ is a contractive operator with modulus $\gamma^m$, and admits a unique fixed point $$V(z) = \mathbb E[\sum_{k=0}^\infty r(S_k)|Z_0=z] = \sum_{k=0}^\infty (\gamma^mK_m)^k\bar{r}_m(z)$$ for any $m\in\mathbb N$. Let $\xi$ be the invariant distribution of $\{Z_k:k\in\mathbb N\}$: $$\xi(z_t)=\sum_{s_{t-m+1:t}}\prod_{k=0}^{m-1}\Phi(y_{t-k}|s_{t-k})\mu(s_{t-k}),$$ where $\mu$ is the stationary distribution of $\{S_k:k\in\mathbb N\}$. Under a non-degenerate $\mathbb{E}_{Z_t\sim \xi}[\psi(Z_t)\otimes\psi(Z_t)]\succ 0$, $m$-step TD learning with the internal state $Z_t$ converges to the unique fixed point $v_m$ of the equation $$v = \Pi\{\bar{r}_m+\gamma^m K_m v\},$$ where $\Pi$ be the ortogonal projection onto the subspace $\mathcal V := \{z\mapsto \langle \psi(z),w\rangle: w \in \mathbb R^d$ with respect to $L^2(\xi)$. Thus, one obtains $$\|v_m - \Pi V\|_{L^2(\xi)} \leq \frac{\gamma^m}{1+\gamma^m}\|V-\Pi V\|_{L^2(\xi)}.$$ As such, it is possible to learn $V(z_0)$ (actually, its projection onto $\mathcal V$ due to linear function approximation) via $m$-step TD learning with an additional error term $\mathcal{O}(\gamma^m)$.

The analysis can be taken one step further indeed. The argument below can be shifted in time, so let us consider $t=0$ for notational simplicity. Let $\mathcal{F}_m :=\sigma(Z_0)=\sigma(Y_{-m+1},\ldots,Y_0)$ be the $\sigma$-algebra generated by the internal state, and $b^{(m)} := P(S_0 = 1|\mathcal{F}_m)$. Then, $V(z_0) =  \mathbb E[\sum_{t=0}^\infty\gamma^tr(S_t)|\mathcal{F}_m]$. Since $V_s^\star = V_0^\star + (V_1^\star-V_0^\star)\mathbf{1}_{\{S_0=s\}}$ and $V(z_0) = V_0^\star + (V_1^\star-V_0^\star)b^{(m)}$, we obtain $$\mathbb{E}[(V_{S_0}^\star-V(z_0))^2] = (V_1^\star-V_0^\star)^2\,\mathbb E\!\left[(\mathbf{1}_{\{S_0=1\}}-b^{(m)})^2\right].$$ Furthermore, since $b^{(m)}=\mathbb E[\mathbf{1}_{\{S_0=1\}}|\mathcal F_m]$, we have 

\begin{align}
E\!\left[(\mathbf{1}_{\{S_0=1\}}-b^{(m)})^2\right]&=\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_m)]\\
&\downarrow \mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_\infty)]
\end{align}
as $m\rightarrow\infty$ due to the law of total variance for nested $\sigma$-algebras $\mathcal{F}_m$. Therefore, $$\mathbb{E}[(V_{s_0}^\star-V(z_0))^2] \downarrow (V_1^\star-V_0^\star)^2\,\mathbb E[Var(\mathbf{1}_{\{S_0=1\}}|\mathcal F_\infty)].$$ If $S_0$ is measurable with respect to $\sigma(Y_{-\infty},\ldots,Y_0)$, we have $V_{s_0}^\star=V(z_0))$ almost surely. Used in conjunction with the previous result on $v_m-V(z_0)$, a simple triangle inequality would demonstrate the effectiveness of TD learning with sliding-window controllers.

With stepsize $\alpha=K^{-1/2}$ and parameter radius $R$, after $K$ critic updates:
$$
\sqrt{\mathbb E\!\left[\big\|\mathcal Q^\pi-\widehat{\mathcal Q}^\pi_K\big\|^2_{\,d^\pi_\xi\otimes\pi}\right]}
\;\lesssim\;
\underbrace{\tfrac{K^{-1/4}}{1-\gamma}}_{\text{statistical}}
+\underbrace{\tfrac{\varepsilon_{\text{app}}(R)}{1-\gamma^m}}_{\text{approximation}}
+\underbrace{\varepsilon_{\text{pa}}(\gamma,m,R)}_{\text{aliasing}},
$$
where $\varepsilon_{\text{app}}(R)$ is the best linear-approximation error within radius $R$, and the aliasing term contracts geometrically:
$$
\varepsilon_{\text{pa}}(\gamma,m,R)=\mathcal O\!\big(\gamma^{m/2}\,\mathrm{poly}(R,(1-\gamma)^{-1})\big).
$$

**Sample–accuracy trade-off.** Each update uses $m$ samples, so total evaluation samples are $\Theta(mK)$.

## Full Actor–Critic Performance (Finite-Time)

After $T$ outer iterations (with tuned stepsizes),
$$
(1-\gamma)\,\min_{t<T}\mathbb E\!\big[\mathcal V^{\pi^\star}(\xi)-\mathcal V^{\pi_t}(\xi)\big]
\;\lesssim\;
\underbrace{T^{-1/2}}_{\text{actor optimization}}
+\underbrace{\varepsilon_{\text{critic}}(K,m,R)}_{\text{TD evaluation}}
+\underbrace{\varepsilon_{\text{actor}}(N,R)}_{\text{compatible approx.}}
+\underbrace{\varepsilon_{\text{inf}}(\xi)}_{\text{inference penalty}}.
$$

The **inference error** is the price of partial observability:
$$
\varepsilon_{\text{inf}}(\xi)
= \mathbb E\!\left[\sum_{k\ge0}\gamma^k
\left\|\,b_k(\cdot)-b_0(\cdot,I_k)\,\right\|_{\mathrm{TV}}
\right],\qquad I_k=(Y_k,Z_k).
$$

## Sliding-Window Controllers (Memory vs. Accuracy)

Under stochastic exploration and filter stability/minorization, the inference error **decays geometrically** with window length $n$:
$$
\varepsilon_{\text{inf}}(\xi)\;\le\;\frac{1}{1-\gamma}\cdot
\mathcal O\!\Big(\rho^{\lfloor n/m_0\rfloor}\Big),
$$
for constants $\rho\in(0,1)$ and $m_0\ge1$. To reach tolerance $\epsilon$:
$$
n=\mathcal O\!\big(m_0\log(1/\epsilon)\big).
$$

## Practical Tuning

- **TD horizon $m$:** choose $m=\Theta(\log_{1/\gamma}(1/\epsilon))$ to make aliasing $\lesssim\epsilon$.
- **Critic steps $K$ and actor inner steps $N$:** about $\Theta(\epsilon^{-4})$.
- **Features:** richer features shrink the compatible approximation term.
- **Window $n$:** grows like $\mathcal O(\log(1/\epsilon))$ under filter stability.

## What’s next: RNN-based NAC for POMDPs

A natural extension replaces the hand-engineered memory $Z_k$ with a **recurrent hidden state** $H_k$ learned end-to-end (RNN-based NAC). The policy becomes $\pi_\theta(a\mid Y_k, H_k)$ with $H_{k+1}=f_\theta(H_k,Y_{k+1},A_k)$, and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs. representation power, and how partial-observability error shows up) in a follow-up post about our **RNN-based natural actor–critic** paper.

---
