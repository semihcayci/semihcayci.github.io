<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Notes on Theoretical RL</title>
<link>https://semihcayci.github.io/</link>
<atom:link href="https://semihcayci.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Sun, 12 Oct 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>RL for POMDPs – Part I: TD Learning with Internal States</title>
  <link>https://semihcayci.github.io/posts/td-learning-for-pomdps.html</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p><strong>TL;DR.</strong> A classic result by <span class="citation" data-cites="singh1994learning">(Singh, Jaakkola, and Jordan 1994)</span> shows that memoryless TD learning fails in POMDPs. One fix is to use multi-step TD learning with an internal (memory) state to mitigate perceptual aliasing. This idea is studied in this post in a simple Hidden Markov Model setting with examples.</p>
</blockquote>
<p><img src="https://semihcayci.github.io/posts/POMDP.png" class="banner-img img-fluid" style="width:50.0%" data-column="page" alt="Partial observability scene"></p>
<section id="formal-pomdp-model" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="formal-pomdp-model"><span class="header-section-number">1</span> Formal POMDP model</h2>
<p>Let us consider a discounted POMDP <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BM%7D=%5Cbig(%5Cmathcal%20S,%5Cmathcal%20A,%5Cmathcal%20Y,%5C;%20P,%5C;%20O,%5C;%20r,%5C;%20%5Cgamma%5Cbig),%0A"></p>
<p>with:</p>
<ul>
<li>state <img src="https://latex.codecogs.com/png.latex?S_k%5Cin%5Cmathcal%20S">,</li>
<li>action <img src="https://latex.codecogs.com/png.latex?A_k%5Cin%5Cmathcal%20A">,</li>
<li>observation <img src="https://latex.codecogs.com/png.latex?Y_k%5Cin%5Cmathcal%20Y">,</li>
<li>transition kernel <img src="https://latex.codecogs.com/png.latex?P(s'%20%5Cmid%20s,a)">,</li>
<li>observation kernel <img src="https://latex.codecogs.com/png.latex?%5CPhi(y%20%5Cmid%20s')">,</li>
<li>reward <img src="https://latex.codecogs.com/png.latex?r(s,a)%5Cin%5B0,1%5D"> (w.l.o.g.),</li>
<li>discount factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Cin(0,1)">.</li>
</ul>
<p>To avoid measure-theoretic concerns, let us consider finite <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S,%20%5Cmathcal%20Y,%20%5Cmathcal%20A">. These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is <img src="https://latex.codecogs.com/png.latex?%0AS_%7Bk+1%7D%5Csim%20P(%5Ccdot%5Cmid%20S_k,A_k),%5Cqquad%0AY_%7Bk%7D%5Csim%20%5CPhi(%5Ccdot%5Cmid%20S_k),%5Cqquad%0AR_k=r(S_k,A_k),%0A"> and the goal is to maximize <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%20V%5E%5Cpi(%5Clambda)=%5Cmathbb%20E_%5Clambda%5E%5Cpi%5C!%5Cleft%5B%5Csum_%7Bk%5Cge0%7D%5Cgamma%5Ek%5C,r(S_k,A_k)%5Cright%5D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Clambda%5Cin%5Cmathcal%7BP%7D(%5Cmathcal%7BY%7D)"> is the distribution of <img src="https://latex.codecogs.com/png.latex?Y_0">.</p>
<p><strong>Why challenging?</strong><br>
Partial observability implies that the optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E%5Cstar=(%5Cpi_0%5E%5Cstar,%5Cpi_1%5E%5Cstar,%5Cldots)"> is non-stationary, therefore <img src="https://latex.codecogs.com/png.latex?%5Cpi_k%5E%5Cstar"> depends on the complete trajectory <img src="https://latex.codecogs.com/png.latex?(Y_%7B0:k%7D,A_%7B0:k-1%7D)"> for each <img src="https://latex.codecogs.com/png.latex?k%5Cin%5Cmathbb%20N"> via the Bayes belief <img src="https://latex.codecogs.com/png.latex?b_k(s)=%5CPr(S_k=s%5Cmid%20Y_%7B0:k%7D,A_%7B0:k-1%7D)">. As such, some very intuitive ideas for partially observable reinforcement learning (PORL) fails:</p>
<ul>
<li><p>Tabular parameterization has a memory complexity of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%7C%5Cmathcal%7BY%7D%7C%5E%7Bk+1%7D%7C%5Cmathcal%7BA%7D%7C%5Ek)"> at time <img src="https://latex.codecogs.com/png.latex?k">, growing exponentially. Even for small action-observation spaces, this would be an intractable solution.</p></li>
<li><p>Reactive (i.e., memoryless) methods for MDPs fail if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see examples below).</p></li>
<li><p>A classical result attributed to <span class="citation" data-cites="astrom1965">(Åström 1965)</span> (see <span class="citation" data-cites="yuksel2025another">(Yüksel 2025)</span> for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state <img src="https://latex.codecogs.com/png.latex?b_k">. However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S">), the conventional RL tools is highly impractical to solve this belief-MDP formulation.</p></li>
</ul>
<p>An alternative and more elegant idea is to incorporate memory into the policies <span class="citation" data-cites="murphy2000survey">(Murphy 2000)</span> and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec">, which summarizes/compresses <img src="https://latex.codecogs.com/png.latex?(Y_%7B0:k-1%7D,A_%7B0:k-1%7D)"> for each <img src="https://latex.codecogs.com/png.latex?k">. In policy-based PORL, the idea is to use a parametric policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta(%5Ccdot%5Cmid%20Y_k,%20Z_k%5Ec)"> based on the internal state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec">.</p>
</section>
<section id="setting-and-goal" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="setting-and-goal"><span class="header-section-number">2</span> Setting and Goal</h2>
<p>At first, we restrict policies to <strong>finite-state controllers (FSCs)</strong> that keep internal memory <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec"> and act from <img src="https://latex.codecogs.com/png.latex?(Y_k,%20Z_k%5Ec)">. The internal memory is updated in a Markovian way: <img src="https://latex.codecogs.com/png.latex?Z_%7Bk+1%7D%5Ec%5Csim%5Cvarphi(%5Ccdot%5Cmid%20z_k,%20y_k,%20a_k)"> given <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec=z_k,Y_k=y_k,A_k=a_k">.</p>
<ul>
<li><p>Learn the best policy within a fixed FSC class <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7BZ,%5Cvarphi%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%5E%5Cstar%20%5Cin%20%5Carg%5Cmax_%7B%5Cpi%20%5Cin%20%5Cpi_%7BZ,%5Cvarphi%7D%7D%20%5C;%20%5Cmathcal%20V%5E%5Cpi(%5Cxi).%0A"></p></li>
<li><p>A useful subclass is the <strong>sliding-window controller (SWC)</strong> with window <img src="https://latex.codecogs.com/png.latex?n">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ_k%5Ec%20=%20(Y_%7Bk-n:k-1%7D,%5C;%20A_%7Bk-n:k-1%7D)%20%5Cin%20%5Cmathcal%20Y%5En%20%5Ctimes%20%5Cmathcal%20A%5En,%0A"></p>
<p>which summarizes the last <img src="https://latex.codecogs.com/png.latex?n"> observations/actions, where <img src="https://latex.codecogs.com/png.latex?n"> is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.</p></li>
</ul>
<p>The actor uses a <strong>linear-softmax FSC</strong>: <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi_%5Ctheta(a%20%5Cmid%20y,z)%20=%0A%5Cfrac%7B%5Cexp%5C%7B%5Ctheta%5E%5Ctop%20%5Cpsi(a,y,z)%5C%7D%7D%0A%7B%5Csum_%7Ba'%7D%20%5Cexp%5C%7B%5Ctheta%5E%5Ctop%20%5Cpsi(a',y,z)%5C%7D%7D,%0A"> with features <img src="https://latex.codecogs.com/png.latex?%5Cpsi(a,y,z)%5Cin%5Cmathbb%20R%5Ed">. The controller memory updates via a transition kernel <img src="https://latex.codecogs.com/png.latex?%5Cvarphi">: <img src="https://latex.codecogs.com/png.latex?%0AZ_%7Bk+1%7D%5Ec%5Csim%5Cvarphi(%5Ccdot%5Cmid%20Z_k%5Ec,%20Y_%7Bk%7D,%20A_k).%0A"></p>
<p>In order to compute a low-variance estimate of the (natural) policy gradient for this linear-softmax FSC within an actor-critic framework, we need to evaluate <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BQ%7D%5E%7B%5Cpi_%5Ctheta%7D(A_k,Y_k,Z_k%5Ec)"> under this internal state representation. This blog post is about this part, the critic.</p>
</section>
<section id="hidden-markov-reward-processes-hmrp" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="hidden-markov-reward-processes-hmrp"><span class="header-section-number">3</span> Hidden Markov Reward Processes (HMRP)</h2>
<p>A common simplification in TD learning analyses in an MDP setting is to consider a Markov reward process (MRP). Since the augmented state <img src="https://latex.codecogs.com/png.latex?X_k:=(S_k,A_k)"> under a given stationary policy forms a Markov chain, the analyses automatically extend to policy evaluation (known as SARSA) since <img src="https://latex.codecogs.com/png.latex?(X_k,r(X_k))"> forms an MRP.</p>
<p>In POMDPs, we can make a similar simplification. First, let us consider a Hidden Markov Process with a Markov chain <img src="https://latex.codecogs.com/png.latex?S_k"> and its partial observation process <img src="https://latex.codecogs.com/png.latex?Y_k%5Csim%5CPhi(%5Ccdot%5Cmid%20S_k)">. Along with the reward <img src="https://latex.codecogs.com/png.latex?r(S_k)">, this HMP induces a Hidden Markov Reward Process (HMRP) <img src="https://latex.codecogs.com/png.latex?%5C%7B(S_k,r(S_k),Y_k):k%5Cin%5Cmathbb%7BN%7D%5C%7D">.</p>
<p>Consider a finite-state controller with an internal state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec">. Then, we have <img src="https://latex.codecogs.com/png.latex?A_k%20%5Csim%20%5Cpi_k(%5Ccdot%5Cmid%20Z_k%5Ec)">, thus <img src="https://latex.codecogs.com/png.latex?X_k:=(S_k,A_k,Z_k%5Ec)"> forms a Markov process with the transition kernel <img src="https://latex.codecogs.com/png.latex?P(S_%7Bk+1%7D=s',A_%7Bk+1%7D=a',Z_%7Bk+1%7D%5Ec=z'%7CS_k,A_k,Z_k%5Ec)%20=%20%5Csum_%7By'%5Cin%5Cmathcal%20Y%7DP(s'%7CS_k,A_k)%5CPhi(y'%7Cs')%5Cvarphi(z'%7CZ_k%5Ec,y',A_k)%5Cpi_%7Bk+1%7D(a'%5Cmid%20y',z')."> The noisy observation is <img src="https://latex.codecogs.com/png.latex?Y_k~%5CPhi(%5Ccdot%7CS_k)">. Defining <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Br%7D(S_k,A_k,Z_k%5Ec)%20=%20r(S_k,A_k)">, we obtain the original process under <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Thus, the controlled process under <img src="https://latex.codecogs.com/png.latex?%5Cpi"> induces the HMRP <img src="https://latex.codecogs.com/png.latex?(X_k,%20%5Ctilde%7Br%7D(X_k),%20Y_k)">.</p>
</section>
<section id="critic-m-step-td-learning-with-an-internal-state" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="critic-m-step-td-learning-with-an-internal-state"><span class="header-section-number">4</span> Critic: <img src="https://latex.codecogs.com/png.latex?m">-step TD Learning with an Internal State</h2>
<p>It is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology <span class="citation" data-cites="singh1994learning">(Singh, Jaakkola, and Jordan 1994)</span>. For a concrete demonstration, let us focus on learning the value function in an HMRP: <img src="https://latex.codecogs.com/png.latex?V(z_0):=%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ek%20r(S_k)%7CZ_0=z_0%5D%5Cmbox%7B%20and%20%7DV_s%5E%5Cstar:=%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20r(S_k)%7CS_0=s%5D"> for an internal state <img src="https://latex.codecogs.com/png.latex?Z_k"> (e.g., <img src="https://latex.codecogs.com/png.latex?Z_t%20=%20(Y_%7Bt-n+1%7D,%5Cldots,Y_t)">), where <img src="https://latex.codecogs.com/png.latex?(S_k,%20r(S_k),%20Y_k)"> is an HMRP.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Two objectives.</strong> In the above display, if we want to perform policy evaluation for an FSC with <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k%5Ec:k%5Cin%5Cmathbb%20N%5C%7D">, we would be interested in <img src="https://latex.codecogs.com/png.latex?V(z_0)">. Additionally, we can use <img src="https://latex.codecogs.com/png.latex?m">-step TD Learning for estimating the value function <img src="https://latex.codecogs.com/png.latex?V_s%5E%5Cstar">. We discuss both problems in what follows.</p>
</div>
</div>
<p><strong>Example 1 (When TD(0) fails to learn <img src="https://latex.codecogs.com/png.latex?V_s%5E%5Cstar">).</strong> Consder an HMRP with <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S%20=%20%5Cmathcal%20Y%20=%20%5C%7B0,1%5C%7D">, and <img src="https://latex.codecogs.com/png.latex?r(s)%20=%20s">. Let <img src="https://latex.codecogs.com/png.latex?P(0%7C1)%20=%20q"> and <img src="https://latex.codecogs.com/png.latex?P(1%7C0)%20=%20p"> for some <img src="https://latex.codecogs.com/png.latex?p,%20q%20%5Cin%20(0,1)">, <img src="https://latex.codecogs.com/png.latex?%5CPhi(1%7Cs)=1"> for all <img src="https://latex.codecogs.com/png.latex?s%5Cin%5Cmathcal%20S"> and <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cin%20(0,%201)"> be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to <img src="https://latex.codecogs.com/png.latex?v%5E%5Cstar%20=%20%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%5Ccdot%20%5Cfrac%7Bp%7D%7Bq+p%7D%20=%20V_0%5E%5Cstar%20+%20%5Cfrac%7Bp%7D%7Bp+q%7D(V_1%5E%5Cstar-V_0%5E%5Cstar)."></p>
<p>For the true value function with the initial latent distribution <img src="https://latex.codecogs.com/png.latex?S_0">, we have <img src="https://latex.codecogs.com/png.latex?V_%7BS_0%7D%5E%5Cstar%20=%20V_0%5E%5Cstar%20+%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D">. Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(V_%7BS_0%7D%5E%5Cstar-v%5E%5Cstar)%5E2%5D%20=%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5Cmathbb%7BE%7D%5B(%5Cfrac%7Bp%7D%7Bp+q%7D-%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D)%5E2%5D=Var(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D)(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2,"> where <img src="https://latex.codecogs.com/png.latex?Var(S_0)"> is under the stationary distribution of <img src="https://latex.codecogs.com/png.latex?S_k">. As such, TD(0) suffers from a non-vanishing mean-square error except the degenerate case where the stationary distribution of <img src="https://latex.codecogs.com/png.latex?S_k"> is a Dirac measure.</p>
<p><strong>Example 2 (When TD(0) fails to learn <img src="https://latex.codecogs.com/png.latex?V(z_0)">).</strong> Consider the two-state HMRP with <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S%20=%20%5C%7B0,1%5C%7D">, <img src="https://latex.codecogs.com/png.latex?r(s)%20=%20s"> and <img src="https://latex.codecogs.com/png.latex?%5CPhi(1%7Cs)%20=%20c_s%5Cin(0,1)">. Let <img src="https://latex.codecogs.com/png.latex?P(0%7C1)%20=%20P(1%7C0)%20=%200.01">, and <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%200.99">. Consider an internal state <img src="https://latex.codecogs.com/png.latex?Z_k%20=%20(Y_%7Bk-1%7D,%20Y_k)">. We want to compute <img src="https://latex.codecogs.com/png.latex?V(z_0)%20=%20%5Cmathbb%7BE%7D%5B%5Csum_k%20%5Cgamma%5Ek%20r(S_k)%7CZ_0=z_0%5D">. For this purpose, one can use tabular-TD(0) with the surrogate state <img src="https://latex.codecogs.com/png.latex?Z_k=(Y_%7Bk-1%7D,Y_k)">. In this case, the limit point of TD(0) would be the unique fixed point of <img src="https://latex.codecogs.com/png.latex?(T_1v)(z_0):=P(S_0=1%7CZ_0=z_0)%20+%20%5Cgamma%20%5Csum_%7Bz_1%5Cin%5C%7B0,1%5C%7D%5E2%7Dv(z_1)P(Z_1=z_1%7CZ_0=z_0)."> Now, let’s diagnose why the true value is <strong>not</strong> the fixed point of the Bellman equation. Note that we can write <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AV(z_0)%20&amp;=%20%5Cmathbb%20E%5Br(S_0)%7CZ_0=z_0%5D%20+%20%5Cgamma%20%5Cmathbb%7BE%7D%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ekr(S_%7Bk+1%7D)%7CZ_0=z_0%5D%5C%5C%0A&amp;=%20%5Cmathbb%20E%5Br(S_0)%7CZ_0=z_0%5D%20+%20%5Cgamma%20%5Cmathbb%7BE%7D%5Cleft%5B%5Cmathbb%7BE%7D%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ekr(S_%7Bk+1%7D)%7CZ_1,%20Z_0=z_0%5D%5Cbig%7CZ_0=z_0%5Cright%5D%0A%5Cend%7Balign*%7D"> Note that <img src="https://latex.codecogs.com/png.latex?(Z_0,Z_1)%20=%20(Y_%7B-1%7D,Y_0,Y_1)">. Thus, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ekr(S_%7Bk+1%7D)%7CZ_1,%20Z_0=z_0%5D%20%5Cneq%20%5Cmathbb%7BE%7D%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ekr(S_%7Bk+1%7D)%7CZ_1%5D">, which implies that <img src="https://latex.codecogs.com/png.latex?V(z_0)"> is <strong>not</strong> the fixed point of <img src="https://latex.codecogs.com/png.latex?T_1">. To see how they differ, for <img src="https://latex.codecogs.com/png.latex?c_1=0.51"> and <img src="https://latex.codecogs.com/png.latex?c_0%20=%200.49">, we have <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B%5Csum_%7Bz_0%5Cin%5C%7B0,1%5C%7D%5E2%7D%5Cmu(z_0)%7Cv_1(z_0)-V(z_0)%7C%5E2%7D%5Capprox%200.448863">, where <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is the stationary distribution of <img src="https://latex.codecogs.com/png.latex?Z_k">. This indicates that a naive application of TD(0) with the internal state <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k:k%5Cin%5Cmathbb%20N%5C%7D"> does not return the true value function <img src="https://latex.codecogs.com/png.latex?V(z_0)">. Thus, in case we want to evaluate the performance of a policy with an internal state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec%20=%20(Y_%7Bk-1%7D,%20A_%7Bk-1%7D)">, TD(0) suffers from an estimation error.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Why <img src="https://latex.codecogs.com/png.latex?m">-step TD?</strong> Examples 1 and 2 show that, in POMDPs, one-step bootstrapping suffers from <strong>perceptual aliasing</strong> (same observation <img src="https://latex.codecogs.com/png.latex?Y_k">, different latent state <img src="https://latex.codecogs.com/png.latex?S_k">). We will see in the following that looking <img src="https://latex.codecogs.com/png.latex?m"> steps ahead stabilizes the target and reduces this bias.</p>
</div>
</div>
<p>Now, let us focus on <img src="https://latex.codecogs.com/png.latex?m">-step TD learning with an internal state <img src="https://latex.codecogs.com/png.latex?Z_k"> for the above problem, where <img src="https://latex.codecogs.com/png.latex?m%5Cin%5Cmathbb%7BZ%7D_+">. Note that the internal state may match the controller state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec"> (in the case of control, see Section 3), or it can be used to obtain an estimate of <img src="https://latex.codecogs.com/png.latex?V_s%5E%5Cstar">.</p>
<section id="learning-vz_0-by-m-step-td0-with-an-internal-state" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="learning-vz_0-by-m-step-td0-with-an-internal-state"><span class="header-section-number">4.1</span> Learning <img src="https://latex.codecogs.com/png.latex?V(z_0)"> by <img src="https://latex.codecogs.com/png.latex?m">-step TD(0) with an Internal State</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?(T_mv)(z)%20=%20%5Cbar%7Br%7D_m(z)%20+%20%5Cgamma%5Em(K_mv)(z),"> where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Br%7D_m(z)%20:=%20%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%7Bm-1%7Dr(S_k)%7CZ_0=z%5D"> and <img src="https://latex.codecogs.com/png.latex?(K_mv)(z):=%5Cmathbb%7BE%7D%5Bv(Z_m)%7CZ_0=z%5D">. Then, it is straightforward to show that <img src="https://latex.codecogs.com/png.latex?T_m"> is a contractive operator with modulus <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Em">, and admits a unique fixed point <img src="https://latex.codecogs.com/png.latex?V(z)%20=%20%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ek%20r(S_k)%7CZ_0=z%5D%20=%20%5Csum_%7Bk=0%7D%5E%5Cinfty%20(%5Cgamma%5EmK_m)%5Ek%5Cbar%7Br%7D_m(z)"> for any <img src="https://latex.codecogs.com/png.latex?m%5Cin%5Cmathbb%20N">. Let <img src="https://latex.codecogs.com/png.latex?%5Cxi"> be the stationary distribution of <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k:k%5Cin%5Cmathbb%20N%5C%7D">, which exists given <img src="https://latex.codecogs.com/png.latex?S_k"> is an ergodic unichain. Under a non-degenerate <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7BZ_t%5Csim%20%5Cxi%7D%5B%5Cpsi(Z_t)%5Cotimes%5Cpsi(Z_t)%5D%5Csucc%200">, <img src="https://latex.codecogs.com/png.latex?m">-step TD learning with the internal state <img src="https://latex.codecogs.com/png.latex?Z_t"> converges to the unique fixed point <img src="https://latex.codecogs.com/png.latex?v_m"> of the equation <img src="https://latex.codecogs.com/png.latex?v%20=%20%5CPi%5C%7B%5Cbar%7Br%7D_m+%5Cgamma%5Em%20K_m%20v%5C%7D,"> where <img src="https://latex.codecogs.com/png.latex?%5CPi"> be the ortogonal projection onto the subspace <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20V%20:=%20%5C%7Bz%5Cmapsto%20%5Clangle%20%5Cpsi(z),w%5Crangle:%20w%20%5Cin%20%5Cmathbb%20R%5Ed%5C%7D"> with respect to <img src="https://latex.codecogs.com/png.latex?L%5E2(%5Cxi)">. Thus, we have the following result.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any <img src="https://latex.codecogs.com/png.latex?m%5Cin%5Cmathbb%7BZ%7D_+">, <img src="https://latex.codecogs.com/png.latex?m">-step TD(0) with an internal state <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k:k%5Cin%5Cmathbb%20N%5C%7D"> converges to <img src="https://latex.codecogs.com/png.latex?v_m">, which satisfies <img src="https://latex.codecogs.com/png.latex?%5C%7Cv_m%20-%20%5CPi%20V%5C%7C_%7BL%5E2(%5Cxi)%7D%20%5Cleq%20%5Cfrac%7B%5Cgamma%5Em%7D%7B1+%5Cgamma%5Em%7D%5C%7CV-%5CPi%20V%5C%7C_%7BL%5E2(%5Cxi)%7D."></p>
</div>
</div>
<p>As such, in case we are interested in policy evaluation for an FSC, this formulation implies the near optimality of <img src="https://latex.codecogs.com/png.latex?m">-step TD learning, which uses the internal state of the controller as the critic internal state, with an additional error term <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cgamma%5Em)">.</p>
</section>
<section id="learning-v_sstar-by-m-step-td0-with-a-sliding-window-internal-state" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="learning-v_sstar-by-m-step-td0-with-a-sliding-window-internal-state"><span class="header-section-number">4.2</span> Learning <img src="https://latex.codecogs.com/png.latex?V_s%5E%5Cstar"> by <img src="https://latex.codecogs.com/png.latex?m">-step TD(0) with a Sliding-Window Internal State</h3>
<p>The analysis of <img src="https://latex.codecogs.com/png.latex?m">-step TD(0) with an internal state <img src="https://latex.codecogs.com/png.latex?Z_k=(Y_%7Bk-n%7D,%5Cldots,Y_k)"> can be taken one step further indeed. Let us consider Example 1 with a general <img src="https://latex.codecogs.com/png.latex?%5C%7B%5CPhi(y%7Cs):s%5Cin%5Cmathcal%20S,y%5Cin%5Cmathcal%7BY%7D%5C%7D">. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D_n%20:=%5Csigma(Z_0)=%5Csigma(Y_%7B-n+1%7D,%5Cldots,Y_0)"> be the <img src="https://latex.codecogs.com/png.latex?%5Csigma">-algebra generated by the internal state, and <img src="https://latex.codecogs.com/png.latex?b%5E%7B(n)%7D%20:=%20P(S_0%20=%201%7C%5Cmathcal%7BF%7D_n)">. Then, <img src="https://latex.codecogs.com/png.latex?V_n(z_0)%20=%20%20%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%5Cgamma%5Ekr(S_k)%7C%5Cmathcal%7BF%7D_n%5D">. Since <img src="https://latex.codecogs.com/png.latex?V_%7BS_0%7D%5E%5Cstar%20=%20V_0%5E%5Cstar%20+%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?V_n(z_0)%20=%20V_0%5E%5Cstar%20+%20(V_1%5E%5Cstar-V_0%5E%5Cstar)b%5E%7B(n)%7D,"> we obtain <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(V_%7BS_0%7D%5E%5Cstar-V_n(Z_0))%5E2%5D%20=%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5C,%5Cmathbb%20E%5C!%5Cleft%5B(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D-b%5E%7B(n)%7D)%5E2%5Cright%5D."> Furthermore, since <img src="https://latex.codecogs.com/png.latex?b%5E%7B(n)%7D=%5Cmathbb%20E%5B%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_n%5D">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cmathbb%20E%5C!%5Cleft%5B(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D-b%5E%7B(n)%7D)%5E2%5Cright%5D&amp;=%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_n)%5D%5C%5C%0A&amp;%5Cdownarrow%20%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_%5Cinfty)%5D%0A%5Cend%7Balign%7D"> as <img src="https://latex.codecogs.com/png.latex?n%5Crightarrow%5Cinfty"> due to the law of total variance for nested <img src="https://latex.codecogs.com/png.latex?%5Csigma">-algebras <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D_n">. Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(V_%7BS_0%7D%5E%5Cstar-V_n(Z_0))%5E2%5D%20%5Cdownarrow%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5C,%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_%5Cinfty)%5D."> If <img src="https://latex.codecogs.com/png.latex?S_0"> is measurable with respect to <img src="https://latex.codecogs.com/png.latex?%5Csigma(Y_%7B-%5Cinfty%7D,%5Cldots,Y_0)">, we have <img src="https://latex.codecogs.com/png.latex?V_%7Bs_0%7D%5E%5Cstar=V_%5Cinfty(z_0))"> almost surely. Used in conjunction with the previous result on <img src="https://latex.codecogs.com/png.latex?v_m-V_n(z_0)">, a simple triangle inequality would demonstrate the effectiveness of TD learning with sliding-window controllers.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>In summary, for Example 1, we have <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%5Cmathbb%7BE%7D%5B(V_%7BS_0%7D%5E%5Cstar%20-%20v%5E%5Cstar)%5E2%5D%20&amp;=%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%20Var(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D)%5C%5C%0A&amp;%5Cgeq%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_n)%5D%20%5Cdownarrow%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_%5Cinfty)%5D.%0A%5Cend%7Balign*%7D"></p>
</div>
</div>
</section>
</section>
<section id="whats-next-nac-with-finite-state-controllers-for-pomdps" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="whats-next-nac-with-finite-state-controllers-for-pomdps"><span class="header-section-number">5</span> What’s next: NAC with Finite-State Controllers for POMDPs</h2>
<p>In the next step, we will show how FSC’s yield improved performance (even global near-optimality) for NAC under certain ergodicity conditions. The discussion will be based on stochastic filtering, and establish a trade-off between memory complexity and optimality gap, governed by the size of the internal state space.</p>
<hr>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-astrom1965" class="csl-entry">
Åström, Karl J. 1965. <span>“Optimal Control of Markov Processes with Incomplete State Information i.”</span> <em>Journal of Mathematical Analysis and Applications</em> 10 (1): 174–205. <a href="https://doi.org/10.1016/0022-247X(65)90154-X">https://doi.org/10.1016/0022-247X(65)90154-X</a>.
</div>
<div id="ref-murphy2000survey" class="csl-entry">
Murphy, Kevin P. 2000. <span>“A Survey of POMDP Solution Techniques.”</span> <em>Environment</em> 2 (10).
</div>
<div id="ref-singh1994learning" class="csl-entry">
Singh, Satinder P, Tommi Jaakkola, and Michael I Jordan. 1994. <span>“Learning Without State-Estimation in Partially Observable Markovian Decision Processes.”</span> In <em>Machine Learning Proceedings 1994</em>, 284–92. Elsevier.
</div>
<div id="ref-yuksel2025another" class="csl-entry">
Yüksel, Serdar. 2025. <span>“Another Look at Partially Observed Optimal Stochastic Control: Existence, Ergodicity, and Approximations Without Belief-Reduction.”</span> <em>Applied Mathematics &amp; Optimization</em> 91 (1): 16.
</div>
</div></section></div> ]]></description>
  <category>reinforcement-learning</category>
  <category>POMDP</category>
  <category>temporal difference learning</category>
  <guid>https://semihcayci.github.io/posts/td-learning-for-pomdps.html</guid>
  <pubDate>Sun, 12 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Posts</title>
  <link>https://semihcayci.github.io/posts/</link>
  <description><![CDATA[ 








<div class="quarto-listing quarto-listing-container-table" id="listing-listing">
<div class="listing-actions-group">
   <div class="input-group input-group-sm quarto-listing-sort">
    <span class="input-group-text"><i class="bi bi-sort-down"></i></span>
    <select id="listing-listing-sort" class="form-select" aria-label="Order By" onchange="window['quarto-listings']['listing-listing'].sort(this.options[this.selectedIndex].value, { order: this.options[this.selectedIndex].getAttribute('data-direction')})">
      <option value="" disabled="" selected="" hidden="">Order By</option>
      <option value="index" data-direction="asc">Default</option>
      <option value="listing-date-sort" data-direction="asc">
        Date - Oldest
      </option>
      <option value="listing-date-sort" data-direction="desc">
        Date - Newest
      </option>
      <option value="listing-title-sort" data-direction="asc">
        Title
      </option>
      <option value="listing-author" data-direction="asc">
        Author
      </option>
    </select>
  </div>
    <div class="input-group input-group-sm quarto-listing-filter">
      <span class="input-group-text"><i class="bi bi-search"></i></span>
      <input type="text" class="search form-control" placeholder="Filter">
    </div>
</div>
<table class="quarto-listing-table table">
<thead>
<tr>

<th>
<a class="sort" data-sort="listing-date-sort" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Date</a>
</th>

<th>
<a class="sort" data-sort="listing-title-sort" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Title</a>
</th>

<th>
<a class="sort" data-sort="listing-author" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Author</a>
</th>

</tr>
</thead>
<tbody class="list">

<tr data-index="0" data-categories="cmVpbmZvcmNlbWVudC1sZWFybmluZyUyQ1BPTURQJTJDdGVtcG9yYWwlMjBkaWZmZXJlbmNlJTIwbGVhcm5pbmc=" data-listing-date-sort="1760227200000" data-listing-file-modified-sort="1760675929010" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="9" data-listing-word-count-sort="1665" data-listing-title-sort="RL for POMDPs – Part I: TD Learning with Internal States" data-listing-filename-sort="td-learning-for-pomdps.qmd">
<td>
<span class="listing-date">Oct 12, 2025</span>
</td>
<td>
<a href="../posts/td-learning-for-pomdps.html" class="title listing-title">RL for POMDPs – Part I: TD Learning with Internal States</a>
</td>
<td>
<span class="listing-author">&nbsp;</span>
</td>

</tr>

</tbody>
</table>
<div class="listing-no-matching d-none">No matching items</div>
</div> ]]></description>
  <guid>https://semihcayci.github.io/posts/</guid>
  <pubDate>Fri, 17 Oct 2025 04:39:09 GMT</pubDate>
</item>
</channel>
</rss>
