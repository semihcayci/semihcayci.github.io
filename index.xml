<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Notes on Theoretical RL</title>
<link>https://semihcayci.github.io/</link>
<atom:link href="https://semihcayci.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Sun, 12 Oct 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>RL for POMDPs – Part I: TD Learning with Internal States</title>
  <link>https://semihcayci.github.io/posts/td-learning-for-pomdps.html</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p><strong>TL;DR.</strong> A classic result by <span class="citation" data-cites="singh1994learning">(Singh, Jaakkola, and Jordan 1994)</span> shows that memoryless TD learning fails in POMDPs. To address this, we studied a natural actor–critic (NAC) method for POMDPs in <span class="citation" data-cites="cayci2024nac">(Cayci, He, and Srikant 2024)</span>, with a critic that uses <img src="https://latex.codecogs.com/png.latex?m">-step TD learning with an internal state to mitigate perceptual aliasing problem. Below, I review this algorithm and provide some follow-up results.</p>
</blockquote>
<section id="formal-pomdp-model" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="formal-pomdp-model"><span class="header-section-number">1</span> Formal POMDP model</h2>
<p>Let us consider a discounted POMDP <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BM%7D=%5Cbig(%5Cmathcal%20S,%5Cmathcal%20A,%5Cmathcal%20Y,%5C;%20P,%5C;%20O,%5C;%20r,%5C;%20%5Cgamma%5Cbig),%0A"></p>
<p>with:</p>
<ul>
<li>state <img src="https://latex.codecogs.com/png.latex?S_k%5Cin%5Cmathcal%20S">,</li>
<li>action <img src="https://latex.codecogs.com/png.latex?A_k%5Cin%5Cmathcal%20A">,</li>
<li>observation <img src="https://latex.codecogs.com/png.latex?Y_k%5Cin%5Cmathcal%20Y">,</li>
<li>transition kernel <img src="https://latex.codecogs.com/png.latex?P(s'%20%5Cmid%20s,a)">,</li>
<li>observation kernel <img src="https://latex.codecogs.com/png.latex?%5CPhi(y%20%5Cmid%20s')">,</li>
<li>reward <img src="https://latex.codecogs.com/png.latex?r(s,a)%5Cin%5B0,1%5D"> (w.l.o.g.),</li>
<li>discount factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Cin(0,1)">.</li>
</ul>
<p>To avoid measure-theoretic concerns, let us consider finite <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S,%20%5Cmathcal%20Y,%20%5Cmathcal%20A">. These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is <img src="https://latex.codecogs.com/png.latex?%0AS_%7Bk+1%7D%5Csim%20P(%5Ccdot%5Cmid%20S_k,A_k),%5Cqquad%0AY_%7Bk%7D%5Csim%20%5CPhi(%5Ccdot%5Cmid%20S_k),%5Cqquad%0AR_k=r(S_k,A_k),%0A"> and the goal is to maximize <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%20V%5E%5Cpi(%5Cxi)=%5Cmathbb%20E_%5Cxi%5E%5Cpi%5C!%5Cleft%5B%5Csum_%7Bk%5Cge0%7D%5Cgamma%5Ek%5C,r(S_k,A_k)%5Cright%5D.%0A"></p>
<p><strong>Why challenging?</strong><br>
Partial observability implies that the optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E%5Cstar=(%5Cpi_0%5E%5Cstar,%5Cpi_1%5E%5Cstar,%5Cldots)"> is non-stationary, therefore <img src="https://latex.codecogs.com/png.latex?%5Cpi_k%5E%5Cstar"> depends on the complete trajectory <img src="https://latex.codecogs.com/png.latex?(Y_%7B0:k%7D,A_%7B0:k-1%7D)"> for each <img src="https://latex.codecogs.com/png.latex?k%5Cin%5Cmathbb%20N"> via the Bayes belief <img src="https://latex.codecogs.com/png.latex?b_k(s)=%5CPr(S_k=s%5Cmid%20Y_%7B0:k%7D,A_%7B0:k-1%7D)">. Reactive (i.e., memoryless) methods for MDPs fail miserably if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see <strong>Example 1</strong> below). A classical result attributed to <span class="citation" data-cites="astrom1965">(Åström 1965)</span> (see <span class="citation" data-cites="yuksel2025another">(Yüksel 2025)</span> for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state <img src="https://latex.codecogs.com/png.latex?b_k">. However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S">), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies <span class="citation" data-cites="murphy2000survey">(Murphy 2000)</span> and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec">, which summarizes/compresses <img src="https://latex.codecogs.com/png.latex?(Y_%7B0:k-1%7D,A_%7B0:k-1%7D)"> for each <img src="https://latex.codecogs.com/png.latex?k">. The idea is to use a parametric policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_k(%5Ccdot%5Cmid%20Y_k,%20Z_k%5Ec)"> based on the internal state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec">.</p>
</section>
<section id="setting-and-goal" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="setting-and-goal"><span class="header-section-number">2</span> Setting and Goal</h2>
<p>At first, we restrict policies to <strong>finite-state controllers (FSCs)</strong> that keep internal memory <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec"> and act from <img src="https://latex.codecogs.com/png.latex?(Y_k,%20Z_k%5Ec)">. The internal memory is updated in a Markovian way: <img src="https://latex.codecogs.com/png.latex?Z_%7Bk+1%7D%5Ec%5Csim%5Cvarphi(%5Ccdot%5Cmid%20z_k,%20y_k,%20a_k)"> given <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec=z_k,Y_k=y_k,A_k=a_k">.</p>
<ul>
<li><p>Learn the best policy within a fixed FSC class <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7BZ,%5Cvarphi%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%5E%5Cstar%20%5Cin%20%5Carg%5Cmax_%7B%5Cpi%20%5Cin%20%5Cpi_%7BZ,%5Cvarphi%7D%7D%20%5C;%20%5Cmathcal%20V%5E%5Cpi(%5Cxi).%0A"></p></li>
<li><p>A useful subclass is the <strong>sliding-window controller (SWC)</strong> with window <img src="https://latex.codecogs.com/png.latex?n">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ_k%5Ec%20=%20(Y_%7Bk-n:k-1%7D,%5C;%20A_%7Bk-n:k-1%7D)%20%5Cin%20%5Cmathcal%20Y%5En%20%5Ctimes%20%5Cmathcal%20A%5En,%0A"></p>
<p>which summarizes the last <img src="https://latex.codecogs.com/png.latex?n"> observations/actions, where <img src="https://latex.codecogs.com/png.latex?n"> is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.</p></li>
</ul>
<p>The actor uses a <strong>linear-softmax FSC</strong>: <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi_%5Ctheta(a%20%5Cmid%20y,z)%20=%0A%5Cfrac%7B%5Cexp%5C%7B%5Ctheta%5E%5Ctop%20%5Cpsi(a,y,z)%5C%7D%7D%0A%7B%5Csum_%7Ba'%7D%20%5Cexp%5C%7B%5Ctheta%5E%5Ctop%20%5Cpsi(a',y,z)%5C%7D%7D,%0A"> with features <img src="https://latex.codecogs.com/png.latex?%5Cpsi(a,y,z)%5Cin%5Cmathbb%20R%5Ed">. The controller memory updates via some <img src="https://latex.codecogs.com/png.latex?%5Cvarphi">: <img src="https://latex.codecogs.com/png.latex?%0AZ_%7Bk+1%7D%5Ec=%5Cvarphi(Z_k%5Ec,%20Y_%7Bk+1%7D,%20A_k).%0A"></p>
</section>
<section id="hidden-markov-reward-processes-hmrp" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="hidden-markov-reward-processes-hmrp"><span class="header-section-number">3</span> Hidden Markov Reward Processes (HMRP)</h2>
<p>A common simplification in TD learning analyses in an MDP setting is to consider a Markov reward process (MRP). Since the augmented state <img src="https://latex.codecogs.com/png.latex?X_k:=(S_k,A_k)"> under a given stationary policy forms a Markov chain, the analyses automatically extend to policy evaluation (known as SARSA) since <img src="https://latex.codecogs.com/png.latex?(X_k,r(X_k))"> forms an MRP.</p>
<p>In POMDPs, we can make a similar simplification. First, let us consider a Hidden Markov Process with a Markov chain <img src="https://latex.codecogs.com/png.latex?S_k"> and its partial observation process <img src="https://latex.codecogs.com/png.latex?Y_k%5Csim%5CPhi(%5Ccdot%5Cmid%20S_k)">. Along with the reward <img src="https://latex.codecogs.com/png.latex?r(S_k)">, this HMP induces a Hidden Markov Reward Process (HMRP) <img src="https://latex.codecogs.com/png.latex?%5C%7B(S_k,r(S_k),Y_k):k%5Cin%5Cmathbb%7BN%7D%5C%7D">.</p>
<p>Consider a finite-state controller with an internal state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec">. Then, we have <img src="https://latex.codecogs.com/png.latex?A_k%20%5Csim%20%5Cpi_k(%5Ccdot%5Cmid%20Z_k%5Ec)">, thus <img src="https://latex.codecogs.com/png.latex?X_k:=(S_k,A_k,Z_k%5Ec)"> forms a Markov process with the transition kernel <img src="https://latex.codecogs.com/png.latex?P(S_%7Bk+1%7D=s',A_%7Bk+1%7D=a',Z_%7Bk+1%7D%5Ec=z'%7CS_k,A_k,Z_k%5Ec)%20=%20%5Csum_%7By'%5Cin%5Cmathcal%20Y%7DP(s'%7CS_k,A_k)%5CPhi(y'%7Cs')%5Cvarphi(z'%7CZ_k%5Ec,y',A_k)%5Cpi_%7Bk+1%7D(a'%5Cmid%20y',z')."> The noisy observation is <img src="https://latex.codecogs.com/png.latex?Y_k~%5CPhi(%5Ccdot%7CS_k)">. Defining <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Br%7D(S_k,A_k,Z_k%5Ec)%20=%20r(S_k,A_k)">, we obtain the original process under <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Thus, the controlled process under <img src="https://latex.codecogs.com/png.latex?%5Cpi"> induces the HMRP <img src="https://latex.codecogs.com/png.latex?(X_k,%20%5Ctilde%7Br%7D(X_k),%20Y_k)">.</p>
</section>
<section id="critic-m-step-td-learning-with-an-internal-state" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="critic-m-step-td-learning-with-an-internal-state"><span class="header-section-number">4</span> Critic: <img src="https://latex.codecogs.com/png.latex?m">-step TD Learning with an Internal State</h2>
<p>It is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology <span class="citation" data-cites="singh1994learning">(Singh, Jaakkola, and Jordan 1994)</span>. For a concrete demonstration, let us focus on learning the value function in an HMRP: <img src="https://latex.codecogs.com/png.latex?V(z_0):=%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ek%20r(S_k)%7CZ_0=z_0%5D%5Cmbox%7B%20and%20%7DV_s%5E%5Cstar:=%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20r(S_k)%7CS_0=s%5D"> for an internal state <img src="https://latex.codecogs.com/png.latex?Z_k"> (e.g., <img src="https://latex.codecogs.com/png.latex?Z_t%20=%20(Y_%7Bt-n+1%7D,%5Cldots,Y_t)">), where <img src="https://latex.codecogs.com/png.latex?(S_k,%20r(S_k),%20Y_k)"> is an HMRP.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Two objectives.</strong> In the above display, if we want to perform policy evaluation for an FSC with <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k%5Ec:k%5Cin%5Cmathbb%20N%5C%7D">, we would be interested in <img src="https://latex.codecogs.com/png.latex?V(z_0)">. Additionally, we can use <img src="https://latex.codecogs.com/png.latex?m">-step TD Learning for estimating the value function <img src="https://latex.codecogs.com/png.latex?V_s%5E%5Cstar">. We discuss both problems in what follows.</p>
</div>
</div>
<p><strong>Example 1 (When TD(0) fails to learn <img src="https://latex.codecogs.com/png.latex?V_s%5E%5Cstar">).</strong> Here we consider an HMRP with <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S%20=%20%5C%7B0,1%5C%7D">, <img src="https://latex.codecogs.com/png.latex?r(s)%20=%20s"> and <img src="https://latex.codecogs.com/png.latex?%5CPhi(1%7Cs)%20=%201"> for all <img src="https://latex.codecogs.com/png.latex?s%5Cin%5Cmathcal%7BS%7D">. Let <img src="https://latex.codecogs.com/png.latex?P(0%7C1)%20=%20q"> and <img src="https://latex.codecogs.com/png.latex?P(1%7C0)%20=%20p"> for some <img src="https://latex.codecogs.com/png.latex?p,%20q%20%5Cin%20(0,1)">, and <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cin%20(0,%201)"> be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to <img src="https://latex.codecogs.com/png.latex?v%5E%5Cstar%20=%20%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%5Ccdot%20%5Cfrac%7Bp%7D%7Bq+p%7D%20=%20V_0%5E%5Cstar%20+%20%5Cfrac%7Bp%7D%7Bp+q%7D(V_1%5E%5Cstar-V_0%5E%5Cstar)">. In the proof of Theorem 4.1 in <span class="citation" data-cites="cayci2024nac">(Cayci, He, and Srikant 2024)</span>, we characterize the issues related to the fixed point of single-step TD(0) for general POMDPs.</p>
<p>On the other hand, the value function for a given <img src="https://latex.codecogs.com/png.latex?z_0"> is <img src="https://latex.codecogs.com/png.latex?V(z_0)%20=%20V_0%5E%5Cstar%20+%20(V_1%5E%5Cstar-V_0%5E%5Cstar)P(S_0=1%7CZ_0=z_0)%20=%20%5Cfrac%7B%5Cgamma%20p%20+%20(1-%5Cgamma)P(S_0=1%7CZ_0=z_0)%7D%7B(1-%5Cgamma)%5CBig%5B%5Cgamma(p+q)+1-%5Cgamma%5CBig%5D%7D,"> since <img src="https://latex.codecogs.com/png.latex?V(z_0)%20=%20V_0%5E%5Cstar+(V_1%5E%5Cstar-V_0%5E%5Cstar)P(S_0=1%7CZ_0=z_0)">. Also, <img src="https://latex.codecogs.com/png.latex?V_%7BS_0%7D%5E%5Cstar%20=%20V_0%5E%5Cstar%20+%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D">. Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(V_%7BS_0%7D%5E%5Cstar-v%5E%5Cstar)%5E2%5D%20=%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5Cmathbb%7BE%7D%5B(%5Cfrac%7Bp%7D%7Bp+q%7D-%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D)%5E2%5D=Var(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D)(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2."></p>
<p><strong>Example 2 (When TD(0) fails to learn <img src="https://latex.codecogs.com/png.latex?V(z_0)">).</strong> Consider the two-state HMRP with <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S%20=%20%5C%7B0,1%5C%7D">, <img src="https://latex.codecogs.com/png.latex?r(s)%20=%20s"> and <img src="https://latex.codecogs.com/png.latex?%5CPhi(1%7Cs)%20=%20c_s%5Cin(0,1)"> now. Let <img src="https://latex.codecogs.com/png.latex?P(0%7C1)%20=%20P(1%7C0)%20=%201/2">, and <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%200.9">. Consider an internal state <img src="https://latex.codecogs.com/png.latex?Z_k%20=%20(Y_%7Bk-1%7D,%20Y_k)">. We want to compute <img src="https://latex.codecogs.com/png.latex?V(z_0)%20=%20%5Cmathbb%7BE%7D%5B%5Csum_k%20%5Cgamma%5Ek%20r(S_k)%7CZ_0=z_0%5D">. For this purpose, one can use tabular-TD(0) with the surrogate state <img src="https://latex.codecogs.com/png.latex?Z_k=(Y_%7Bk-1%7D,Y_k)">. In this case, the limit point of TD(0) would be the unique fixed point of <img src="https://latex.codecogs.com/png.latex?v_1(z_0)%20=%20P(S_0=1%7CZ_0=z_0)%20+%20%5Cgamma%20%5Csum_%7Bz_1%5Cin%5C%7B0,1%5C%7D%5E2%7Dv(z_1)P(Z_1=z_1%7CZ_0=z_0)."> Substituting <img src="https://latex.codecogs.com/png.latex?c_0%20=%201/4,%20c_1%20=%203/4">, we obtain the fixed point <img src="https://latex.codecogs.com/png.latex?%0Av_1(z_0)%20=%0A%5Cbegin%7Bcases%7D%0A4.1396,%20&amp;%20z_0%20=%20(0,0),%20%5C%5C%0A5.2739,%20&amp;%20z_0%20=%20(0,1),%20%5C%5C%0A4.7261,%20&amp;%20z_0%20=%20(1,0),%20%5C%5C%0A5.8604,%20&amp;%20z_0%20=%20(1,1).%0A%5Cend%7Bcases%7D%0A"> On the other hand, the true value function <img src="https://latex.codecogs.com/png.latex?V(z_0)"> is as follows: <img src="https://latex.codecogs.com/png.latex?%0AV(z_0)%20=%0A%5Cbegin%7Bcases%7D%0A1.6306,%20&amp;%20z_0%20=%20(0,0),%20%5C%5C%0A5.0561,%20&amp;%20z_0%20=%20(0,1),%20%5C%5C%0A4.9439,%20&amp;%20z_0%20=%20(1,0),%20%5C%5C%0A8.3694,%20&amp;%20z_0%20=%20(1,1).%0A%5Cend%7Bcases%7D%0A"> This indicates that a naive application of TD(0) with the internal state <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k:k%5Cin%5Cmathbb%20N%5C%7D"> does not return the true value function <img src="https://latex.codecogs.com/png.latex?V(z_0)">. Thus, in case we want to evaluate the performance of a policy with an internal state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec%20=%20(Y_%7Bk-1%7D,%20A_%7Bk-1%7D)">, TD(0) is not useful.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Why <img src="https://latex.codecogs.com/png.latex?m">-step TD?</strong> Examples 1 and 2 show that, in POMDPs, one-step bootstrapping suffers from <strong>perceptual aliasing</strong> (same observation <img src="https://latex.codecogs.com/png.latex?Y_k">, different latent state <img src="https://latex.codecogs.com/png.latex?S_k">). We will see in the following that looking <img src="https://latex.codecogs.com/png.latex?m"> steps ahead stabilizes the target and reduces this bias.</p>
</div>
</div>
<p>Now, let us focus on <img src="https://latex.codecogs.com/png.latex?m">-step TD learning with an internal state <img src="https://latex.codecogs.com/png.latex?Z_k"> for the above problem, where <img src="https://latex.codecogs.com/png.latex?m%5Cin%5Cmathbb%7BZ%7D_+">. Note that the internal state may match the controller state <img src="https://latex.codecogs.com/png.latex?Z_k%5Ec"> (in the case of control, see Section 3), or it can be used to obtain an estimate of <img src="https://latex.codecogs.com/png.latex?V_s%5E%5Cstar">.</p>
<section id="learning-vz_0-by-m-step-td0-with-an-internal-state" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="learning-vz_0-by-m-step-td0-with-an-internal-state"><span class="header-section-number">4.1</span> Learning <img src="https://latex.codecogs.com/png.latex?V(z_0)"> by <img src="https://latex.codecogs.com/png.latex?m">-step TD(0) with an Internal State</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?(T_mv)(z)%20=%20%5Cbar%7Br%7D_m(z)%20+%20%5Cgamma%5Em(K_mv)(z),"> where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Br%7D_m(z)%20:=%20%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%7Bm-1%7Dr(S_k)%7CZ_0=z%5D"> and <img src="https://latex.codecogs.com/png.latex?(K_mv)(z):=%5Cmathbb%7BE%7D%5Bv(Z_m)%7CZ_0=z%5D">. Then, it is straightforward to show that <img src="https://latex.codecogs.com/png.latex?T_m"> is a contractive operator with modulus <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Em">, and admits a unique fixed point <img src="https://latex.codecogs.com/png.latex?V(z)%20=%20%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ek%20r(S_k)%7CZ_0=z%5D%20=%20%5Csum_%7Bk=0%7D%5E%5Cinfty%20(%5Cgamma%5EmK_m)%5Ek%5Cbar%7Br%7D_m(z)"> for any <img src="https://latex.codecogs.com/png.latex?m%5Cin%5Cmathbb%20N">. Let <img src="https://latex.codecogs.com/png.latex?%5Cxi"> be the stationary distribution of <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k:k%5Cin%5Cmathbb%20N%5C%7D">, which exists given <img src="https://latex.codecogs.com/png.latex?S_k"> is an ergodic unichain. Under a non-degenerate <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7BZ_t%5Csim%20%5Cxi%7D%5B%5Cpsi(Z_t)%5Cotimes%5Cpsi(Z_t)%5D%5Csucc%200">, <img src="https://latex.codecogs.com/png.latex?m">-step TD learning with the internal state <img src="https://latex.codecogs.com/png.latex?Z_t"> converges to the unique fixed point <img src="https://latex.codecogs.com/png.latex?v_m"> of the equation <img src="https://latex.codecogs.com/png.latex?v%20=%20%5CPi%5C%7B%5Cbar%7Br%7D_m+%5Cgamma%5Em%20K_m%20v%5C%7D,"> where <img src="https://latex.codecogs.com/png.latex?%5CPi"> be the ortogonal projection onto the subspace <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20V%20:=%20%5C%7Bz%5Cmapsto%20%5Clangle%20%5Cpsi(z),w%5Crangle:%20w%20%5Cin%20%5Cmathbb%20R%5Ed"> with respect to <img src="https://latex.codecogs.com/png.latex?L%5E2(%5Cxi)">. Thus, we have the following result. ::: {.theorem} For any <img src="https://latex.codecogs.com/png.latex?m%5Cin%5Cmathbb%7BZ%7D_+">, <img src="https://latex.codecogs.com/png.latex?m">-step TD(0) with an internal state <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k:k%5Cin%5Cmathbb%20N%5C%7D"> converges to <img src="https://latex.codecogs.com/png.latex?v_m">, which satisfies <img src="https://latex.codecogs.com/png.latex?%5C%7Cv_m%20-%20%5CPi%20V%5C%7C_%7BL%5E2(%5Cxi)%7D%20%5Cleq%20%5Cfrac%7B%5Cgamma%5Em%7D%7B1+%5Cgamma%5Em%7D%5C%7CV-%5CPi%20V%5C%7C_%7BL%5E2(%5Cxi)%7D."> ::: As such, in case we are interested in policy evaluation for an FSC, this formulation implies the near optimality of <img src="https://latex.codecogs.com/png.latex?m">-step TD learning, which uses the internal state of the controller as the critic internal state, with an additional error term <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cgamma%5Em)">.</p>
<p>The analysis can be taken one step further indeed. The argument below can be shifted in time, so let us consider <img src="https://latex.codecogs.com/png.latex?t=0"> for notational simplicity. Let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D_m%20:=%5Csigma(Z_0)=%5Csigma(Y_%7B-m+1%7D,%5Cldots,Y_0)"> be the <img src="https://latex.codecogs.com/png.latex?%5Csigma">-algebra generated by the internal state, and <img src="https://latex.codecogs.com/png.latex?b%5E%7B(m)%7D%20:=%20P(S_0%20=%201%7C%5Cmathcal%7BF%7D_m)">. Then, <img src="https://latex.codecogs.com/png.latex?V(z_0)%20=%20%20%5Cmathbb%20E%5B%5Csum_%7Bt=0%7D%5E%5Cinfty%5Cgamma%5Etr(S_t)%7C%5Cmathcal%7BF%7D_m%5D">. Since <img src="https://latex.codecogs.com/png.latex?V_s%5E%5Cstar%20=%20V_0%5E%5Cstar%20+%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5Cmathbf%7B1%7D_%7B%5C%7BS_0=s%5C%7D%7D"> and <img src="https://latex.codecogs.com/png.latex?V(z_0)%20=%20V_0%5E%5Cstar%20+%20(V_1%5E%5Cstar-V_0%5E%5Cstar)b%5E%7B(m)%7D">, we obtain <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(V_%7BS_0%7D%5E%5Cstar-V(z_0))%5E2%5D%20=%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5C,%5Cmathbb%20E%5C!%5Cleft%5B(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D-b%5E%7B(m)%7D)%5E2%5Cright%5D."> Furthermore, since <img src="https://latex.codecogs.com/png.latex?b%5E%7B(m)%7D=%5Cmathbb%20E%5B%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_m%5D">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cmathbb%20E%5C!%5Cleft%5B(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D-b%5E%7B(m)%7D)%5E2%5Cright%5D&amp;=%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_m)%5D%5C%5C%0A&amp;%5Cdownarrow%20%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_%5Cinfty)%5D%0A%5Cend%7Balign%7D"> as <img src="https://latex.codecogs.com/png.latex?m%5Crightarrow%5Cinfty"> due to the law of total variance for nested <img src="https://latex.codecogs.com/png.latex?%5Csigma">-algebras <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D_m">. Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(V_%7Bs_0%7D%5E%5Cstar-V(z_0))%5E2%5D%20%5Cdownarrow%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5C,%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_%5Cinfty)%5D."> If <img src="https://latex.codecogs.com/png.latex?S_0"> is measurable with respect to <img src="https://latex.codecogs.com/png.latex?%5Csigma(Y_%7B-%5Cinfty%7D,%5Cldots,Y_0)">, we have <img src="https://latex.codecogs.com/png.latex?V_%7Bs_0%7D%5E%5Cstar=V(z_0))"> almost surely. Used in conjunction with the previous result on <img src="https://latex.codecogs.com/png.latex?v_m-V(z_0)">, a simple triangle inequality would demonstrate the effectiveness of TD learning with sliding-window controllers.</p>
<p>In summary, we have <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%5Cmathbb%7BE%7D%5B(V_%7BS_0%7D%5E%5Cstar%20-%20v%5E%5Cstar)%5E2%5D%20&amp;=%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%20Var(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D)%5C%5C%0A&amp;%5Cgeq%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_m)%5D%20%5Cdownarrow%20(V_1%5E%5Cstar-V_0%5E%5Cstar)%5E2%5Cmathbb%20E%5BVar(%5Cmathbf%7B1%7D_%7B%5C%7BS_0=1%5C%7D%7D%7C%5Cmathcal%20F_%5Cinfty)%5D.%0A%5Cend%7Balign*%7D"></p>
<p>With stepsize <img src="https://latex.codecogs.com/png.latex?%5Calpha=K%5E%7B-1/2%7D"> and parameter radius <img src="https://latex.codecogs.com/png.latex?R">, after <img src="https://latex.codecogs.com/png.latex?K"> critic updates: <img src="https://latex.codecogs.com/png.latex?%0A%5Csqrt%7B%5Cmathbb%20E%5C!%5Cleft%5B%5Cbig%5C%7C%5Cmathcal%20Q%5E%5Cpi-%5Cwidehat%7B%5Cmathcal%20Q%7D%5E%5Cpi_K%5Cbig%5C%7C%5E2_%7B%5C,d%5E%5Cpi_%5Cxi%5Cotimes%5Cpi%7D%5Cright%5D%7D%0A%5C;%5Clesssim%5C;%0A%5Cunderbrace%7B%5Ctfrac%7BK%5E%7B-1/4%7D%7D%7B1-%5Cgamma%7D%7D_%7B%5Ctext%7Bstatistical%7D%7D%0A+%5Cunderbrace%7B%5Ctfrac%7B%5Cvarepsilon_%7B%5Ctext%7Bapp%7D%7D(R)%7D%7B1-%5Cgamma%5Em%7D%7D_%7B%5Ctext%7Bapproximation%7D%7D%0A+%5Cunderbrace%7B%5Cvarepsilon_%7B%5Ctext%7Bpa%7D%7D(%5Cgamma,m,R)%7D_%7B%5Ctext%7Baliasing%7D%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon_%7B%5Ctext%7Bapp%7D%7D(R)"> is the best linear-approximation error within radius <img src="https://latex.codecogs.com/png.latex?R">, and the aliasing term contracts geometrically: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarepsilon_%7B%5Ctext%7Bpa%7D%7D(%5Cgamma,m,R)=%5Cmathcal%20O%5C!%5Cbig(%5Cgamma%5E%7Bm/2%7D%5C,%5Cmathrm%7Bpoly%7D(R,(1-%5Cgamma)%5E%7B-1%7D)%5Cbig).%0A"></p>
<p><strong>Sample–accuracy trade-off.</strong> Each update uses <img src="https://latex.codecogs.com/png.latex?m"> samples, so total evaluation samples are <img src="https://latex.codecogs.com/png.latex?%5CTheta(mK)">.</p>
</section>
</section>
<section id="full-actorcritic-performance-finite-time" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="full-actorcritic-performance-finite-time"><span class="header-section-number">5</span> Full Actor–Critic Performance (Finite-Time)</h2>
<p>After <img src="https://latex.codecogs.com/png.latex?T"> outer iterations (with tuned stepsizes), <img src="https://latex.codecogs.com/png.latex?%0A(1-%5Cgamma)%5C,%5Cmin_%7Bt%3CT%7D%5Cmathbb%20E%5C!%5Cbig%5B%5Cmathcal%20V%5E%7B%5Cpi%5E%5Cstar%7D(%5Cxi)-%5Cmathcal%20V%5E%7B%5Cpi_t%7D(%5Cxi)%5Cbig%5D%0A%5C;%5Clesssim%5C;%0A%5Cunderbrace%7BT%5E%7B-1/2%7D%7D_%7B%5Ctext%7Bactor%20optimization%7D%7D%0A+%5Cunderbrace%7B%5Cvarepsilon_%7B%5Ctext%7Bcritic%7D%7D(K,m,R)%7D_%7B%5Ctext%7BTD%20evaluation%7D%7D%0A+%5Cunderbrace%7B%5Cvarepsilon_%7B%5Ctext%7Bactor%7D%7D(N,R)%7D_%7B%5Ctext%7Bcompatible%20approx.%7D%7D%0A+%5Cunderbrace%7B%5Cvarepsilon_%7B%5Ctext%7Binf%7D%7D(%5Cxi)%7D_%7B%5Ctext%7Binference%20penalty%7D%7D.%0A"></p>
<p>The <strong>inference error</strong> is the price of partial observability: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarepsilon_%7B%5Ctext%7Binf%7D%7D(%5Cxi)%0A=%20%5Cmathbb%20E%5C!%5Cleft%5B%5Csum_%7Bk%5Cge0%7D%5Cgamma%5Ek%0A%5Cleft%5C%7C%5C,b_k(%5Ccdot)-b_0(%5Ccdot,I_k)%5C,%5Cright%5C%7C_%7B%5Cmathrm%7BTV%7D%7D%0A%5Cright%5D,%5Cqquad%20I_k=(Y_k,Z_k).%0A"></p>
</section>
<section id="sliding-window-controllers-memory-vs.-accuracy" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="sliding-window-controllers-memory-vs.-accuracy"><span class="header-section-number">6</span> Sliding-Window Controllers (Memory vs.&nbsp;Accuracy)</h2>
<p>Under stochastic exploration and filter stability/minorization, the inference error <strong>decays geometrically</strong> with window length <img src="https://latex.codecogs.com/png.latex?n">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarepsilon_%7B%5Ctext%7Binf%7D%7D(%5Cxi)%5C;%5Cle%5C;%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%5Ccdot%0A%5Cmathcal%20O%5C!%5CBig(%5Crho%5E%7B%5Clfloor%20n/m_0%5Crfloor%7D%5CBig),%0A"> for constants <img src="https://latex.codecogs.com/png.latex?%5Crho%5Cin(0,1)"> and <img src="https://latex.codecogs.com/png.latex?m_0%5Cge1">. To reach tolerance <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">: <img src="https://latex.codecogs.com/png.latex?%0An=%5Cmathcal%20O%5C!%5Cbig(m_0%5Clog(1/%5Cepsilon)%5Cbig).%0A"></p>
</section>
<section id="practical-tuning" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="practical-tuning"><span class="header-section-number">7</span> Practical Tuning</h2>
<ul>
<li><strong>TD horizon <img src="https://latex.codecogs.com/png.latex?m">:</strong> choose <img src="https://latex.codecogs.com/png.latex?m=%5CTheta(%5Clog_%7B1/%5Cgamma%7D(1/%5Cepsilon))"> to make aliasing <img src="https://latex.codecogs.com/png.latex?%5Clesssim%5Cepsilon">.</li>
<li><strong>Critic steps <img src="https://latex.codecogs.com/png.latex?K"> and actor inner steps <img src="https://latex.codecogs.com/png.latex?N">:</strong> about <img src="https://latex.codecogs.com/png.latex?%5CTheta(%5Cepsilon%5E%7B-4%7D)">.</li>
<li><strong>Features:</strong> richer features shrink the compatible approximation term.</li>
<li><strong>Window <img src="https://latex.codecogs.com/png.latex?n">:</strong> grows like <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20O(%5Clog(1/%5Cepsilon))"> under filter stability.</li>
</ul>
</section>
<section id="whats-next-rnn-based-nac-for-pomdps" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="whats-next-rnn-based-nac-for-pomdps"><span class="header-section-number">8</span> What’s next: RNN-based NAC for POMDPs</h2>
<p>A natural extension replaces the hand-engineered memory <img src="https://latex.codecogs.com/png.latex?Z_k"> with a <strong>recurrent hidden state</strong> <img src="https://latex.codecogs.com/png.latex?H_k"> learned end-to-end (RNN-based NAC). The policy becomes <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta(a%5Cmid%20Y_k,%20H_k)"> with <img src="https://latex.codecogs.com/png.latex?H_%7Bk+1%7D=f_%5Ctheta(H_k,Y_%7Bk+1%7D,A_k)">, and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs.&nbsp;representation power, and how partial-observability error shows up) in a follow-up post about our <strong>RNN-based natural actor–critic</strong> paper.</p>
<hr>




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-astrom1965" class="csl-entry">
Åström, Karl J. 1965. <span>“Optimal Control of Markov Processes with Incomplete State Information i.”</span> <em>Journal of Mathematical Analysis and Applications</em> 10 (1): 174–205. <a href="https://doi.org/10.1016/0022-247X(65)90154-X">https://doi.org/10.1016/0022-247X(65)90154-X</a>.
</div>
<div id="ref-cayci2024nac" class="csl-entry">
Cayci, Semih, Niao He, and R. Srikant. 2024. <span>“Finite-Time Analysis of Natural Actor-Critic for POMDPs.”</span> <em>SIAM Journal on Mathematics of Data Science</em>.
</div>
<div id="ref-murphy2000survey" class="csl-entry">
Murphy, Kevin P. 2000. <span>“A Survey of POMDP Solution Techniques.”</span> <em>Environment</em> 2 (10).
</div>
<div id="ref-singh1994learning" class="csl-entry">
Singh, Satinder P, Tommi Jaakkola, and Michael I Jordan. 1994. <span>“Learning Without State-Estimation in Partially Observable Markovian Decision Processes.”</span> In <em>Machine Learning Proceedings 1994</em>, 284–92. Elsevier.
</div>
<div id="ref-yuksel2025another" class="csl-entry">
Yüksel, Serdar. 2025. <span>“Another Look at Partially Observed Optimal Stochastic Control: Existence, Ergodicity, and Approximations Without Belief-Reduction.”</span> <em>Applied Mathematics &amp; Optimization</em> 91 (1): 16.
</div>
</div></section></div> ]]></description>
  <category>reinforcement-learning</category>
  <category>POMDP</category>
  <category>temporal difference learning</category>
  <guid>https://semihcayci.github.io/posts/td-learning-for-pomdps.html</guid>
  <pubDate>Sun, 12 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Posts</title>
  <link>https://semihcayci.github.io/posts/</link>
  <description><![CDATA[ 








<div class="quarto-listing quarto-listing-container-table" id="listing-listing">
<div class="listing-actions-group">
   <div class="input-group input-group-sm quarto-listing-sort">
    <span class="input-group-text"><i class="bi bi-sort-down"></i></span>
    <select id="listing-listing-sort" class="form-select" aria-label="Order By" onchange="window['quarto-listings']['listing-listing'].sort(this.options[this.selectedIndex].value, { order: this.options[this.selectedIndex].getAttribute('data-direction')})">
      <option value="" disabled="" selected="" hidden="">Order By</option>
      <option value="index" data-direction="asc">Default</option>
      <option value="listing-date-sort" data-direction="asc">
        Date - Oldest
      </option>
      <option value="listing-date-sort" data-direction="desc">
        Date - Newest
      </option>
      <option value="listing-title-sort" data-direction="asc">
        Title
      </option>
      <option value="listing-author" data-direction="asc">
        Author
      </option>
    </select>
  </div>
    <div class="input-group input-group-sm quarto-listing-filter">
      <span class="input-group-text"><i class="bi bi-search"></i></span>
      <input type="text" class="search form-control" placeholder="Filter">
    </div>
</div>
<table class="quarto-listing-table table">
<thead>
<tr>

<th>
<a class="sort" data-sort="listing-date-sort" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Date</a>
</th>

<th>
<a class="sort" data-sort="listing-title-sort" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Title</a>
</th>

<th>
<a class="sort" data-sort="listing-author" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Author</a>
</th>

</tr>
</thead>
<tbody class="list">

<tr data-index="0" data-categories="cmVpbmZvcmNlbWVudC1sZWFybmluZyUyQ1BPTURQJTJDdGVtcG9yYWwlMjBkaWZmZXJlbmNlJTIwbGVhcm5pbmc=" data-listing-date-sort="1760227200000" data-listing-file-modified-sort="1760610046623" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="9" data-listing-word-count-sort="1703" data-listing-title-sort="RL for POMDPs – Part I: TD Learning with Internal States" data-listing-filename-sort="td-learning-for-pomdps.qmd">
<td>
<span class="listing-date">Oct 12, 2025</span>
</td>
<td>
<a href="../posts/td-learning-for-pomdps.html" class="title listing-title">RL for POMDPs – Part I: TD Learning with Internal States</a>
</td>
<td>
<span class="listing-author">&nbsp;</span>
</td>

</tr>

</tbody>
</table>
<div class="listing-no-matching d-none">No matching items</div>
</div> ]]></description>
  <guid>https://semihcayci.github.io/posts/</guid>
  <pubDate>Thu, 16 Oct 2025 10:20:59 GMT</pubDate>
</item>
</channel>
</rss>
