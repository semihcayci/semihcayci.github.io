<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Semih Cayci — Blog</title>
<link>https://semihcayci.github.io/</link>
<atom:link href="https://semihcayci.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.25</generator>
<lastBuildDate>Sun, 12 Oct 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers</title>
  <link>https://semihcayci.github.io/posts/nac-for-pomdps.html</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p><strong>TL;DR.</strong> In <span class="citation" data-cites="cayci2024nac">(Cayci, He, and Srikant 2024)</span>, we studied a natural actor–critic (NAC) method in POMDPs. Critic uses <img src="https://latex.codecogs.com/png.latex?m">-step TD learning to mitigate perceptual aliasing and the actor uses an internal state to incorporate memory. We establish non-asymptotic bounds for optimization/approximation, evaluation, and inference (partial observability) errors. For sliding-window controllers, the inference error decays geometrically with the window size under certain ergodicity conditions.</p>
</blockquote>
<section id="formal-pomdp-model" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="formal-pomdp-model"><span class="header-section-number">1</span> Formal POMDP model</h2>
<p>Let us consider a discounted POMDP <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BM%7D=%5Cbig(%5Cmathcal%20S,%5Cmathcal%20A,%5Cmathcal%20Y,%5C;%20P,%5C;%20O,%5C;%20r,%5C;%20%5Cgamma%5Cbig),%0A"></p>
<p>with:</p>
<ul>
<li>state <img src="https://latex.codecogs.com/png.latex?S_k%5Cin%5Cmathcal%20S">,</li>
<li>action <img src="https://latex.codecogs.com/png.latex?A_k%5Cin%5Cmathcal%20A">,</li>
<li>observation <img src="https://latex.codecogs.com/png.latex?Y_k%5Cin%5Cmathcal%20Y">,</li>
<li>transition kernel <img src="https://latex.codecogs.com/png.latex?P(s'%20%5Cmid%20s,a)">,</li>
<li>observation kernel <img src="https://latex.codecogs.com/png.latex?%5CPhi(y%20%5Cmid%20s')">,</li>
<li>reward <img src="https://latex.codecogs.com/png.latex?r(s,a)%5Cin%5B0,1%5D"> (w.l.o.g.),</li>
<li>discount factor <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Cin(0,1)">.</li>
</ul>
<p>To avoid measure-theoretic concerns, let us consider finite <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S,%20%5Cmathcal%20Y,%20%5Cmathcal%20A">. These simplifications can later be relaxed, but the problem itself is quite challenging; thus we need to avoid clutter as much as possible. The generative process is <img src="https://latex.codecogs.com/png.latex?%0AS_%7Bk+1%7D%5Csim%20P(%5Ccdot%5Cmid%20S_k,A_k),%5Cqquad%0AY_%7Bk%7D%5Csim%20%5CPhi(%5Ccdot%5Cmid%20S_k),%5Cqquad%0AR_k=r(S_k,A_k),%0A"> and the goal is to maximize <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%20V%5E%5Cpi(%5Cxi)=%5Cmathbb%20E_%5Cxi%5E%5Cpi%5C!%5Cleft%5B%5Csum_%7Bk%5Cge0%7D%5Cgamma%5Ek%5C,r(S_k,A_k)%5Cright%5D.%0A"></p>
<p><strong>Why challenging?</strong><br>
Partial observability implies that the optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E%5Cstar=(%5Cpi_0%5E%5Cstar,%5Cpi_1%5E%5Cstar,%5Cldots)"> is non-stationary, therefore <img src="https://latex.codecogs.com/png.latex?%5Cpi_k%5E%5Cstar"> depends on the complete trajectory <img src="https://latex.codecogs.com/png.latex?(Y_%7B0:k%7D,A_%7B0:k-1%7D)"> for each <img src="https://latex.codecogs.com/png.latex?k%5Cin%5Cmathbb%20N"> via the Bayes belief <img src="https://latex.codecogs.com/png.latex?b_k(s)=%5CPr(S_k=s%5Cmid%20Y_%7B0:k%7D,A_%7B0:k-1%7D)">. Reactive (i.e., memoryless) methods for MDPs fail miserably if they are used in POMDPs due to the discrepancy between the partial observation and the latent state (see <strong>Example 1</strong> below). A classical result attributed to <span class="citation" data-cites="astrom1965">(Åström 1965)</span> (see <span class="citation" data-cites="yuksel2025another">(Yüksel 2025)</span> for further information) shows that a POMDP can be formulated as an MDP with a distribution-valued state <img src="https://latex.codecogs.com/png.latex?b_k">. However, since the state-space for this induced MDP is huge (remember that it is a probability simplex over <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S">), the conventional RL tools is highly impractical to solve POMDPs via this belief-MDP formulation. An alternative and more elegant idea is to incorporate memory into the policies <span class="citation" data-cites="murphy2000survey">(Murphy 2000)</span> and design/adapt RL algorithms enriched with this memory structure. Suppose that we have an internal state <img src="https://latex.codecogs.com/png.latex?Z_k">, which summarizes/compresses <img src="https://latex.codecogs.com/png.latex?(Y_%7B0:k-1%7D,A_%7B0:k-1%7D)"> for each <img src="https://latex.codecogs.com/png.latex?k">. The idea is to use a parametric policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_k(%5Ccdot%5Cmid%20Y_k,%20Z_k)"> based on the internal state <img src="https://latex.codecogs.com/png.latex?Z_k">.</p>
</section>
<section id="setting-and-goal" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="setting-and-goal"><span class="header-section-number">2</span> Setting and Goal</h2>
<p>At first, we restrict policies to <strong>finite-state controllers (FSCs)</strong> that keep internal memory <img src="https://latex.codecogs.com/png.latex?Z_k"> and act from <img src="https://latex.codecogs.com/png.latex?(Y_k,%20Z_k)">. The internal memory is updated in a Markovian way: <img src="https://latex.codecogs.com/png.latex?Z_%7Bk+1%7D~%5Cvarphi(%5Ccdot%5Cmid%20z_k,%20y_k,%20a_k)"> given <img src="https://latex.codecogs.com/png.latex?Z_k=z_k,Y_k=y_k,A_k=a_k">.</p>
<ul>
<li><p>Learn the best policy within a fixed FSC class <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7BZ,%5Cvarphi%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%5E%5Cstar%20%5Cin%20%5Carg%5Cmax_%7B%5Cpi%20%5Cin%20%5Cpi_%7BZ,%5Cvarphi%7D%7D%20%5C;%20%5Cmathcal%20V%5E%5Cpi(%5Cxi).%0A"></p></li>
<li><p>A useful subclass is the <strong>sliding-window controller (SWC)</strong> with window <img src="https://latex.codecogs.com/png.latex?n">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ_k%20=%20(Y_%7Bk-n:k-1%7D,%5C;%20A_%7Bk-n:k-1%7D)%20%5Cin%20%5Cmathcal%20Y%5En%20%5Ctimes%20%5Cmathcal%20A%5En,%0A"></p>
<p>which summarizes the last <img src="https://latex.codecogs.com/png.latex?n"> observations/actions, where <img src="https://latex.codecogs.com/png.latex?n"> is a design parameter that governs the tradeoff between memory complexity and optimality in solving POMDPs.</p></li>
</ul>
<p>The actor uses a <strong>linear-softmax FSC</strong>: <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi_%5Ctheta(a%20%5Cmid%20y,z)%20=%0A%5Cfrac%7B%5Cexp%5C%7B%5Ctheta%5E%5Ctop%20%5Cpsi(a,y,z)%5C%7D%7D%0A%7B%5Csum_%7Ba'%7D%20%5Cexp%5C%7B%5Ctheta%5E%5Ctop%20%5Cpsi(a',y,z)%5C%7D%7D,%0A"> with features <img src="https://latex.codecogs.com/png.latex?%5Cpsi(a,y,z)%5Cin%5Cmathbb%20R%5Ed">. The controller memory updates via some <img src="https://latex.codecogs.com/png.latex?%5Cvarphi">: <img src="https://latex.codecogs.com/png.latex?%0AZ_%7Bk+1%7D=%5Cvarphi(Z_k,%20Y_%7Bk+1%7D,%20A_k).%0A"></p>
</section>
<section id="sampling-measure" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sampling-measure"><span class="header-section-number">3</span> Sampling Measure</h2>
<p>Let the <strong>discounted visitation distribution</strong> over information states be <img src="https://latex.codecogs.com/png.latex?%0Ad%5E%5Cpi_%5Cxi(y,z)=(1-%5Cgamma)%5Csum_%7Bk%5Cge0%7D%5Cgamma%5Ek%5C,%0AP%5E%5Cpi%5C!%5Cbig%5B(Y_k,Z_k)=(y,z)%5C,%5Cbig%7C%5C,H_0%5Csim%5Cxi%5Cbig%5D.%0A"></p>
</section>
<section id="algorithm-fs-nac" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="algorithm-fs-nac"><span class="header-section-number">4</span> Algorithm (FS-NAC)</h2>
<ol type="1">
<li><strong>Critic</strong> — run <strong><img src="https://latex.codecogs.com/png.latex?m">-step TD(0)</strong> with linear function approximation to estimate <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7B%5Cmathcal%20Q%7D%5E%7B%5Cpi_t%7D(y,z,a)">.</li>
<li><strong>Actor</strong> — take a <strong>natural-gradient</strong> step using the critic’s advantage estimate.</li>
</ol>
</section>
<section id="critic-m-step-td-learning-with-an-internal-state" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="critic-m-step-td-learning-with-an-internal-state"><span class="header-section-number">5</span> Critic: <img src="https://latex.codecogs.com/png.latex?m">-step TD Learning with an Internal State</h2>
<p>It is straightforward to construct an example which shows that (memoryless) TD learning cannot learn the value functions in a POMDP, which is known as the perceptual aliasing pathology <span class="citation" data-cites="singh1994learning">(Singh, Jaakkola, and Jordan 1994)</span>. For a concrete demonstration, let us focus on learning the value function in a Hidden Markov Reward Process (HMRP): <img src="https://latex.codecogs.com/png.latex?V(z_0):=%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20%5Cgamma%5Ek%20r(S_k)%7CZ_0=z_0%5D"> for an internal state <img src="https://latex.codecogs.com/png.latex?Z_t"> (e.g., <img src="https://latex.codecogs.com/png.latex?Z_t%20=%20(Y_%7Bt-m+1%7D,%5Cldots,Y_t)">), where <img src="https://latex.codecogs.com/png.latex?(S_k,%20r(S_k),%20Y_k)"> is a HMRP.</p>
<p><strong>Example 1</strong> Consider an HMRP with <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20S%20=%20%5C%7B0,1%5C%7D">, <img src="https://latex.codecogs.com/png.latex?r(s)%20=%20s"> and <img src="https://latex.codecogs.com/png.latex?%5CPhi(1%7Cs)%20=%201"> for all <img src="https://latex.codecogs.com/png.latex?s%5Cin%5Cmathcal%7BS%7D">. Let <img src="https://latex.codecogs.com/png.latex?P(0%7C1)%20=%20p"> and <img src="https://latex.codecogs.com/png.latex?P(1%7C0)%20=%20q"> for some <img src="https://latex.codecogs.com/png.latex?p,%20q%20%5Cin%20(0,1)">, and <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%5Cin%20(0,%201)"> be the discount factor. As TD(0) cannot distinguish different latent states due to the noisy observation and the lack of memory, it converges to <img src="https://latex.codecogs.com/png.latex?v%5E%5Cstar%20=%20%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%5Ccdot%20%5Cfrac%7Bp%7D%7Bq+p%7D">. On the other hand, the value function for a given internal state <img src="https://latex.codecogs.com/png.latex?Z_0=z_0"> is <img src="https://latex.codecogs.com/png.latex?V(z_0)%20=%20%5Cfrac%7B%5Cgamma%20p%20+%20(1-%5Cgamma)P(S_0=1%7CZ_0=z_0)%7D%7B(1-%5Cgamma)%5CBig%5B%5Cgamma(p+q)+1-%5Cgamma%5CBig%5D%7D%5Cneq%20v%5E%5Cstar."> As such, memoryless TD learning cannot learn <img src="https://latex.codecogs.com/png.latex?V(z_0)">.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Why <img src="https://latex.codecogs.com/png.latex?m">-step TD?</strong> In POMDPs, one-step bootstrapping suffers from <strong>perceptual aliasing</strong> (same observation <img src="https://latex.codecogs.com/png.latex?Y_k">, different latent state <img src="https://latex.codecogs.com/png.latex?S_k">). Looking <img src="https://latex.codecogs.com/png.latex?m"> steps ahead stabilizes the target and reduces this bias.</p>
</div>
</div>
<p>To see how <img src="https://latex.codecogs.com/png.latex?m">-step TD learning with internal state <img src="https://latex.codecogs.com/png.latex?Z_t%20=%20(Y_%7Bt-m+1%7D,%5Cldots,Y_t)"> mitigates perceptual aliasing, let us focus on the limit point. To that end, let <img src="https://latex.codecogs.com/png.latex?(T_mv)(z)%20=%20%5Cbar%7Br%7D_m(z)%20+%20%5Cgamma%5Em(K_mv)(z),"> where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Br%7D_m(z)%20:=%20%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%7Bm-1%7Dr(S_k)%7CZ_0=z%5D"> and <img src="https://latex.codecogs.com/png.latex?(K_mv)(z):=%5Cmathbb%7BE%7D%5Bv(Z_m)%7CZ_0=z%5D">. Then, it is straightforward to show that <img src="https://latex.codecogs.com/png.latex?T_m"> is a contractive operator with modulus <img src="https://latex.codecogs.com/png.latex?%5Cgamma%5Em">, and admits a unique fixed point <img src="https://latex.codecogs.com/png.latex?V(z)%20=%20%5Cmathbb%20E%5B%5Csum_%7Bk=0%7D%5E%5Cinfty%20r(S_k)%7CZ_0=z%5D%20=%20%5Csum_%7Bk=0%7D%5E%5Cinfty%20(%5Cgamma%5EmK_m)%5Ek%5Cbar%7Br%7D_m(z)"> for any <img src="https://latex.codecogs.com/png.latex?m%5Cin%5Cmathbb%20N">. Let <img src="https://latex.codecogs.com/png.latex?%5Cxi"> be the invariant distribution of <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_k:k%5Cin%5Cmathbb%20N%5C%7D">: <img src="https://latex.codecogs.com/png.latex?%5Cxi(z_t)=%5Csum_%7Bs_%7Bt-m+1:t%7D%7D%5Cprod_%7Bk=0%7D%5E%7Bm-1%7D%5CPhi(y_%7Bt-k%7D%7Cs_%7Bt-k%7D)%5Cmu(s_%7Bt-k%7D),"> where <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is the stationary distribution of <img src="https://latex.codecogs.com/png.latex?%5C%7BS_k:k%5Cin%5Cmathbb%20N%5C%7D">. Under a non-degenerate <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7BZ_t%5Csim%20%5Cxi%7D%5B%5Cpsi(Z_t)%5Cotimes%5Cpsi(Z_t)%5D%5Csucc%200">, <img src="https://latex.codecogs.com/png.latex?m">-step TD learning with the internal state <img src="https://latex.codecogs.com/png.latex?Z_t"> converges to the unique fixed point <img src="https://latex.codecogs.com/png.latex?v_m"> of the equation <img src="https://latex.codecogs.com/png.latex?v%20=%20%5CPi%5C%7B%5Cbar%7Br%7D_m+%5Cgamma%5Em%20K_m%20v%5C%7D,"> where <img src="https://latex.codecogs.com/png.latex?%5CPi"> be the ortogonal projection onto the subspace <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20V%20:=%20%5C%7Bz%5Cmapsto%20%5Clangle%20%5Cpsi(z),w%5Crangle:%20w%20%5Cin%20%5Cmathbb%20R%5Ed"> with respect to <img src="https://latex.codecogs.com/png.latex?L%5E2(%5Cxi)">. Thus, one obtains <img src="https://latex.codecogs.com/png.latex?%5C%7Cv_m%20-%20%5CPi%20V%5C%7C_%7BL%5E2(%5Cxi)%7D%20%5Cleq%20%5Cfrac%7B%5Cgamma%5Em%7D%7B1+%5Cgamma%5Em%7D%5C%7CV-%5CPi%20V%5C%7C_%7BL%5E2(%5Cxi)%7D."> As such, it is possible to learn <img src="https://latex.codecogs.com/png.latex?V(z_0)"> via <img src="https://latex.codecogs.com/png.latex?m">-step TD learning with an additional error term <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(%5Cgamma%5Em)">.</p>
<p>With stepsize <img src="https://latex.codecogs.com/png.latex?%5Calpha=K%5E%7B-1/2%7D"> and parameter radius <img src="https://latex.codecogs.com/png.latex?R">, after <img src="https://latex.codecogs.com/png.latex?K"> critic updates: <img src="https://latex.codecogs.com/png.latex?%0A%5Csqrt%7B%5Cmathbb%20E%5C!%5Cleft%5B%5Cbig%5C%7C%5Cmathcal%20Q%5E%5Cpi-%5Cwidehat%7B%5Cmathcal%20Q%7D%5E%5Cpi_K%5Cbig%5C%7C%5E2_%7B%5C,d%5E%5Cpi_%5Cxi%5Cotimes%5Cpi%7D%5Cright%5D%7D%0A%5C;%5Clesssim%5C;%0A%5Cunderbrace%7B%5Ctfrac%7BK%5E%7B-1/4%7D%7D%7B1-%5Cgamma%7D%7D_%7B%5Ctext%7Bstatistical%7D%7D%0A+%5Cunderbrace%7B%5Ctfrac%7B%5Cvarepsilon_%7B%5Ctext%7Bapp%7D%7D(R)%7D%7B1-%5Cgamma%5Em%7D%7D_%7B%5Ctext%7Bapproximation%7D%7D%0A+%5Cunderbrace%7B%5Cvarepsilon_%7B%5Ctext%7Bpa%7D%7D(%5Cgamma,m,R)%7D_%7B%5Ctext%7Baliasing%7D%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon_%7B%5Ctext%7Bapp%7D%7D(R)"> is the best linear-approximation error within radius <img src="https://latex.codecogs.com/png.latex?R">, and the aliasing term contracts geometrically: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarepsilon_%7B%5Ctext%7Bpa%7D%7D(%5Cgamma,m,R)=%5Cmathcal%20O%5C!%5Cbig(%5Cgamma%5E%7Bm/2%7D%5C,%5Cmathrm%7Bpoly%7D(R,(1-%5Cgamma)%5E%7B-1%7D)%5Cbig).%0A"></p>
<p><strong>Sample–accuracy trade-off.</strong> Each update uses <img src="https://latex.codecogs.com/png.latex?m"> samples, so total evaluation samples are <img src="https://latex.codecogs.com/png.latex?%5CTheta(mK)">.</p>
</section>
<section id="full-actorcritic-performance-finite-time" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="full-actorcritic-performance-finite-time"><span class="header-section-number">6</span> Full Actor–Critic Performance (Finite-Time)</h2>
<p>After <img src="https://latex.codecogs.com/png.latex?T"> outer iterations (with tuned stepsizes), <img src="https://latex.codecogs.com/png.latex?%0A(1-%5Cgamma)%5C,%5Cmin_%7Bt%3CT%7D%5Cmathbb%20E%5C!%5Cbig%5B%5Cmathcal%20V%5E%7B%5Cpi%5E%5Cstar%7D(%5Cxi)-%5Cmathcal%20V%5E%7B%5Cpi_t%7D(%5Cxi)%5Cbig%5D%0A%5C;%5Clesssim%5C;%0A%5Cunderbrace%7BT%5E%7B-1/2%7D%7D_%7B%5Ctext%7Bactor%20optimization%7D%7D%0A+%5Cunderbrace%7B%5Cvarepsilon_%7B%5Ctext%7Bcritic%7D%7D(K,m,R)%7D_%7B%5Ctext%7BTD%20evaluation%7D%7D%0A+%5Cunderbrace%7B%5Cvarepsilon_%7B%5Ctext%7Bactor%7D%7D(N,R)%7D_%7B%5Ctext%7Bcompatible%20approx.%7D%7D%0A+%5Cunderbrace%7B%5Cvarepsilon_%7B%5Ctext%7Binf%7D%7D(%5Cxi)%7D_%7B%5Ctext%7Binference%20penalty%7D%7D.%0A"></p>
<p>The <strong>inference error</strong> is the price of partial observability: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarepsilon_%7B%5Ctext%7Binf%7D%7D(%5Cxi)%0A=%20%5Cmathbb%20E%5C!%5Cleft%5B%5Csum_%7Bk%5Cge0%7D%5Cgamma%5Ek%0A%5Cleft%5C%7C%5C,b_k(%5Ccdot)-b_0(%5Ccdot,I_k)%5C,%5Cright%5C%7C_%7B%5Cmathrm%7BTV%7D%7D%0A%5Cright%5D,%5Cqquad%20I_k=(Y_k,Z_k).%0A"></p>
</section>
<section id="sliding-window-controllers-memory-vs.-accuracy" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="sliding-window-controllers-memory-vs.-accuracy"><span class="header-section-number">7</span> Sliding-Window Controllers (Memory vs.&nbsp;Accuracy)</h2>
<p>Under stochastic exploration and filter stability/minorization, the inference error <strong>decays geometrically</strong> with window length <img src="https://latex.codecogs.com/png.latex?n">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cvarepsilon_%7B%5Ctext%7Binf%7D%7D(%5Cxi)%5C;%5Cle%5C;%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%5Ccdot%0A%5Cmathcal%20O%5C!%5CBig(%5Crho%5E%7B%5Clfloor%20n/m_0%5Crfloor%7D%5CBig),%0A"> for constants <img src="https://latex.codecogs.com/png.latex?%5Crho%5Cin(0,1)"> and <img src="https://latex.codecogs.com/png.latex?m_0%5Cge1">. To reach tolerance <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">: <img src="https://latex.codecogs.com/png.latex?%0An=%5Cmathcal%20O%5C!%5Cbig(m_0%5Clog(1/%5Cepsilon)%5Cbig).%0A"></p>
</section>
<section id="practical-tuning" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="practical-tuning"><span class="header-section-number">8</span> Practical Tuning</h2>
<ul>
<li><strong>TD horizon <img src="https://latex.codecogs.com/png.latex?m">:</strong> choose <img src="https://latex.codecogs.com/png.latex?m=%5CTheta(%5Clog_%7B1/%5Cgamma%7D(1/%5Cepsilon))"> to make aliasing <img src="https://latex.codecogs.com/png.latex?%5Clesssim%5Cepsilon">.</li>
<li><strong>Critic steps <img src="https://latex.codecogs.com/png.latex?K"> and actor inner steps <img src="https://latex.codecogs.com/png.latex?N">:</strong> about <img src="https://latex.codecogs.com/png.latex?%5CTheta(%5Cepsilon%5E%7B-4%7D)">.</li>
<li><strong>Features:</strong> richer features shrink the compatible approximation term.</li>
<li><strong>Window <img src="https://latex.codecogs.com/png.latex?n">:</strong> grows like <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%20O(%5Clog(1/%5Cepsilon))"> under filter stability.</li>
</ul>
</section>
<section id="whats-next-rnn-based-nac-for-pomdps" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="whats-next-rnn-based-nac-for-pomdps"><span class="header-section-number">9</span> What’s next: RNN-based NAC for POMDPs</h2>
<p>A natural extension replaces the hand-engineered memory <img src="https://latex.codecogs.com/png.latex?Z_k"> with a <strong>recurrent hidden state</strong> <img src="https://latex.codecogs.com/png.latex?H_k"> learned end-to-end (RNN-based NAC). The policy becomes <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta(a%5Cmid%20Y_k,%20H_k)"> with <img src="https://latex.codecogs.com/png.latex?H_%7Bk+1%7D=f_%5Ctheta(H_k,Y_%7Bk+1%7D,A_k)">, and the natural-gradient machinery is adapted to the recurrent parameterization. I’ll cover how this compares to FSCs (sample complexity vs.&nbsp;representation power, and how partial-observability error shows up) in a follow-up post about our <strong>RNN-based natural actor–critic</strong> paper.</p>
<hr>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-astrom1965" class="csl-entry">
Åström, Karl J. 1965. <span>“Optimal Control of Markov Processes with Incomplete State Information i.”</span> <em>Journal of Mathematical Analysis and Applications</em> 10 (1): 174–205. <a href="https://doi.org/10.1016/0022-247X(65)90154-X">https://doi.org/10.1016/0022-247X(65)90154-X</a>.
</div>
<div id="ref-cayci2024nac" class="csl-entry">
Cayci, Semih, Niao He, and R. Srikant. 2024. <span>“Finite-Time Analysis of Natural Actor-Critic for POMDPs.”</span> <em>SIAM Journal on Mathematics of Data Science</em>.
</div>
<div id="ref-murphy2000survey" class="csl-entry">
Murphy, Kevin P. 2000. <span>“A Survey of POMDP Solution Techniques.”</span> <em>Environment</em> 2 (10).
</div>
<div id="ref-singh1994learning" class="csl-entry">
Singh, Satinder P, Tommi Jaakkola, and Michael I Jordan. 1994. <span>“Learning Without State-Estimation in Partially Observable Markovian Decision Processes.”</span> In <em>Machine Learning Proceedings 1994</em>, 284–92. Elsevier.
</div>
<div id="ref-yuksel2025another" class="csl-entry">
Yüksel, Serdar. 2025. <span>“Another Look at Partially Observed Optimal Stochastic Control: Existence, Ergodicity, and Approximations Without Belief-Reduction.”</span> <em>Applied Mathematics &amp; Optimization</em> 91 (1): 16.
</div>
</div></section></div> ]]></description>
  <category>reinforcement-learning</category>
  <category>POMDP</category>
  <category>actor-critic</category>
  <category>theory</category>
  <guid>https://semihcayci.github.io/posts/nac-for-pomdps.html</guid>
  <pubDate>Sun, 12 Oct 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>All Posts</title>
  <link>https://semihcayci.github.io/posts/</link>
  <description><![CDATA[ 








<div class="quarto-listing quarto-listing-container-table" id="listing-listing">
<div class="listing-actions-group">
   <div class="input-group input-group-sm quarto-listing-sort">
    <span class="input-group-text"><i class="bi bi-sort-down"></i></span>
    <select id="listing-listing-sort" class="form-select" aria-label="Order By" onchange="window['quarto-listings']['listing-listing'].sort(this.options[this.selectedIndex].value, { order: this.options[this.selectedIndex].getAttribute('data-direction')})">
      <option value="" disabled="" selected="" hidden="">Order By</option>
      <option value="index" data-direction="asc">Default</option>
      <option value="listing-date-sort" data-direction="asc">
        Date - Oldest
      </option>
      <option value="listing-date-sort" data-direction="desc">
        Date - Newest
      </option>
      <option value="listing-title-sort" data-direction="asc">
        Title
      </option>
      <option value="listing-author" data-direction="asc">
        Author
      </option>
    </select>
  </div>
    <div class="input-group input-group-sm quarto-listing-filter">
      <span class="input-group-text"><i class="bi bi-search"></i></span>
      <input type="text" class="search form-control" placeholder="Filter">
    </div>
</div>
<table class="quarto-listing-table table">
<thead>
<tr>

<th>
<a class="sort" data-sort="listing-date-sort" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Date</a>
</th>

<th>
<a class="sort" data-sort="listing-title-sort" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Title</a>
</th>

<th>
<a class="sort" data-sort="listing-author" onclick="if (this.classList.contains('sort-asc')) { this.classList.add('sort-desc'); this.classList.remove('sort-asc') } else { this.classList.add('sort-asc'); this.classList.remove('sort-desc')} return false;">Author</a>
</th>

</tr>
</thead>
<tbody class="list">

<tr data-index="0" data-categories="cmVpbmZvcmNlbWVudC1sZWFybmluZyUyQ1BPTURQJTJDYWN0b3ItY3JpdGljJTJDdGhlb3J5" data-listing-date-sort="1760227200000" data-listing-file-modified-sort="1760493241005" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="6" data-listing-word-count-sort="1073" data-listing-title-sort="Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers" data-listing-filename-sort="nac-for-pomdps.qmd">
<td>
<span class="listing-date">Oct 12, 2025</span>
</td>
<td>
<a href="../posts/nac-for-pomdps.html" class="title listing-title">Natural Policy Gradient for POMDPs – Part I: Finite-State Controllers</a>
</td>
<td>
<span class="listing-author">&nbsp;</span>
</td>

</tr>

</tbody>
</table>
<div class="listing-no-matching d-none">No matching items</div>
</div> ]]></description>
  <guid>https://semihcayci.github.io/posts/</guid>
  <pubDate>Wed, 15 Oct 2025 01:54:22 GMT</pubDate>
</item>
</channel>
</rss>
